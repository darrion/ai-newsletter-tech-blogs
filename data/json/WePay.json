{
  "company": "WePay",
  "title": "WePay",
  "xmlUrl": "https://wecode.wepay.com/feed.xml",
  "htmlUrl": "https://wecode.wepay.com/",
  "content": "\n\n\n\n\n\nWePay's Logging Infrastructure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwepay.com\ncareers\nfollow \n\n\n\n\n\n\n\n\n\n\n      Kartik Deshpande\u00a0\n      \u00a0\n      \n      \n\n\nSenior Engineer\n\n\n\nWePay's Logging Infrastructure\n\n        By Kartik Deshpande\n        on Aug 10, 2021\n\n\n\nIntroduction\nWithout logs all of us would be stumbling in the dark. We would know that something is wrong, but be unable to figure out exactly what. This article is going to talk about how WePay\u00e2\u0080\u0099s logging infrastructure is set up.\nOur centralized logging architecture collects logs from all of our microservices, virtual machines, audit logs from our cloud provider and external vendors, and stores them in a proper format to help search and analyze issues and errors.\nWePay uses the ELK stack and Kafka components in it\u00e2\u0080\u0099s core logging pipeline.\nIn this article we are going to talk about:\n\n\nLog Aggregation\n\n\nLog Processing and Enrichment\n\n\nLog Buffering and Ingestion\n\n\nStoring Logs\n\n\nSearching Logs\n\n\n\nLog Aggregation\nWePay uses filebeat for tailing and shipping logs from VM\u00e2\u0080\u0099s and all our GKE microservices. Filebeat guarantees that events will be delivered to the configured output at least once and with no data loss. Filebeat is able to achieve this behavior because it stores the delivery state of each event in the registry file. In situations where the defined output is blocked and has not confirmed all events, Filebeat will keep trying to send events until the output acknowledges that it has received the events.\nFilebeat forwards all logs to logstash for more advanced processing and enrichment.\nLog Processing and Enrichment\nLogstash filters parse each log event, identify named fields to build structure, and transform them to converge on a common format for more powerful analysis. Logstash helps us in:\n\n\nDeriving structure from unstructured data with powerful grok and mutate filters.\n\n\nDecipher geo coordinates from IP addresses.\n\n\nAnonymize PII data, exclude sensitive fields completely from the logs.\n\n\nEase overall processing, independent of the data source, format, or schema.\n\n\nConvert json log messages into avro format that match an avro schema so then they can be stored in Kafka.\n\n\nA raw nginx log message can be parsed into something like:\n10.73.1.39 - test [29/May/2021:23:10:31 +0000] \"GET /v1/sys/health HTTP/1.0\" 200 293 \"-\" \"-\" \"-\"\n\nClient IP : 10.73.1.39\nUser : test \nTimestamp : 29/May/2021:23:10:31\nMethod: GET\nURL: /v1/sys/health\nHTTP_Version : 1.0\nResponse : 200\nRequest Bytes : 293\n\nWePay relies on Avro messages because the structure of logs changes over time. If a \u00e2\u0080\u009cstring\u00e2\u0080\u009d field is converted into an \u00e2\u0080\u009cint\u00e2\u0080\u009d field we would have elasticsearch mapping conflicts, Avro schemas help ensure that we don\u00e2\u0080\u0099t have conflicts in our ElasticSchema\u00e2\u0080\u0099s.\nLog Buffering and Ingestion\nBuffering\nSometimes applications log at unprecedented levels, during incidents, incorrect logging format, or when there are bugs in the application. These sudden log surges can overwhelm our logging infrastructure. To protect Elasticsearch from such cases of data surges. WePay uses Apache Kafka to buffer logs. In our logging pipeline, logstash forwards all the logs to Kafka in Avro format.\nIngestion\nWePay uses light weight Confluent Kafka Connectors for ingesting logs into Elastic/Google BigQuery/Google Cloud Storage Buckets. We use 3 kind of connectors:\n\n\nKafka Connect Elasticsearch to ingest logs into our elastic Cluster\n\n\nOur inhouse developed Google BigQuery Sink Connector to ingest logs into Google\u00e2\u0080\u0099s BigQuery.\n\n\nGCS Sink Connector to ingest logs into Google Cloud Storage Buckets.\n\n\n\nStoring Logs\nWePay, as a payments company, is subject to PCI DSS audits. PCI requirements state that logs must be retained for at least one year. Ninety days of logs must also be available for immediate analysis. We store our logs in 3 places:\n\n\nElasticsearch - 90 days of retention policy for immediate analysis.\n\n\nGoogle Big Query - Logs are retained for long-term per regulatory and compliance requirements\nFor analysis when we want to look for historic data ( data not in elastic because of retention policies ). With BigQuery\u00e2\u0080\u0099s powerful query interfaces we can structure and search for logs as needed.\n\n\nGoogle Cloud Storage - Logs are retained for long-term per regulatory and compliance requirements. We mainly store logs in GCS to backfill Google\u00e2\u0080\u0099s BQ in times when the connector to BigQuery is broken.\n\n\nAt WePay we have 4 environments: dev, testing, staging and production. Each environment generates a lot of logs. We have two elastic clusters to store these logs:\n\n\nA Dev Elastic cluster for development, Testing and staging environment logs.\n\n\nA production Elastic cluster for our production environment logs.\n\n\nSetting up two elastic clusters had the following advantages:\n\n\nIf an application in dev or testing environments goes rogue and starts logging at unprecedented levels because of a bug or incorrect logging format, this will impact only our dev cluster.\n\n\nWhen upgrading elastic to a new version, we can first upgrade the dev cluster; if we encounter any backward compatibility issues, only the dev cluster would be impacted. The production cluster remains safe.\n\n\n\nHot Warm Cold Architecture\nWith the amount of data that we get, we had to think upstream while designing the data retention and index life cycle management policies.\nThere are 3 kinds of elastic data nodes setup:\n\n\nHot nodes which will have all new indexes, and will get extensive reads and writes. This is where all new data will be indexed. These are CPU extensive nodes.\n\n\nWarm nodes will host old indexes and will get all reads and no writes.\n\n\nCold nodes will host old indices and will barely get any reads.\n\n\nElastic\u00e2\u0080\u0099s ILM gives us the ability to manage indexes by triggering actions when conditions are met. With ILM policies we have something like:\n\n\nFor creating a new index and indexing all data use hot nodes. The ILM policy will start by setting the index priority to a high value for the indices so that hot indexes will recover before other indexes in the event of failures.\n\n\nAfter the index has completed one week of it\u00e2\u0080\u0099s time in a hot node, merge the number of segments to 1 and move it to the warm node.\n\n\nAfter the index is completed 1 month of its time in the warm node, freeze the index and move it to the cold node.\n\n\nDelete the index after 3 months.\n\n\nSearching - Kibana\nIn the previous section, we talked about how we have two elastic clusters. We didn\u00e2\u0080\u0099t want two Kibana\u00e2\u0080\u0099s for searching logs in each cluster, so we decided to go with cross cluster search.\nWe achieved this by creating a third elasticsearch cluster, that sends requests to the dev and production clusters. This architecture has several benefits:\n\n\nAll security roles and access permissions for end users are defined in this cluster, hence we avoid creating duplicate security mappings in dev and production logging clusters.\n\n\nLicensed Machine learning nodes run only on this proxy cluster to gather insights from the logs stored in Dev and Production Cluster.\n\n\nA Proxy cluster can help set search thread throttling.\n\n\nAn end user in Kibana with superuser permissions won\u00e2\u0080\u0099t be able to make changes to Dev and Production clusters, hence increasing the overall security.\n\n\n\nFuture Work\n\n\nWe aim at getting APM data from our applications into elasticsearch. This will help us to strongly correlate logging events with APM insights.\n\n\nWe plan on setting up Cross Cluster Replication across multiple regions with the active passive model, and switching between each other during region failures for better fault tolerance.\n\n\nWe plan on using Frozen Tier for elastic, for searching for longer duration of logs ( 90+ days ) by referencing storage buckets.\n\n\nConclusion\nThe elastic clusters have been stress tested to handle upto 80k logs per second. Separating the Dev and Production elastic clusters has helped us protect the Production logging pipeline from Dev environment data surges. The log pipeline to BigQuery makes sure we have longer retention of data at low cost. The Proxy cluster along with Cross Cluster Search has helped us protect the elasticsearch data in Dev and Production clusters by having a single point authenticating and surfacing all search requests.\n\n\nLike this post? Join WePay's engineering team.\nView Openings\n\n\n\nBack to blog\nPrevious post\nNext post\n\n\n\n\n\n\n\n\n\nSupport\n\n\nSecurity\n\n\nTerms of Service\n\n\nPrivacy Policy\n\n\nSitemap\n\n\n          \u00a92018 WePay\n        \n\n\n\n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "title": "WePay's Logging Infrastructure",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://wecode.wepay.com/feed.xml",
      "value": "WePay's Logging Infrastructure"
    },
    "summary": "<h3 id=\"introduction\">Introduction</h3>\n\n<p>Without logs all of us would be stumbling in the dark. We would know that something is wrong, but be unable to figure out exactly what. This article is going to talk about how WePay\u2019s logging infrastructure is set up.</p>\n\n<p>Our centralized logging architecture collects logs from all of our microservices, virtual machines, audit logs from our cloud provider and external vendors, and stores them in a proper format to help search and analyze issues and errors.</p>\n\n<p>WePay uses the <a href=\"https://www.elastic.co/what-is/elk-stack\">ELK stack</a> and <a href=\"https://www.confluent.io/what-is-apache-kafka/\">Kafka</a> components in it\u2019s core logging pipeline.</p>\n\n<p>In this article we are going to talk about:</p>\n\n<ol>\n  <li>\n    <p>Log Aggregation</p>\n  </li>\n  <li>\n    <p>Log Processing and Enrichment</p>\n  </li>\n  <li>\n    <p>Log Buffering and Ingestion</p>\n  </li>\n  <li>\n    <p>Storing Logs</p>\n  </li>\n  <li>\n    <p>Searching Logs</p>\n  </li>\n</ol>\n\n<p><img alt=\"log_pipeline\" class=\"center-image\" id=\"log_pipeline\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_0.png\" /></p>\n\n<h3 id=\"log-aggregation\">Log Aggregation</h3>\n\n<p>WePay uses filebeat for tailing and shipping logs from VM\u2019s and all our GKE microservices. Filebeat guarantees that events will be delivered to the configured output at least once and with no data loss. Filebeat is able to achieve this behavior because it stores the delivery state of each event in the registry file. In situations where the defined output is blocked and has not confirmed all events, Filebeat will keep trying to send events until the output acknowledges that it has received the events.</p>\n\n<p>Filebeat forwards all logs to logstash for more advanced processing and enrichment.</p>\n\n<h3 id=\"log-processing-and-enrichment\">Log Processing and Enrichment</h3>\n\n<p>Logstash filters parse each log event, identify named fields to build structure, and transform them to converge on a common format for more powerful analysis. Logstash helps us in:</p>\n\n<ul>\n  <li>\n    <p>Deriving structure from unstructured data with powerful grok and mutate filters.</p>\n  </li>\n  <li>\n    <p>Decipher geo coordinates from IP addresses.</p>\n  </li>\n  <li>\n    <p>Anonymize PII data, exclude sensitive fields completely from the logs.</p>\n  </li>\n  <li>\n    <p>Ease overall processing, independent of the data source, format, or schema.</p>\n  </li>\n  <li>\n    <p>Convert json log messages into avro format that match an avro schema so then they can be stored in Kafka.</p>\n  </li>\n</ul>\n\n<p>A raw nginx log message can be parsed into something like:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>10.73.1.39 - test [29/May/2021:23:10:31 +0000] \"GET /v1/sys/health HTTP/1.0\" 200 293 \"-\" \"-\" \"-\"\n\nClient IP : 10.73.1.39\nUser : test \nTimestamp : 29/May/2021:23:10:31\nMethod: GET\nURL: /v1/sys/health\nHTTP_Version : 1.0\nResponse : 200\nRequest Bytes : 293\n</code></pre></div></div>\n\n<p>WePay relies on Avro messages because the structure of logs changes over time. If a \u201cstring\u201d field is converted into an \u201cint\u201d field we would have elasticsearch mapping conflicts, Avro schemas help ensure that we don\u2019t have conflicts in our ElasticSchema\u2019s.</p>\n\n<h3 id=\"log-buffering-and-ingestion\">Log Buffering and Ingestion</h3>\n\n<h4 id=\"buffering\">Buffering</h4>\n\n<p>Sometimes applications log at unprecedented levels, during incidents, incorrect logging format, or when there are bugs in the application. These sudden log surges can overwhelm our logging infrastructure. To protect Elasticsearch from such cases of data surges. WePay uses Apache Kafka to buffer logs. In our logging pipeline, logstash forwards all the logs to Kafka in Avro format.</p>\n\n<h4 id=\"ingestion\">Ingestion</h4>\n\n<p>WePay uses light weight Confluent Kafka Connectors for ingesting logs into Elastic/Google BigQuery/Google Cloud Storage Buckets. We use 3 kind of connectors:</p>\n\n<ul>\n  <li>\n    <p><a href=\"https://github.com/confluentinc/kafka-connect-elasticsearch\">Kafka Connect Elasticsearch</a> to ingest logs into our elastic Cluster</p>\n  </li>\n  <li>\n    <p>Our inhouse developed <a href=\"https://github.com/confluentinc/kafka-connect-bigquery\">Google BigQuery Sink Connector</a> to ingest logs into Google\u2019s BigQuery.</p>\n  </li>\n  <li>\n    <p><a href=\"https://docs.confluent.io/kafka-connect-gcs-sink/current/overview.html\">GCS Sink Connector</a> to ingest logs into Google Cloud Storage Buckets.</p>\n  </li>\n</ul>\n\n<p><img alt=\"kafka_connectors\" class=\"center-image\" id=\"kafka_connectors\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_1.png\" /></p>\n\n<h3 id=\"storing-logs\">Storing Logs</h3>\n\n<p>WePay, as a payments company, is subject to <a href=\"https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard\">PCI DSS audits</a>. PCI requirements state that logs must be retained for at least one year. Ninety days of logs must also be available for immediate analysis. We store our logs in 3 places:</p>\n\n<ul>\n  <li>\n    <p><a href=\"https://www.elastic.co/elasticsearch/\">Elasticsearch</a> - 90 days of retention policy for immediate analysis.</p>\n  </li>\n  <li>\n    <p><a href=\"https://cloud.google.com/bigquery\">Google Big Query</a><a href=\"https://cloud.google.com/bigquery\"> </a>- Logs are retained for long-term per regulatory and compliance requirements\nFor analysis when we want to look for historic data ( data not in elastic because of retention policies ). With BigQuery\u2019s powerful query interfaces we can structure and search for logs as needed.</p>\n  </li>\n  <li>\n    <p><a href=\"https://cloud.google.com/storage\">Google Cloud Storage</a> - Logs are retained for long-term per regulatory and compliance requirements. We mainly store logs in GCS to backfill Google\u2019s BQ in times when the connector to BigQuery is broken.</p>\n  </li>\n</ul>\n\n<p>At WePay we have 4 environments: dev, testing, staging and production. Each environment generates a lot of logs. We have two elastic clusters to store these logs:</p>\n\n<ul>\n  <li>\n    <p>A Dev Elastic cluster for development, Testing and staging environment logs.</p>\n  </li>\n  <li>\n    <p>A production Elastic cluster for our production environment logs.</p>\n  </li>\n</ul>\n\n<p>Setting up two elastic clusters had the following advantages:</p>\n\n<ul>\n  <li>\n    <p>If an application in dev or testing environments goes rogue and starts logging at unprecedented levels because of a bug or incorrect logging format, this will impact only our dev cluster.</p>\n  </li>\n  <li>\n    <p>When upgrading elastic to a new version, we can first upgrade the dev cluster; if we encounter any backward compatibility issues, only the dev cluster would be impacted. The production cluster remains safe.</p>\n  </li>\n</ul>\n\n<p><img alt=\"elastic_dev_prd\" class=\"center-image\" id=\"elastic_dev_prd\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_2.png\" /></p>\n\n<h4 id=\"hot-warm-cold-architecture\">Hot Warm Cold Architecture</h4>\n\n<p>With the amount of data that we get, we had to think upstream while designing the data retention and index life cycle management policies.</p>\n\n<p>There are 3 kinds of elastic data nodes setup:</p>\n\n<ul>\n  <li>\n    <p>Hot nodes which will have all new indexes, and will get extensive reads and writes. This is where all new data will be indexed. These are CPU extensive nodes.</p>\n  </li>\n  <li>\n    <p>Warm nodes will host old indexes and will get all reads and no writes.</p>\n  </li>\n  <li>\n    <p>Cold nodes will host old indices and will barely get any reads.</p>\n  </li>\n</ul>\n\n<p><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html\">Elastic\u2019s ILM</a> gives us the ability to manage indexes by triggering actions when conditions are met. With ILM policies we have something like:</p>\n\n<ul>\n  <li>\n    <p>For creating a new index and indexing all data use hot nodes. The ILM policy will start by setting the index priority to a high value for the indices so that hot indexes will recover before other indexes in the event of failures.</p>\n  </li>\n  <li>\n    <p>After the index has completed one week of it\u2019s time in a hot node, merge the number of segments to 1 and move it to the warm node.</p>\n  </li>\n  <li>\n    <p>After the index is completed 1 month of its time in the warm node, freeze the index and move it to the cold node.</p>\n  </li>\n  <li>\n    <p>Delete the index after 3 months.</p>\n  </li>\n</ul>\n\n<h3 id=\"searching---kibana\">Searching - Kibana</h3>\n\n<p>In the previous section, we talked about how we have two elastic clusters. We didn\u2019t want two Kibana\u2019s for searching logs in each cluster, so we decided to go with cross cluster search.</p>\n\n<p>We achieved this by creating a third elasticsearch cluster, that sends requests to the dev and production clusters. This architecture has several benefits:</p>\n\n<ul>\n  <li>\n    <p>All security roles and access permissions for end users are defined in this cluster, hence we avoid creating duplicate security mappings in dev and production logging clusters.</p>\n  </li>\n  <li>\n    <p>Licensed Machine learning nodes run only on this proxy cluster to gather insights from the logs stored in Dev and Production Cluster.</p>\n  </li>\n  <li>\n    <p>A Proxy cluster can help set search thread throttling.</p>\n  </li>\n  <li>\n    <p>An end user in Kibana with superuser permissions won\u2019t be able to make changes to Dev and Production clusters, hence increasing the overall security.</p>\n  </li>\n</ul>\n\n<p><img alt=\"proxy_search\" class=\"center-image\" id=\"proxy_search\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_3.png\" /></p>\n\n<h3 id=\"future-work\">Future Work</h3>\n\n<ul>\n  <li>\n    <p>We aim at getting APM data from our applications into elasticsearch. This will help us to strongly correlate logging events with APM insights.</p>\n  </li>\n  <li>\n    <p>We plan on setting up <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-ccr.html\">Cross Cluster Replication</a> across multiple regions with the active passive model, and switching between each other during region failures for better fault tolerance.</p>\n  </li>\n  <li>\n    <p>We plan on using <a href=\"https://www.elastic.co/blog/introducing-elasticsearch-frozen-tier-searchbox-on-s3\">Frozen Tier</a> for elastic, for searching for longer duration of logs ( 90+ days ) by referencing storage buckets.</p>\n  </li>\n</ul>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>The elastic clusters have been stress tested to handle upto 80k logs per second. Separating the Dev and Production elastic clusters has helped us protect the Production logging pipeline from Dev environment data surges. The log pipeline to BigQuery makes sure we have longer retention of data at low cost. The Proxy cluster along with Cross Cluster Search has helped us protect the elasticsearch data in Dev and Production clusters by having a single point authenticating and surfacing all search requests.</p>",
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://wecode.wepay.com/feed.xml",
      "value": "<h3 id=\"introduction\">Introduction</h3>\n\n<p>Without logs all of us would be stumbling in the dark. We would know that something is wrong, but be unable to figure out exactly what. This article is going to talk about how WePay\u2019s logging infrastructure is set up.</p>\n\n<p>Our centralized logging architecture collects logs from all of our microservices, virtual machines, audit logs from our cloud provider and external vendors, and stores them in a proper format to help search and analyze issues and errors.</p>\n\n<p>WePay uses the <a href=\"https://www.elastic.co/what-is/elk-stack\">ELK stack</a> and <a href=\"https://www.confluent.io/what-is-apache-kafka/\">Kafka</a> components in it\u2019s core logging pipeline.</p>\n\n<p>In this article we are going to talk about:</p>\n\n<ol>\n  <li>\n    <p>Log Aggregation</p>\n  </li>\n  <li>\n    <p>Log Processing and Enrichment</p>\n  </li>\n  <li>\n    <p>Log Buffering and Ingestion</p>\n  </li>\n  <li>\n    <p>Storing Logs</p>\n  </li>\n  <li>\n    <p>Searching Logs</p>\n  </li>\n</ol>\n\n<p><img alt=\"log_pipeline\" class=\"center-image\" id=\"log_pipeline\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_0.png\" /></p>\n\n<h3 id=\"log-aggregation\">Log Aggregation</h3>\n\n<p>WePay uses filebeat for tailing and shipping logs from VM\u2019s and all our GKE microservices. Filebeat guarantees that events will be delivered to the configured output at least once and with no data loss. Filebeat is able to achieve this behavior because it stores the delivery state of each event in the registry file. In situations where the defined output is blocked and has not confirmed all events, Filebeat will keep trying to send events until the output acknowledges that it has received the events.</p>\n\n<p>Filebeat forwards all logs to logstash for more advanced processing and enrichment.</p>\n\n<h3 id=\"log-processing-and-enrichment\">Log Processing and Enrichment</h3>\n\n<p>Logstash filters parse each log event, identify named fields to build structure, and transform them to converge on a common format for more powerful analysis. Logstash helps us in:</p>\n\n<ul>\n  <li>\n    <p>Deriving structure from unstructured data with powerful grok and mutate filters.</p>\n  </li>\n  <li>\n    <p>Decipher geo coordinates from IP addresses.</p>\n  </li>\n  <li>\n    <p>Anonymize PII data, exclude sensitive fields completely from the logs.</p>\n  </li>\n  <li>\n    <p>Ease overall processing, independent of the data source, format, or schema.</p>\n  </li>\n  <li>\n    <p>Convert json log messages into avro format that match an avro schema so then they can be stored in Kafka.</p>\n  </li>\n</ul>\n\n<p>A raw nginx log message can be parsed into something like:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>10.73.1.39 - test [29/May/2021:23:10:31 +0000] \"GET /v1/sys/health HTTP/1.0\" 200 293 \"-\" \"-\" \"-\"\n\nClient IP : 10.73.1.39\nUser : test \nTimestamp : 29/May/2021:23:10:31\nMethod: GET\nURL: /v1/sys/health\nHTTP_Version : 1.0\nResponse : 200\nRequest Bytes : 293\n</code></pre></div></div>\n\n<p>WePay relies on Avro messages because the structure of logs changes over time. If a \u201cstring\u201d field is converted into an \u201cint\u201d field we would have elasticsearch mapping conflicts, Avro schemas help ensure that we don\u2019t have conflicts in our ElasticSchema\u2019s.</p>\n\n<h3 id=\"log-buffering-and-ingestion\">Log Buffering and Ingestion</h3>\n\n<h4 id=\"buffering\">Buffering</h4>\n\n<p>Sometimes applications log at unprecedented levels, during incidents, incorrect logging format, or when there are bugs in the application. These sudden log surges can overwhelm our logging infrastructure. To protect Elasticsearch from such cases of data surges. WePay uses Apache Kafka to buffer logs. In our logging pipeline, logstash forwards all the logs to Kafka in Avro format.</p>\n\n<h4 id=\"ingestion\">Ingestion</h4>\n\n<p>WePay uses light weight Confluent Kafka Connectors for ingesting logs into Elastic/Google BigQuery/Google Cloud Storage Buckets. We use 3 kind of connectors:</p>\n\n<ul>\n  <li>\n    <p><a href=\"https://github.com/confluentinc/kafka-connect-elasticsearch\">Kafka Connect Elasticsearch</a> to ingest logs into our elastic Cluster</p>\n  </li>\n  <li>\n    <p>Our inhouse developed <a href=\"https://github.com/confluentinc/kafka-connect-bigquery\">Google BigQuery Sink Connector</a> to ingest logs into Google\u2019s BigQuery.</p>\n  </li>\n  <li>\n    <p><a href=\"https://docs.confluent.io/kafka-connect-gcs-sink/current/overview.html\">GCS Sink Connector</a> to ingest logs into Google Cloud Storage Buckets.</p>\n  </li>\n</ul>\n\n<p><img alt=\"kafka_connectors\" class=\"center-image\" id=\"kafka_connectors\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_1.png\" /></p>\n\n<h3 id=\"storing-logs\">Storing Logs</h3>\n\n<p>WePay, as a payments company, is subject to <a href=\"https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard\">PCI DSS audits</a>. PCI requirements state that logs must be retained for at least one year. Ninety days of logs must also be available for immediate analysis. We store our logs in 3 places:</p>\n\n<ul>\n  <li>\n    <p><a href=\"https://www.elastic.co/elasticsearch/\">Elasticsearch</a> - 90 days of retention policy for immediate analysis.</p>\n  </li>\n  <li>\n    <p><a href=\"https://cloud.google.com/bigquery\">Google Big Query</a><a href=\"https://cloud.google.com/bigquery\"> </a>- Logs are retained for long-term per regulatory and compliance requirements\nFor analysis when we want to look for historic data ( data not in elastic because of retention policies ). With BigQuery\u2019s powerful query interfaces we can structure and search for logs as needed.</p>\n  </li>\n  <li>\n    <p><a href=\"https://cloud.google.com/storage\">Google Cloud Storage</a> - Logs are retained for long-term per regulatory and compliance requirements. We mainly store logs in GCS to backfill Google\u2019s BQ in times when the connector to BigQuery is broken.</p>\n  </li>\n</ul>\n\n<p>At WePay we have 4 environments: dev, testing, staging and production. Each environment generates a lot of logs. We have two elastic clusters to store these logs:</p>\n\n<ul>\n  <li>\n    <p>A Dev Elastic cluster for development, Testing and staging environment logs.</p>\n  </li>\n  <li>\n    <p>A production Elastic cluster for our production environment logs.</p>\n  </li>\n</ul>\n\n<p>Setting up two elastic clusters had the following advantages:</p>\n\n<ul>\n  <li>\n    <p>If an application in dev or testing environments goes rogue and starts logging at unprecedented levels because of a bug or incorrect logging format, this will impact only our dev cluster.</p>\n  </li>\n  <li>\n    <p>When upgrading elastic to a new version, we can first upgrade the dev cluster; if we encounter any backward compatibility issues, only the dev cluster would be impacted. The production cluster remains safe.</p>\n  </li>\n</ul>\n\n<p><img alt=\"elastic_dev_prd\" class=\"center-image\" id=\"elastic_dev_prd\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_2.png\" /></p>\n\n<h4 id=\"hot-warm-cold-architecture\">Hot Warm Cold Architecture</h4>\n\n<p>With the amount of data that we get, we had to think upstream while designing the data retention and index life cycle management policies.</p>\n\n<p>There are 3 kinds of elastic data nodes setup:</p>\n\n<ul>\n  <li>\n    <p>Hot nodes which will have all new indexes, and will get extensive reads and writes. This is where all new data will be indexed. These are CPU extensive nodes.</p>\n  </li>\n  <li>\n    <p>Warm nodes will host old indexes and will get all reads and no writes.</p>\n  </li>\n  <li>\n    <p>Cold nodes will host old indices and will barely get any reads.</p>\n  </li>\n</ul>\n\n<p><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html\">Elastic\u2019s ILM</a> gives us the ability to manage indexes by triggering actions when conditions are met. With ILM policies we have something like:</p>\n\n<ul>\n  <li>\n    <p>For creating a new index and indexing all data use hot nodes. The ILM policy will start by setting the index priority to a high value for the indices so that hot indexes will recover before other indexes in the event of failures.</p>\n  </li>\n  <li>\n    <p>After the index has completed one week of it\u2019s time in a hot node, merge the number of segments to 1 and move it to the warm node.</p>\n  </li>\n  <li>\n    <p>After the index is completed 1 month of its time in the warm node, freeze the index and move it to the cold node.</p>\n  </li>\n  <li>\n    <p>Delete the index after 3 months.</p>\n  </li>\n</ul>\n\n<h3 id=\"searching---kibana\">Searching - Kibana</h3>\n\n<p>In the previous section, we talked about how we have two elastic clusters. We didn\u2019t want two Kibana\u2019s for searching logs in each cluster, so we decided to go with cross cluster search.</p>\n\n<p>We achieved this by creating a third elasticsearch cluster, that sends requests to the dev and production clusters. This architecture has several benefits:</p>\n\n<ul>\n  <li>\n    <p>All security roles and access permissions for end users are defined in this cluster, hence we avoid creating duplicate security mappings in dev and production logging clusters.</p>\n  </li>\n  <li>\n    <p>Licensed Machine learning nodes run only on this proxy cluster to gather insights from the logs stored in Dev and Production Cluster.</p>\n  </li>\n  <li>\n    <p>A Proxy cluster can help set search thread throttling.</p>\n  </li>\n  <li>\n    <p>An end user in Kibana with superuser permissions won\u2019t be able to make changes to Dev and Production clusters, hence increasing the overall security.</p>\n  </li>\n</ul>\n\n<p><img alt=\"proxy_search\" class=\"center-image\" id=\"proxy_search\" src=\"https://wecode.wepay.com/assets/2021-08-10-wepay-logging-infra/image_3.png\" /></p>\n\n<h3 id=\"future-work\">Future Work</h3>\n\n<ul>\n  <li>\n    <p>We aim at getting APM data from our applications into elasticsearch. This will help us to strongly correlate logging events with APM insights.</p>\n  </li>\n  <li>\n    <p>We plan on setting up <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-ccr.html\">Cross Cluster Replication</a> across multiple regions with the active passive model, and switching between each other during region failures for better fault tolerance.</p>\n  </li>\n  <li>\n    <p>We plan on using <a href=\"https://www.elastic.co/blog/introducing-elasticsearch-frozen-tier-searchbox-on-s3\">Frozen Tier</a> for elastic, for searching for longer duration of logs ( 90+ days ) by referencing storage buckets.</p>\n  </li>\n</ul>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>The elastic clusters have been stress tested to handle upto 80k logs per second. Separating the Dev and Production elastic clusters has helped us protect the Production logging pipeline from Dev environment data surges. The log pipeline to BigQuery makes sure we have longer retention of data at low cost. The Proxy cluster along with Cross Cluster Search has helped us protect the elasticsearch data in Dev and Production clusters by having a single point authenticating and surfacing all search requests.</p>"
    },
    "published": "Tue, 10 Aug 2021 07:00:00 +0000",
    "published_parsed": [
      2021,
      8,
      10,
      7,
      0,
      0,
      1,
      222,
      0
    ],
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://wecode.wepay.com/posts/wepay-logging-infra"
      }
    ],
    "link": "https://wecode.wepay.com/posts/wepay-logging-infra",
    "id": "https://wecode.wepay.com/posts/wepay-logging-infra",
    "guidislink": false,
    "tags": [
      {
        "term": "Elasticsearch",
        "scheme": null,
        "label": null
      }
    ]
  }
}