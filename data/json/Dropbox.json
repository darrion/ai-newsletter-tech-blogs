{
  "company": "Dropbox",
  "title": "Dropbox",
  "xmlUrl": "https://blogs.dropbox.com/tech/feed/",
  "htmlUrl": "https://blogs.dropbox.com/tech/",
  "content": "\n\n\n\nFrom AI to sustainability, why our latest data centers use 400G networking - Dropbox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDropbox.Tech\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopics\n\nApplication\nFront End\nInfrastructure\nMachine Learning\nMobile\nSecurity\nCulture\n\n\n\nDevelopers\n\n\nJobs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\r\n                // Press enter to search\r\n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom AI to sustainability, why our latest data centers use 400G networking\n\n\n// By Daniel Parker\n\u00a0and Amit Chudasma \u2022 Nov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n The case for 400G\n\n\n Fabric core: Zero-optic, energy efficient\n\n\n Top-of-rack interconnect: Backwards compatibility\n\n\n Data center interconnect: Enhanced efficiency, scalability\n\n\n Optical transport: Backbone connectivity\n\n\n What we learned\n\n\n What\u2019s next\n\n\n\n\n\n\nAt Dropbox, AI-powered tools and features are quickly transforming the way our customers find, organize, and understand their data. Dropbox Dash brings AI-powered universal search to all your apps, browser tabs, and cloud docs, while Dropbox AI can summarize and answer questions about the content of your files. To meet the bandwidth requirements of new and future AI workloads\u2014and stay committed to our sustainability goals\u2014the Dropbox networking team recently designed and launched our first data center architecture using highly efficient, cutting edge 400 gigabit per second (400G) ethernet technology.\n400G uses a combination of advanced technologies\u2014such as digital signal processing chips capable of pulse-amplitude modulation and forward error correction\u2014to achieve four times the data rate of its predecessor, 100G, through a single link. Because a single 400G port along with optics is more cost efficient and consumes less power than four individual 100G ports, adopting 400G has enabled us to effectively quadruple the bandwidth in our newest data center while significantly reducing our power usage and cabling footprint. Our new design also streamlines the way our data centers connect to the network backbone, allowing us to realize further cost and energy savings by consolidating what was previously three separate data center interconnect device roles into one.\n400G is a relatively new technology, and has not been as widely adopted by the industry as 100G\u2014though that\u2019s beginning to change. In this story, we\u2019ll discuss why we chose to embark on our 400G journey ahead of the pack, review the design requirements and architectural details of our first 400G datacenter, and touch on some of the challenges faced as early adopters and lessons learned. We\u2019ll conclude with our future plans for continuing to build with this exciting new technology.\n\n\n\n\n\n\n\nA high-level overview of our 400G network architecture\n\n\n\n\n\n The case for 400G\n\n\n\nDropbox has come a long way since launching as a simple file storage company in 2008. We are now a global cloud content platform at scale, providing an AI-powered, multi-product portfolio to our more than 700 million registered users, while also securely storing more than 800 billion pieces of content.\u00a0\nThe Dropbox platform runs on a hybrid cloud infrastructure that encompasses our data centers, global backbone, public cloud, and edge points-of-presence (POPs). To efficiently meet our growing resource needs, the Dropbox hardware team is continuously redesigning our high performance server racks using the latest state-of-the-art components. Recently, these designs reached a critical density where the bandwidth requirements of a server rack are expected to exceed the capabilities of 100G ethernet. For example, our upcoming seventh generation storage servers will require 200G network interface cards (NICs) and 1.6Tb/s of uplink bandwidth per rack in order to meet their data replication SLAs!\u00a0\n\n\n\n\n\n\n\nThe Dropbox platform\u2019s hybrid cloud infrastructure. Our first 400G data center is located in the US-WEST region\n\n\n\n\nWhile we considered trying to scale our 100G-based architecture by using bigger devices with a larger numbers of 100G links, we calculated that, for us, this would be wasteful from a power, cabling, and materials standpoint. We anticipated an inevitable need to upgrade to 400G within the next 24 months at most, and deemed it contrary to our sustainability goals to ship a bandaid 100G architecture comprised of hundreds of devices and thousands of optics, only for them to become e-waste within a year or two.\nOur decision to adopt 400G stemmed from hardware advancements made by our server design team, increasing levels of video and images uploaded to Dropbox, and the growing adoption of our latest product experiences, Dash, Capture, and Replay. Our hardware and storage teams are in the process of finalizing the manufacture of servers that will require network interface speeds of up to 200G per host, and throughput requirements that greatly exceed the 3.2Tb/s switching rate of our current-generation top-of-rack switch.\nOur final design produced efficiency improvements at four sections of our network: the fabric core, the connections to the top-of-rack switches, the data center interconnect routers, and the optical transport shelves.\n\n\n\n Fabric core: Zero-optic, energy efficient\n\n\n\nAt the heart of our 400G data center design, we retained our production-proven quad-plane fabric topology, updated to use 12.8T 32x400G switches in the place of 3.2T 32x100G devices. Sticking with a fabric architecture allowed us to retain the desirable features of our existing 100G design\u2014non-blocking oversubscription rates, small failure domains, and scale-on-demand modularity\u2014while increasing its speed by a factor of four.\u00a0\nCrucially, we were able to do this without expanding our power requirements. We accomplished this by leveraging 400G direct attach copper (DAC) cabling for the dense spine-leaf interconnection links. 400G-DAC is an electrically passive cable that requires virtually no additional power or cooling, so by choosing it we were able to fully offset the increased energy requirements of the faster chips powering the 400G switches themselves.\nComparing power usage metrics from our new 400G fabric core with our legacy 100G data center confirms that the 400G fabric is 3x more energy efficient per Gigabit.\n\n\n\n\n\n\n\nWe based the core of our 400G fabric on the same quad-plane fabric architecture we\u2019ve successfully deployed in various iterations for our past five 100G data center builds, but updated it to use 32x400G devices and extremely energy-efficient 400G-DAC cabling\n\n\n\n\nThe drawbacks of 400G-DAC were its short three meter range and wider cable thickness. We solved for these constraints by meticulously planning (and mocking up in our lab) different permutations of device placement, port assignments, and cable management strategies until we reached an optimal configuration. This culminated in what we call our \u201codd-even split\u201d main distribution frame (MDF) design, pictured below.\n\n\n\n\n\n\n\nA simplified version of our 400G data center MDF racks using 400G-DAC interconnects. Spine switches are stacked in the center rack, connected to leaf switches that are striped evenly between the adjacent racks. Only DAC cables to the first leaf switch in each of the odd (left) and even (right) racks are pictured. This design was repeated four times for each of the data center\u2019s four parallel fabric planes\n\n\n\n\n\n Top-of-rack interconnect: Backwards compatibility\n\n\n\nAnother key architectural component we needed to consider was the optical fiber plant which connects the top-of-rack switches in the data hall to the 400G fabric core. We designed these links based on three requirements:\n\nThe need to support connectivity to both our existing 100G as well as next generation 400G top-of-rack switches\nThe ability to extend these runs up to 500 meters to accommodate multi-megawatt-scale deployments\nThe desire to provide the most reliable infrastructure while optimizing power usage and materials cost\n\nAfter testing various 400G transceivers in this role, we selected the 400G-DR4 optic, which provided the best fit for the three requirements mentioned above:\n\n400G-DR4 can support our existing 100G top-of-rack switches by fanning out to 4x100G-DR links. Its built-in digital signal processor chip is able to convert between 400G and 100G signals without imposing any additional computational costs on the switches themselves.\nThe 400G-DR4 optic has a max range of 500 meters, which meets the distance requirements of even our largest data center facilities.\nAt 8 watts of max power draw per optic, 400G-DR4 is more energy efficient than 4x100G-SR4 optics at 2.5 watts (2.5 * 4 = 10W). 400G-DR4 also runs over single mode fiber, which requires 30% less energy and materials to manufacture than the multi-mode fiber we\u2019ve used in our previous generation 100G architectures.\n\n\n\n\n Data center interconnect: Enhanced efficiency, scalability\n\n\n\nThe data center interconnect (DI) layer has been completely revamped to reflect updates in both bandwidth density and a more powerful, feature-filled networking tier. Today, DI traffic patterns consist of:\u00a0\n\nCross-datacenter traffic between data centers\u00a0\nExternal traffic between data centers and POPs, such as Dropbox customers, cloud storage providers, or corporate networks\n\nPreviously, the network used distinct tiers to manage these traffic types\u2014one tier for cross-datacenter traffic and another tier for external traffic between data centers and POPs. This involved three separate networking devices, pictured below.\n\n\n\n\n\n\n\nOur old data center interconnect design\n\n\n\n\n400G technology enabled us to combine these three devices into a single data center interconnect. At the same time, features such as class-based forwarding\u2014which wasn\u2019t available during the initial tiered design\u2014made it possible to use quality-of-service markings to logically separate traffic over different label-switched paths with the appropriate priorities.\n\n\n\n\n\n\n\nOur new data center interconnect design\n\n\n\n\nThe optimized DI tier offers multiple advantages:\n\nThere is a 60% reduction in the number of devices employed at the tier, resulting in notable improvements in space utilization, energy efficiency, and device cost savings, thereby enhancing the network's environmental and economic sustainability.\nThe new architecture leverages MPLS RSVP TE to replace ECMP, making the data center edge bandwidth-aware, thereby boosting resiliency and efficiency.\nNew architecture allows us to streamline routing by incorporating route aggregation, community tags, and advertising only the default route down to the fabric.\nThe new DI tier seamlessly maintains backward compatibility with 100G-based hardware and technology, enabling us to upgrade specific parts of the network while still leveraging the value of our existing 100G hardware investments.\n\nFurthermore, the adoption of 400G hardware unlocks the potential for the DI to scale up to eight times its current maximum capacity, paving the way for future expansion and adaptability. This comprehensive reimagining of the DI marks a significant stride towards an optimized architecture that prioritizes efficiency, scalability, and reliability.\n\n\n\n Optical transport: Backbone connectivity\n\n\n\nThe optical transport tier is a dense wavelength division multiplexing system (DWDM) that is responsible for all data plane connectivity between the data center and the backbone. Utilizing two strands of fiber optics between the data center and each backbone POP in the metro, the new architecture provides two 6.4 Tb/s tranches of completely diverse network capacity to the data center, for a total of 12.8 Tb/s of available capacity. The system can scale up to 76.8 Tb/s (38.4 Tb/s diverse) before additional dark fiber is required.\nIn comparison, the largest capacity a pair of fiber can carry without this DWDM system is 400 Gb/s.\n\n\n\n\n\n\n\nOne of the two 6.4 Tb/s diverse data center uplinks spans\n\n\n\n\nNew to the optical tier in this generation is the use of 800 Gb/s tuned waves (versus 250 Gb/s in the previous generation) which allows for greatly increased density and significantly lower cost-per-gigabit compared to previous deployments. Additionally, this tier was engineered to afford significant flexibility in the deployment of 100G/400G client links. The multi-faceted nature of this architecture enabled Dropbox to adapt to unexpected delays in equipment deliveries due to commodity shortages, ensuring on-time turn-up of our 400G data center.\n\n\n\n What we learned\n\n\n\nSince its launch in December 2022, our first 400G data center has been serving Dropbox customers at blazingly fast speeds, with additional facilities slated to come online before the end of 2023. But as with any new technological development, adopting 400G forced us to overcome new obstacles and chart new paths along the way.\u00a0\nHere are some lessons learned from our multi-year journey to this point:\n\nMeticulously test all components. Since every 400G router, switch, cable, and optic in our design was one of the first of its kind to be manufactured, our team recognized the need to evaluate each product\u2019s ability to perform and interoperate in a multi-vendor architecture. To this end, we designed a purpose-built 400G test lab equipped with a packet generator capable of emulating future-scale workloads, and physically and logically stress-tested each component.\nEnsure backwards compatibility at the 400G-100G boundary. We discovered in testing that a 100G top-of-rack switch we deploy extensively in our production environment was missing support for the 100G-DR optic we\u2019d selected to connect our existing 100G top-of-rack switches to the new 400G fabric. Fortunately, we were able to surface the issue early enough to request a patch from the vendor to add support for this optic.\nHave contingency plans for supply chain headwinds. During our design and build cycle for 400G, unpredictability in the global supply chain was an unfortunate reality. We mitigated these risks by qualifying multiple sources for each component in our design. When the vendor supplying our 400G DI devices backed out one month before launch due to a chip shortage, the team rapidly developed a contingency plan. Because 400G QSFP-DD ports are backwards compatible with 100G QSFP28 optics, we devised a temporary interconnect strategy using 100G devices in the DI role until their permanent 400G replacements could be swapped in.\n\n\n\n\n What\u2019s next\n\n\n\nThe successful launch of our first 400G data center has given us the confidence needed to continue rolling out 400G technology to other areas of the Dropbox production network. 400G data centers based on this same design are slated to launch in US-CENTRAL and US-EAST by the end of 2023. Test racks of our 7th generation servers with 400G top-of-rack switches are already running in US-WEST and will be deployed at scale in early 2024. We also plan on extending 400G to the Dropbox backbone throughout 2024 and 2025.\nFinally, an emerging long-haul optical technology called 400G-ZR+ promises to deliver even greater efficiency gains. With 400G-ZR+, we can replace our existing 12-foot-high optical transport shelves with a pluggable transceiver the size of a stick of gum!\u00a0\u00a0\n\n\n\n\n\n\n\nDaniel King, one of our data center operations technicians, holds a pluggable transceiver in front of the equipment it will eventually replace.\n\n\n\n\n~ ~ ~\nIf building innovative products, experiences, and infrastructure excites you, come build the future with us! Visit dropbox.com/jobs to see our open roles, and follow @LifeInsideDropbox on Instagram and Facebook to see what it's like to create a more enlightened way of working.\u00a0\n\n\n\n\n\n\n\r\n            // Tags \n\n\n\r\n                        Infrastructure\r\n                    \n\nHardware\nTraffic\nAI\ndata center\nNetworking\n400G\nsustainability\n\n\n\n\n\n\n\n\n\n\r\n            // Copy link \nLink copied\n\n\n\n\n\n\n\nLink copied\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\r\n                Related posts\r\n            \n\r\n                See more\r\n            \n\n\n\n\r\n                    How the data center site selection process works at Dropbox\r\n                \n\r\n                    // Jun 13, 2023\r\n                \n\n\n\r\n                    After four years of SMR storage, here's what we love\u2014and what comes next\r\n                \n\r\n                    // Mar 08, 2023\r\n                \n\n\n\r\n                    Balancing quality and coverage with our data validation framework\r\n                \n\r\n                    // Feb 07, 2023\r\n                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Jobs\n          \n\n\n\n            Medium\n          \n\n\n\n            Privacy\n          \n\n\n\n            twitter\n          \n\n\n\n            Terms\n          \n\n\n\n            Instagram\n          \n\n\n\n            Work In Progress\n          \n\n\n\n            RSS feed\n          \n\n\n\n            Cookies & CCPA preferences\n          \n\n\n\n            Engineering Career Framework\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "title": "From AI to sustainability, why our latest data centers use 400G networking",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://dropbox.tech/feed",
      "value": "From AI to sustainability, why our latest data centers use 400G networking"
    },
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://dropbox.tech/infrastructure/from-ai-to-sustainability-why-our-latest-data-centers-use-400g-networking"
      }
    ],
    "link": "https://dropbox.tech/infrastructure/from-ai-to-sustainability-why-our-latest-data-centers-use-400g-networking",
    "authors": [
      {
        "name": "Daniel Parker,Amit Chudasma"
      }
    ],
    "author": "Daniel Parker,Amit Chudasma",
    "author_detail": {
      "name": "Daniel Parker,Amit Chudasma"
    },
    "tags": [
      {
        "term": "Hardware",
        "scheme": null,
        "label": null
      },
      {
        "term": "Traffic",
        "scheme": null,
        "label": null
      },
      {
        "term": "AI",
        "scheme": null,
        "label": null
      },
      {
        "term": "Infrastructure",
        "scheme": null,
        "label": null
      },
      {
        "term": "data center",
        "scheme": null,
        "label": null
      },
      {
        "term": "Networking",
        "scheme": null,
        "label": null
      },
      {
        "term": "400G",
        "scheme": null,
        "label": null
      },
      {
        "term": "sustainability",
        "scheme": null,
        "label": null
      }
    ],
    "summary": "null",
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://dropbox.tech/feed",
      "value": "null"
    },
    "id": "https://dropbox.tech/infrastructure/from-ai-to-sustainability-why-our-latest-data-centers-use-400g-networking",
    "guidislink": false,
    "published": "Tue, 14 Nov 2023 06:00:00 -0800",
    "published_parsed": [
      2023,
      11,
      14,
      14,
      0,
      0,
      1,
      318,
      0
    ],
    "content": [
      {
        "type": "text/html",
        "language": null,
        "base": "https://dropbox.tech/feed",
        "value": "<div class=\"aem-Grid aem-Grid--12 aem-Grid--default--12 \">\n    \n    <div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>At Dropbox, AI-powered tools and features are quickly transforming the way our customers find, organize, and understand their data. <a href=\"https://dropbox.com/dash\" target=\"_blank\">Dropbox Dash</a> brings AI-powered universal search to all your apps, browser tabs, and cloud docs, while Dropbox AI can summarize and answer questions about the content of your files. To meet the bandwidth requirements of <a href=\"https://blog.dropbox.com/topics/company/updated-tools-new-plans-and-web-redesign\" target=\"_blank\">new and future AI workloads</a>\u2014and stay committed to our <a href=\"https://blog.dropbox.com/topics/company/dropbox-sets-sustainability-goals-for-2030\" target=\"_blank\">sustainability goals</a>\u2014the Dropbox networking team recently designed and launched our first data center architecture using highly efficient, cutting edge 400 gigabit per second (400G) ethernet technology.</p>\n<p>400G uses a combination of advanced technologies\u2014such as digital signal processing chips capable of pulse-amplitude modulation and forward error correction\u2014to achieve four times the data rate of its predecessor, 100G, through a single link. Because a single 400G port along with optics is more cost efficient and consumes less power than four individual 100G ports, adopting 400G has enabled us to effectively quadruple the bandwidth in our newest data center while significantly reducing our power usage and cabling footprint. Our new design also streamlines the way our data centers connect to the network backbone, allowing us to realize further cost and energy savings by consolidating what was previously three separate data center interconnect device roles into one.</p>\n<p>400G is a relatively new technology, and has not been as widely adopted by the industry as 100G\u2014though that\u2019s beginning to change. In this story, we\u2019ll discuss why we chose to embark on our 400G journey ahead of the pack, review the design requirements and architectural details of our first 400G datacenter, and touch on some of the challenges faced as early adopters and lessons learned. We\u2019ll conclude with our future plans for continuing to build with this exciting new technology.</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram1.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram1.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1263\"\n             data-sly-attribute.height=\"578\"\n             data-aem-asset-id=\"31f12ffb-19f4-4629-8b80-6bc85d69fb08:Diagram1.jpg\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram1.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram1.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1263\"\n             data-sly-attribute.height=\"578\"\n             data-aem-asset-id=\"31f12ffb-19f4-4629-8b80-6bc85d69fb08:Diagram1.jpg\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"578\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram1.jpg/_jcr_content/renditions/Diagram1.webp\" width=\"1263\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">A high-level overview of our 400G network architecture</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"section aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-article-content__section\" id=\"-the-case-for-400g\">\n    <h2 class=\"dr-article-content__section-title\"> The case for 400G</h2>\n</div>\n</div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>Dropbox has come a long way since launching as a simple file storage company in 2008. We are now a global cloud content platform at scale, providing an AI-powered, multi-product portfolio to our more than 700 million registered users, while also securely storing more than 800 billion pieces of content.\u00a0</p>\n<p>The Dropbox platform runs on a hybrid cloud infrastructure that encompasses our data centers, global backbone, public cloud, and <a href=\"https://dropbox.tech/infrastructure/dropbox-traffic-infrastructure-edge-network\" target=\"_blank\">edge points-of-presence (POPs)</a>. To efficiently meet our growing resource needs, the Dropbox hardware team is continuously redesigning our <a href=\"https://dropbox.tech/infrastructure/sixth-generation-server-hardware\" target=\"_blank\">high performance server racks</a> using the latest state-of-the-art components. Recently, these designs reached a critical density where the bandwidth requirements of a server rack are expected to exceed the capabilities of 100G ethernet. For example, our upcoming seventh generation storage servers will require 200G network interface cards (NICs) and 1.6Tb/s of uplink bandwidth per rack in order to meet their data replication SLAs!\u00a0</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram2-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram2-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"912\"\n             data-aem-asset-id=\"93ee6563-d2d1-42d3-91e6-fe8f7654633c:Diagram2-720xauto.png\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram2-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram2-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"912\"\n             data-aem-asset-id=\"93ee6563-d2d1-42d3-91e6-fe8f7654633c:Diagram2-720xauto.png\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"912\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram2-720xauto.png/_jcr_content/renditions/Diagram2-720xauto.webp\" width=\"1440\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">The Dropbox platform\u2019s hybrid cloud infrastructure. Our first 400G data center is located in the US-WEST region</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>While we considered trying to scale our 100G-based architecture by using bigger devices with a larger numbers of 100G links, we calculated that, for us, this would be wasteful from a power, cabling, and materials standpoint. We anticipated an inevitable need to upgrade to 400G within the next 24 months at most, and deemed it <a href=\"https://dropbox.tech/infrastructure/making-dropbox-data-centers-carbon-neutral\" target=\"_blank\">contrary to our sustainability goals</a> to ship a bandaid 100G architecture comprised of hundreds of devices and thousands of optics, only for them to become e-waste within a year or two.</p>\n<p>Our decision to adopt 400G stemmed from hardware advancements made by our server design team, increasing levels of video and images uploaded to Dropbox, and the growing adoption of our latest product experiences, <a href=\"https://www.dropbox.com/dash\" target=\"_blank\">Dash</a>, <a href=\"https://www.dropbox.com/capture\" target=\"_blank\">Capture</a>, and <a href=\"https://www.dropbox.com/replay\" target=\"_blank\">Replay</a>. Our hardware and storage teams are in the process of finalizing the manufacture of servers that will require network interface speeds of up to 200G per host, and throughput requirements that greatly exceed the 3.2Tb/s switching rate of our current-generation top-of-rack switch.</p>\n<p>Our final design produced efficiency improvements at four sections of our network: the fabric core, the connections to the top-of-rack switches, the data center interconnect routers, and the optical transport shelves.</p>\n\n</div>\n<div class=\"section aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-article-content__section\" id=\"-fabric-core-zero-optic-energy-efficient\">\n    <h2 class=\"dr-article-content__section-title\"> Fabric core: Zero-optic, energy efficient</h2>\n</div>\n</div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>At the heart of our 400G data center design, we retained our <a href=\"https://dropbox.tech/infrastructure/the-scalable-fabric-behind-our-growing-data-center-network\" target=\"_blank\">production-proven quad-plane fabric topology</a>, updated to use 12.8T 32x400G switches in the place of 3.2T 32x100G devices. Sticking with a fabric architecture allowed us to retain the desirable features of our existing 100G design\u2014non-blocking oversubscription rates, small failure domains, and scale-on-demand modularity\u2014while increasing its speed by a factor of four.\u00a0</p>\n<p>Crucially, we were able to do this without expanding our power requirements. We accomplished this by leveraging 400G direct attach copper (DAC) cabling for the dense spine-leaf interconnection links. 400G-DAC is an electrically passive cable that requires virtually no additional power or cooling, so by choosing it we were able to fully offset the increased energy requirements of the faster chips powering the 400G switches themselves.</p>\n<p>Comparing power usage metrics from our new 400G fabric core with our legacy 100G data center confirms that the 400G fabric is 3x more energy efficient per Gigabit.</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram3.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram3.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1138\"\n             data-sly-attribute.height=\"373\"\n             data-aem-asset-id=\"cfbc99e5-2b1b-4ffb-a2cf-aa78fc58858f:Diagram3.jpg\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram3.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram3.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1138\"\n             data-sly-attribute.height=\"373\"\n             data-aem-asset-id=\"cfbc99e5-2b1b-4ffb-a2cf-aa78fc58858f:Diagram3.jpg\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"373\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram3.jpg/_jcr_content/renditions/Diagram3.webp\" width=\"1138\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">We based the core of our 400G fabric on the same quad-plane fabric architecture we\u2019ve successfully deployed in various iterations for our past five 100G data center builds, but updated it to use 32x400G devices and extremely energy-efficient 400G-DAC cabling</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>The drawbacks of 400G-DAC were its short three meter range and wider cable thickness. We solved for these constraints by meticulously planning (and mocking up in our lab) different permutations of device placement, port assignments, and cable management strategies until we reached an optimal configuration. This culminated in what we call our \u201codd-even split\u201d main distribution frame (MDF) design, pictured below.</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram4-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram4-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"1298\"\n             data-aem-asset-id=\"3af158c8-beb0-4cc5-87ef-790b9d583d9c:Diagram4-720xauto.png\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram4-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram4-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"1298\"\n             data-aem-asset-id=\"3af158c8-beb0-4cc5-87ef-790b9d583d9c:Diagram4-720xauto.png\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"1298\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram4-720xauto.png/_jcr_content/renditions/Diagram4-720xauto.webp\" width=\"1440\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">A simplified version of our 400G data center MDF racks using 400G-DAC interconnects. Spine switches are stacked in the center rack, connected to leaf switches that are striped evenly between the adjacent racks. Only DAC cables to the first leaf switch in each of the odd (left) and even (right) racks are pictured. This design was repeated four times for each of the data center\u2019s four parallel fabric planes</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"section aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-article-content__section\" id=\"-top-of-rack-interconnect-backwards-compatibility\">\n    <h2 class=\"dr-article-content__section-title\"> Top-of-rack interconnect: Backwards compatibility</h2>\n</div>\n</div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>Another key architectural component we needed to consider was the optical fiber plant which connects the top-of-rack switches in the data hall to the 400G fabric core. We designed these links based on three requirements:</p>\n<ul>\n<li>The need to support connectivity to both our existing 100G as well as next generation 400G top-of-rack switches</li>\n<li>The ability to extend these runs up to 500 meters to accommodate multi-megawatt-scale deployments</li>\n<li>The desire to provide the most reliable infrastructure while optimizing power usage and materials cost</li>\n</ul>\n<p>After testing various 400G transceivers in this role, we selected the 400G-DR4 optic, which provided the best fit for the three requirements mentioned above:</p>\n<ul>\n<li>400G-DR4 can support our existing 100G top-of-rack switches by fanning out to 4x100G-DR links. Its built-in digital signal processor chip is able to convert between 400G and 100G signals without imposing any additional computational costs on the switches themselves.</li>\n<li>The 400G-DR4 optic has a max range of 500 meters, which meets the distance requirements of even our largest data center facilities.</li>\n<li>At 8 watts of max power draw per optic, 400G-DR4 is more energy efficient than 4x100G-SR4 optics at 2.5 watts (2.5 * 4 = 10W). 400G-DR4 also runs over single mode fiber, which requires 30% less energy and materials to manufacture than the multi-mode fiber we\u2019ve used in our previous generation 100G architectures.</li>\n</ul>\n\n</div>\n<div class=\"section aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-article-content__section\" id=\"-data-center-interconnect-enhanced-efficiency-scalability\">\n    <h2 class=\"dr-article-content__section-title\"> Data center interconnect: Enhanced efficiency, scalability</h2>\n</div>\n</div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>The data center interconnect (DI) layer has been completely revamped to reflect updates in both bandwidth density and a more powerful, feature-filled networking tier. Today, DI traffic patterns consist of:\u00a0</p>\n<ul>\n<li><b>Cross-datacenter traffic</b> between data centers\u00a0</li>\n<li><b>External traffic </b>between data centers and POPs, such as Dropbox customers, cloud storage providers, or corporate networks</li>\n</ul>\n<p>Previously, the network used distinct tiers to manage these traffic types\u2014one tier for cross-datacenter traffic and another tier for external traffic between data centers and POPs. This involved three separate networking devices, pictured below.</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram5-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram5-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"1290\"\n             data-aem-asset-id=\"1e302615-4cde-4c47-8fe9-1d1b246e18aa:Diagram5-720xauto.png\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram5-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram5-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"1290\"\n             data-aem-asset-id=\"1e302615-4cde-4c47-8fe9-1d1b246e18aa:Diagram5-720xauto.png\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"1290\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram5-720xauto.png/_jcr_content/renditions/Diagram5-720xauto.webp\" width=\"1440\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">Our old data center interconnect design</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>400G technology enabled us to combine these three devices into a single data center interconnect. At the same time, features such as class-based forwarding\u2014which wasn\u2019t available during the initial tiered design\u2014made it possible to use quality-of-service markings to logically separate traffic over different label-switched paths with the appropriate priorities.</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram6-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram6-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"1174\"\n             data-aem-asset-id=\"a1375c8d-310b-4aad-afd6-751e1942c36a:Diagram6-720xauto.png\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram6-720xauto.png 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram6-720xauto.png\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1440\"\n             data-sly-attribute.height=\"1174\"\n             data-aem-asset-id=\"a1375c8d-310b-4aad-afd6-751e1942c36a:Diagram6-720xauto.png\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"1174\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram6-720xauto.png/_jcr_content/renditions/Diagram6-720xauto.webp\" width=\"1440\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">Our new data center interconnect design</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>The optimized DI tier offers multiple advantages:</p>\n<ul>\n<li>There is a 60% reduction in the number of devices employed at the tier, resulting in notable improvements in space utilization, energy efficiency, and device cost savings, thereby enhancing the network's environmental and economic sustainability.</li>\n<li>The new architecture leverages MPLS RSVP TE to replace ECMP, making the data center edge bandwidth-aware, thereby boosting resiliency and efficiency.</li>\n<li>New architecture allows us to streamline routing by incorporating route aggregation, community tags, and advertising only the default route down to the fabric.</li>\n<li>The new DI tier seamlessly maintains backward compatibility with 100G-based hardware and technology, enabling us to upgrade specific parts of the network while still leveraging the value of our existing 100G hardware investments.</li>\n</ul>\n<p>Furthermore, the adoption of 400G hardware unlocks the potential for the DI to scale up to eight times its current maximum capacity, paving the way for future expansion and adaptability. This comprehensive reimagining of the DI marks a significant stride towards an optimized architecture that prioritizes efficiency, scalability, and reliability.</p>\n\n</div>\n<div class=\"section aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-article-content__section\" id=\"-optical-transport-backbone-connectivity\">\n    <h2 class=\"dr-article-content__section-title\"> Optical transport: Backbone connectivity</h2>\n</div>\n</div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>The optical transport tier is a dense wavelength division multiplexing system (DWDM) that is responsible for all data plane connectivity between the data center and the backbone. Utilizing two strands of fiber optics between the data center and each backbone POP in the metro, the new architecture provides two 6.4 Tb/s tranches of completely diverse network capacity to the data center, for a total of 12.8 Tb/s of available capacity. The system can scale up to 76.8 Tb/s (38.4 Tb/s diverse) before additional dark fiber is required.</p>\n<p>In comparison, the largest capacity a pair of fiber can carry <i>without</i> this DWDM system is 400 Gb/s.</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram7.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram7.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1173\"\n             data-sly-attribute.height=\"282\"\n             data-aem-asset-id=\"118dbf24-fda2-422d-b766-f5ca8c3819b6:Diagram7.jpg\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram7.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram7.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1173\"\n             data-sly-attribute.height=\"282\"\n             data-aem-asset-id=\"118dbf24-fda2-422d-b766-f5ca8c3819b6:Diagram7.jpg\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"282\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/Diagram7.jpg/_jcr_content/renditions/Diagram7.webp\" width=\"1173\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">One of the two 6.4 Tb/s diverse data center uplinks spans</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>New to the optical tier in this generation is the use of 800 Gb/s tuned waves (versus 250 Gb/s in the previous generation) which allows for greatly increased density and significantly lower cost-per-gigabit compared to previous deployments. Additionally, this tier was engineered to afford significant flexibility in the deployment of 100G/400G client links. The multi-faceted nature of this architecture enabled Dropbox to adapt to unexpected delays in equipment deliveries due to commodity shortages, ensuring on-time turn-up of our 400G data center.</p>\n\n</div>\n<div class=\"section aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-article-content__section\" id=\"-what-we-learned\">\n    <h2 class=\"dr-article-content__section-title\"> What we learned</h2>\n</div>\n</div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>Since its launch in December 2022, our first 400G data center has been serving Dropbox customers at blazingly fast speeds, with additional facilities slated to come online before the end of 2023. But as with any new technological development, adopting 400G forced us to overcome new obstacles and chart new paths along the way.\u00a0</p>\n<p>Here are some lessons learned from our multi-year journey to this point:</p>\n<ul>\n<li><b>Meticulously test all components. </b>Since every 400G router, switch, cable, and optic in our design was one of the first of its kind to be manufactured, our team recognized the need to evaluate each product\u2019s ability to perform and interoperate in a multi-vendor architecture. To this end, we designed a purpose-built 400G test lab equipped with a packet generator capable of emulating future-scale workloads, and physically and logically stress-tested each component.</li>\n<li><b>Ensure backwards compatibility at the 400G-100G boundary. </b>We discovered in testing that a 100G top-of-rack switch we deploy extensively in our production environment was missing support for the 100G-DR optic we\u2019d selected to connect our existing 100G top-of-rack switches to the new 400G fabric. Fortunately, we were able to surface the issue early enough to request a patch from the vendor to add support for this optic.</li>\n<li><b>Have contingency plans for supply chain headwinds. </b>During our design and build cycle for 400G, unpredictability in the global supply chain was an unfortunate reality. We mitigated these risks by qualifying multiple sources for each component in our design. When the vendor supplying our 400G DI devices backed out one month before launch due to a chip shortage, the team rapidly developed a contingency plan. Because 400G QSFP-DD ports are backwards compatible with 100G QSFP28 optics, we devised a temporary interconnect strategy using 100G devices in the DI role until their permanent 400G replacements could be swapped in.</li>\n</ul>\n\n</div>\n<div class=\"section aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-article-content__section\" id=\"-whats-next\">\n    <h2 class=\"dr-article-content__section-title\"> What\u2019s next</h2>\n</div>\n</div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p>The successful launch of our first 400G data center has given us the confidence needed to continue rolling out 400G technology to other areas of the Dropbox production network. 400G data centers based on this same design are slated to launch in US-CENTRAL and US-EAST by the end of 2023. Test racks of our 7th generation servers with 400G top-of-rack switches are already running in US-WEST and will be deployed at scale in early 2024. We also plan on extending 400G to the Dropbox backbone throughout 2024 and 2025.</p>\n<p>Finally, an emerging long-haul optical technology called 400G-ZR+ promises to deliver even greater efficiency gains. With 400G-ZR+, we can replace our existing 12-foot-high optical transport shelves with a pluggable transceiver the size of a stick of gum!\u00a0\u00a0</p>\n\n</div>\n<div class=\"image c04-image aem-GridColumn aem-GridColumn--default--12\">\n<div class=\"dr-image image cq-dd-image  \">\n    <figure class=\"dr-margin-0 dr-display-inline-block\">\n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        <!--optimized image webp-->\n        \n\n        \n        <!-- <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/danielking-400g-zrplus.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/danielking-400g-zrplus.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1280\"\n             data-sly-attribute.height=\"920\"\n             data-aem-asset-id=\"bcb6d405-833f-4177-b6df-71579d418617:danielking-400g-zrplus.jpg\"\n             data-trackable=\"true\" />\n        <img data-sly-test.highRes=\"false\"\n             srcset=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/danielking-400g-zrplus.jpg 2x,  1x\"\n             src=\"/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/danielking-400g-zrplus.jpg\"\n             aria-hidden=\"\"\n             alt=\"\"\n             class=\"\"\n             data-sly-attribute.width=\"1280\"\n             data-sly-attribute.height=\"920\"\n             data-aem-asset-id=\"bcb6d405-833f-4177-b6df-71579d418617:danielking-400g-zrplus.jpg\"\n             data-trackable=\"true\" /> -->\n\n        \n         \n        <img alt=\"\" height=\"920\" src=\"https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/danielking-400g-zrplus.jpg/_jcr_content/renditions/danielking-400g-zrplus.webp\" width=\"1280\" />\n    \n\n            <figcaption class=\"dr-typography-t5 dr-color-ink-60 dr-image-rte\"><p style=\"text-align: center;\">Daniel King, one of our data center operations technicians, holds a pluggable transceiver in front of the equipment it will eventually replace.</p>\n</figcaption>\n        \n    </figure>\n</div></div>\n<div class=\"text parbase aem-GridColumn aem-GridColumn--default--12\">\n<p style=\"text-align: center;\">~ ~ ~</p>\n<p><i>If building innovative products, experiences, and infrastructure excites you, come build the future with us! Visit </i><a href=\"https://dropbox.com/jobs\" target=\"_blank\"><i><u>dropbox.com/jobs</u></i></a><i> to see our open roles, and follow @LifeInsideDropbox on </i><a href=\"https://www.instagram.com/lifeinsidedropbox/?hl=en\" target=\"_blank\"><i><u>Instagram</u></i></a><i> and </i><a href=\"https://www.facebook.com/lifeinsidedropbox/\" target=\"_blank\"><i><u>Facebook</u></i></a><i> to see what it's like to create a more enlightened way of working.\u00a0</i></p>\n\n</div>\n\n    \n</div>"
      }
    ],
    "media_thumbnail": [
      {
        "url": "https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/400GNetworking-1440x305-light.png"
      }
    ],
    "href": "",
    "media_content": [
      {
        "url": "https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2023/11/400g/400GNetworking-1440x305-light.png",
        "medium": "image"
      }
    ]
  }
}