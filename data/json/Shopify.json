{
  "company": "Shopify",
  "title": "Shopify",
  "xmlUrl": "https://shopify.engineering/blog.atom",
  "htmlUrl": "https://shopify.engineering/",
  "content": "\n\n\n\n\n\n\n\n\nHorizontally scaling the Rails backend of Shop app with Vitess (2024)\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nSkip to Content\n\n\n\nShopify  \n \n\nEngineering BlogWorking at ShopifyOpen Source at Shopify\u00a0  Dev Degree\u00a0  \n\n\nSee open roles\n\n\nOpen Main Navigation  \n\n\n\n\n\n\nShopify Engineering\n\n\n\n\n      Show\n    \n  \n\n\nLatest articlesDevelopmentInfrastructureMobileDeveloper ToolingSecurityData Science & EngineeringCulture\n\n\n\n\n\n\n\n\nHome  \n \n\nClose Main Navigation  \n \n\n\nEngineering BlogWorking at ShopifyOpen Source at Shopify\u00a0  Dev Degree\u00a0  \n\n\nSee open roles\n\n\n\n\n\nOpens in a new windowOpens an external siteOpens an external site in a new window\n\n\n\n\n\n\n\n\nHorizontally scaling the Rails backend of Shop app with Vitess\n\n\n\n\n            by Hammad Khalid\n\n\n\n\nJan 17, 2024\n\n\n          28 minute read\n        \n\n\n\n\n  Email\n  Facebook\n  Twitter\n  LinkedIn\n\n\n\n\n\n\nGood problems\nWe experienced hockey stick growth after we launched the Shop app.\u00a0We were glued to our dashboards and saw millions of users onboard onto the app. This was gratifying, but we were becoming more nervous as our backend was pushed closer to its limit.\nWe wrote the backend in Ruby on Rails, and used a MySQL database (Shopify managed system called KateSQL). The first order of business was to identify the bottlenecks. We iterated, horizontally scaling our background job system, caching system, and used a horizontally scaled Message bus where appropriate. We then invested into detecting the usual suspects: slow queries, limited connections etc. We also dropped the  \u201cThe\u201d\u00a0from  \u201cThe Shop app\u201d because it was cleaner.\nAs MySQL is the primary datastore, this would become the main bottleneck as well. To deal with this, we started off by splitting the primary database into separate parts \u2013 Rails makes it easy to interact\u00a0with multiple databases. We identified groups of large tables that could exist on separate databases, and used GhostFerry\u00a0to move a few tables onto a new database. We also created new databases for tables in separate domains \u2013 this had the added benefit of limiting the blast radius of issues with one domain impacting others.\nAs the app further grew, we were starting to hit the limit of a single MySQL. We started planning for the next phase of our growth. We were holding off on sharding the primary database as we were weary of the complexity it would add. As time went on, the disk size grew to many Terabytes, schema migrations would take weeks, and we were throttling more background jobs when the database was too busy. We couldn\u2019t further split the primary database, as that would add more complexity in the application layer, and require cross database transactions. We had reached a point where incremental enhancements were no longer sufficient. It was time to overhaul our approach to scaling.\nPreparing for Vitess\nThere are many different approaches to scaling your system, and each one has tradeoffs. Below are some examples:\n\n\nMulti-tenant architecture: When Shopify horizontally scaled, the whole system was divided with a \u201cPod Architecture\u201d (Blog post with details). Each \u201cPod\u201d has its own dedicated resources including MySQL. This allowed Shopify to horizontally scale, and gain the benefits of a system where issues with one merchant do not impact the others. Shop app has different properties than the rest of Shopify. While Shopify has millions of merchants, Shop app has an order of magnitude more users. The impact of one user is small compared to the impact of a large merchant.\n\n\nMove frequent writes to Key-value stores: Another approach is to move everything that\u2019s frequently updated over to a horizontally scaled key-value store and use MySQL for the core data that\u2019s rarely updated. The downside of this approach is that we lose out on the ability to define custom indexes, as key-value stores are typically only fetched by the primary key. We also want to stick with SQL as the primary query language for datastores. There are exceptions to this, where we store some data in memory-stores, but they do not provide the durability guarantees that we need.\n\nStore data in separate MySQLs: This scaling approach is referred to as \u201cfederation\u201d where tables are stored in different MySQLs. This was our initial approach, and it took us quite far. Rails makes it really easy to work with multiple databases at the same time (docs). The downsides of this approach are: 1) Schema migrations take a really long time on larger tables, 2) Joining tables efficiently across different databases is challenging, 3) The application layer becomes more complicated as developers have to consider where each table is stored.\n\n\nHorizontally scaled systems provided by Cloud providers: We opted not to go with this approach, as we prefer to run our own primary datastores at Shopify. Since our use-case was a mature Rails app, It was also important for us to stick to a system that was SQL compatible.\n\nAfter exploring various options, we found that Vitess stood out significantly compared to the others.\nWhy Vitess?\nVitess is an open source database system abstraction on top of MySQL that provides many benefits (docs with details):\n\n\nIt allows you to shard and re-shard MySQL (docs)\n\nIt makes it easy to coordinate schema migrations across many shards (docs)\n\nIt provides connection pooling for queries, as well as some protection from bad queries.\nIt is mostly compatible with the SQL query language (docs).\n\nSee full list of of features here\u00a0and the philosophy behind it here.\n\n\nWe picked Vitess as it provided many of the features we needed. Before we dive into our approach to Vitess, let\u2019s look at the basic terminology used in this post.\nTerminology\nShard\n\nA shard is a subset of a database. In our case, it is composed of a single writer and multiple replicas.\n\nKeyspace\n\nA keyspace is a logical database that represents a set of tables stored on one or more shards. A keyspace can be sharded (multiple shards) or unsharded (single shard). Sharded keyspaces require a sharding key to be defined for each table which determines what shard a row will be stored on.\n\nVSchema\n\n\nA VSchema, or Vitess Schema, describes how keyspaces and shards are organized. It helps Vitess route queries to appropriate shards, and coordinate the more advanced features (docs).\n\n\nVTGate\n\n\nWith Vitess, the query path looks like App \n\u2192 VTGate \n\u2192 VTTablet \n\u2192 MySQL . VTGate is a proxy that performs query planning, transaction coordination, and query routing to the VTTablet(s). VTGate also keeps track of all VSchemas.\n\n\nVTTablet\n\nVTTablet is deployed on the same machine as the MySQL server and it's responsible for many features including connection pooling.\n\nChoosing our sharding key\nThe first thing before any other consideration was choosing our sharding key. Most of the tables in our database are associated with a user, so naturally user_id was chosen as the sharding key. All the data would move to Vitess, but only\u00a0the user-owned-data was to be sharded. This data would become the \"users\" keyspace and the rest would become the unsharded \"global\" keyspace. Note that while Vitess allows tables in a keyspace to be sharded by multiple sharding keys, we opted to enforce that the \u201cusers\u201d keyspace only contains user-owned-tables. This makes it easier to test, and reason about, the system.\nRe-organizing the data model\nBefore starting the work to migrate the Vitess, we had to ensure all the tables associated with user data actually had a user_id column. Unfortunately our data was modeled in a way where a lot of the data belonged to an Account (which belonged to a User) and many of the tables did not have a user_id column, only an account_id.\nWe had to run a number of migrations to add the user_id column and backfill the data. Both the migration and the backfill were time consuming processes since these tables were very large (with billions of rows) and the database was operating at capacity during peak hours. Takeaway: do this early, it may take longer than expected.\nSetting up query verifiers\nWe built query verifiers in the application layer that helped us find queries that would not be compatible with Vitess. These query verifiers were critical for migrating to a sharded database in Vitess. They validated query correctness, routing, and data distribution, preventing query errors, performance issues caused by cross shard queries and data inconsistencies caused by partially committed transactions. If we could credit any single thing to a successful move to Vitess, it would be the correctness of the verifiers.\nWe had set up the verifiers in a way, where we could represent the future state of the system. For example, we could represent that some tables would exist in a keyspace that did not exist at the time. We could also represent that a keyspace would be sharded in the future.\nTypes of Verifiers\n\nMissing Sharding Key: This verifier was essential in making sure that queries against sharded tables incorporated the sharding key in the WHERE clause. By including the sharding key, Vitess could route the query to the correct shards.\nCross Database Transaction: This verifier ensured that all write queries executed within a transaction were performed on the same database, irrespective of whether it was in Vitess or not. It achieved this by examining the connections used for the write queries within the transaction.\n\nCross Shard Transaction: This verifier ensured that all write queries executed within a transaction, focusing on the sharded keyspace in Vitess, were performed on the same shard with the matching sharding key. The goal was to prevent partial commits on shards in case of a transaction rollback. We limited transactions to update only a single user's data, ensuring consistency and averting potential complications with distributed updates across multiple shards. The default atomicity model of Vitess allows cross shard transactions (docs). Consider the case where a cross-shard transaction makes changes to SHARD_1 and SHARD_2. If the change is committed to SHARD_1, but errors out on SHARD_2, the change on SHARD_1 will not be rolled back. We opted to remove all cross shard transactions to avoid any issues.\n\nCross Shard Write: This verifier served a similar role as the Cross Shard Transaction Verifier but concentrated on queries that were not explicitly within a transaction. It specifically inspected calls such as update_all() or delete_all() that operated outside of a transaction.\nCross Keyspace Query: This verifier identified queries that would include tables in multiple keyspaces. It identified violations that took place when a table from one keyspace was joined with a table from another keyspace. For example, if a table in KEYSPACE_1 was joined with a table in KEYSPACE_2, this verifier would flag it as a violation.\n\n\nApplying verifiers to production code \n\nWe implemented the rollout of verifiers in separate steps to ensure their correctness. This involved multiple rounds of thorough testing, including unit tests and manual tests covering various query types and edge cases. Additionally, we created a list of offending queries, allowing us to enable the verifiers early for newly added code while gradually addressing existing violations.\nInitially, we enabled the verifiers in the development and test environments. This meant that any newly added code would fail in these environments if there were any verifier violations. We also ran a copy of the CI pipeline with Vitess as the backend in parallel with the MySQL database, enabling us to catch new query violations during the CI process.\nTo address the violations, we first worked through our list of offending queries. The main changes made to the queries included ensuring the inclusion of the sharding key wherever possible. We also rewrote many queries to be Vitess compliant, adding patches to automatically inject the sharding key into queries when appropriate.\nFor transactions that crossed between the sharded and unsharded keyspaces, we carefully split them up and analyzed their atomicity to ensure partial failures could self-heal. Similarly, we reworked complicated flows with transactions that crossed between shards, by rewriting them to avoid unsupported operations on Vitess. For example, UPDATE <table_name> SET <sharding_key>= ? WHERE id = ? would fail once we sharded,\u00a0as this required\u00a0moving the row to a different shard. We\u00a0turned this into\u00a0a two step process, where\u00a0we first inserted a\u00a0new row\u00a0in the destination shard, and then removed the old row from the source shard.\nTo allow certain queries to skip verifiers under specific conditions, we implemented some \"Danger\" helper methods. These methods were useful for scenarios like running maintenance tasks across different users or performing cross-shard queries when user information was not available.\nFinally, we enabled the verifiers in production but with only log level instead of raising exceptions to avoid unexpected disruptions. To gain better visibility into possible violations, we generated a weekly report that highlighted the total violations occurring in production and their corresponding callsites. This allowed us to investigate any potential violations missed during testing or any edge cases that were not covered by the verifiers.\nEnsuring that sharding key is included in all queries\nNormally, MySQL uses the SQL query to plan how to search through the tables and indexes. Specifically, it uses columns in the WHERE clause to figure out the optimal indexes. With sharding, and Vitess specifically, there comes another layer to this, where Vitess (the VTGate proxy) must first inspect the query and route it to the appropriate shards. Some queries work on Vitess, like SELECT * FROM orders WHERE id = ?, but forces Vitess to send the query to all shards (scatter query), and then combine the result. If we slightly adjust the query to SELECT * FROM orders WHERE user_id = ? AND id = ?, Vitess can determine that this data can only be present on a specific shard, the one that houses the data for that user.\nUsing the verifiers described above, we could ensure all queries that target tables in the sharded keyspace also include the sharding key. For some queries this is simple, but it\u2019s harder for queries with complex joins and table renames. With the verifiers and the log of violating queries, we found a few common query patterns that we needed to fix:\n\nThe initial load of some set of data.\nLoad of additional association data.\nMutations of data.\n\nWe were working with Rails 7.0 at the time. We had realized that we would need to add patches to ActiveRecord (a core abstraction component in Rails that handles objects whose data requires persistent storage). We wanted to avoid patches as that makes it really difficult to maintain Rails over the long term. Alas, these would be temporary as in a future version of Rails, the Composite keys (docs) would make this a lot easier.\nWe started off by creating an abstraction that allowed us to identify the sharding key for any given ActiveRecord object. After that, we created an ActiveRecord patch so the sharding key from an object would then be passed down to update, delete, lock/reload statements. By default rails has_many, belongs_to, and has_one relationships produce queries that Vitess cannot scope to a single shard. We created a join_condition option, which with some patches, passed the sharding key down to the association.\nSchema migrations\nBeing able to run database migrations quickly was one of the reasons for sharding the database - some of our largest tables would take many weeks to migrate. This was becoming a major pain point and slowed down product development and introduced tech debt.\nVitess supports different migration strategies including vitess (native) and gh-ost (from Github). The vitess migration strategy seemed solid at first and comes with a number of advantages, for example the migration can survive database failovers and be reverted instantly. During our initial experimentation we were running Vitess V14, where the vitess strategy was still experimental. Testing with larger tables we started to experience some issues. When migration was throttled for longer than 10 minutes (due to replication lag for example), migrations would get terminated. We raised this issue on the Vitess Community slack, the community was responsive and helpful. They confirmed the bug and submitted a bugfix.\u00a0Today, vitess is the recommended strategy and this is what we use in production with V15 (editor's note: An unrelated lesson is that if you are in a restaurant\u00a0that has the word \u2018Pizza\u2019 in their name, you should probably order their Pizza).\nTo manage the migrations we built a custom UI. Vitess comes with a number of SQL commands to\u00a0view/manage\u00a0migration, which we leveraged for this.\nSchema caches\nWe rely on the\u00a0schema cache\u00a0feature in Rails to prevent the database from being overloaded when loading schema on boot during deployments. Previously, we had a hook that would be called when the migration was completed, to dump the newly updated schema - which would then be picked up on the next deployment. Vitess does not have an easy way to hook into migrations from the application code. We built a background job to do that in the application layer. The job would queue when the migration is submitted to Vitess and periodically check the status of migrations. When the migration is complete on all shards and there are no other active migrations running, it would dump the new schema. This was necessary as Rails could connect to any of the shards when querying the table schema, potentially leading to an inconsistent schema being returned if migration is completed on some shards but not all.\nPhase 1: \u201cVitessifying\u201d\nStart state: The main database of Shop app was running a regular MySQL database (Shopify managed system called KateSQL), and all connections went through a proxy called ProxySQL.\nEnd state: Shop app was running a Vitessified MySQL with a single unsharded users keyspace, that contains all of the tables, and all connections go through VTGate.\n\nVitessifying is, our internal terminology for, the process of transforming an existing MySQL into a keyspace in a Vitess cluster. This allows us to start using core Vitess functionality without explicitly moving data. The core changes were:\u00a0\n\n\nAdd a VTTablet process alongside each mysqld process (docs). The VTTablet is configured to serve as the sole shard of a new keyspace. We run these on the same host, communicating over a socket, but technically they can be run on separate hosts communicating over a network connection. Note that you'll need to allocate fairly significant resources to the VTTablet. Vitess' rule of thumb is an equal number of CPUs for VTTablet and mysqld (source) though memory consumption for VTTablet is generally quite low.\n\n\nEnsure that the new keyspace is accessible via VTGates(docs).\n\n\n\nThis process required no downtime and was completely transparent to the application.\nSafely switching connections\nAfter Vitessifying our main database, we proceeded to add the capability in our application code to establish a connection to the underlying database through VTGate.\nSince this was Shopify's initial deployment of Vitess in a production environment, we adopted a cautious approach during the rollout. Our main objective was to guarantee that the implementation would not cause any irreversible or substantial impact on the system.\u00a0We implemented a dynamic connection switcher that granted us control\u00a0over throughout\u00a0during the rollout procedure. This switcher was integrated at the application layer, leveraging a staged rollout primitive that we had previously developed.\nThe dynamic connection switcher allowed us to modify the routing of requests, giving us the ability to choose whether they would be directed through our original SQL connections (ProxySQL) or via VTGate. This level of control enabled us to carefully manage the percentage of requests that would be routed through Vitess. By gradually increasing the traffic going through Vitess, we were able to closely monitor its performance and promptly address any issues or unexpected behaviors that arose.\n\nTo minimize risks, we initially started the connection switch with components in production that posed the least risk. For example, we targeted background jobs that had built-in mechanisms for automatic retries in case of any errors during job processing. This approach ensured that we could safely test and validate the new VTGate connection without the risk of data loss or disruption to critical processes. Once fully rolled out, the ProxySQLs were removed.\nPhase 2: Splitting into multiple keyspaces\nStart state: Shop app was running a Vitessified MySQL with a single unsharded keyspace.\nEnd state: Shop app had three unsharded keyspaces: global, users, and configuration.\nAfter Vitessifying in Phase 1, all of our tables were stored in the same keyspace as a preliminary step. Now it was time to split these tables into their appropriate future keyspaces. We decided on the following split:\n\nUsers: This is the keyspace with all user related data.\n\nGlobal: This is the keyspace for data that\u2019s not owned by a user.\n\nConfiguration: This keyspace is tables that are rarely written to. It would also be used for Sequence tables in Phase 3.\n\n\nMoving tables between keyspaces.\nVitess provides a MoveTables workflow (docs) which makes it easy to move tables between keyspaces. Before we kicked this off in production, we practiced every step in a staging environment. Practicing thoroughly in staging is a lesson we learned early on, and this consistently turned out to be correct as we discovered some bugs with Vitess and issues with our setup. We prepared a big checklist, and this list included commands to bail out of the process if we ran into issues.\nGetting ready to do the same in production, we blocked schema migrations to avoid any issues during the operation. We also disabled schema caches to be safe and to prevent issues where Rails could raise errors if we query tables that are not present in the schema cache. After that, we created the two new global and configuration keyspaces.\u00a0We had previously made a list of tables that we knew needed to be moved over to other keyspaces. We had also previously created a \u201cCross Keyspace Query\u201d verifier which allowed us to identify and remove any future cross-keyspace queries. We had a separate entry for each keyspace in Rails's database.yml file, treating each keyspace as a separate database.\nDuring our practice in staging, we discovered and worked through a few issues. We found that erroneous or canceled operations could leave behind journal entries and artifacts which could interfere with future operations. We had to clean those up before attempting a second round. Once we gained more confidence, we started moving tables in production.\nOnce all table data had been moved over, we performed Vdiffs to verify the integrity of the move (docs). In addition, we performed manual checks including verifying that the collation and character_set remained the same. We then switched traffic, and completed the operation using the --keep_data --keep_routing_rules options. We didn\u2019t want to remove the tables from the source keyspace, as dropping large tables could stall the database (this is an issue with MySQL 5.7). We renamed the tables in the old keyspace to include a \u201c_old\u201d suffix to their names, and then removed the routing rules.\nPhase 3: Sharding the \u201cUsers\u201d keyspace.\nStart state: Shop app had three unsharded keyspaces: global, users, and configuration.\nEnd state:\u00a0Shop app has one sharded users keyspace, two unsharded global and configuration keyspaces, and one sharded lookup keyspace for Lookup Vindexes.\nVitess is very similar to a regular MySQL if you stick with unsharded keyspaces. You don\u2019t really need to manage a VSchema which defines how your keyspace is sharded (docs). Once you shard, this is where the real complexity is introduced. Before we begin sharding, we have two major prerequisites to take care of: Sequences and Vindexes.\n\nSequences for Auto-incrementing primary IDs\nRails apps default to creating tables with an integer primary ID that\u2019s auto incremented. That does not work in a sharded system, where the primary ID needs to be unique across the shards. Sequences in Vitess play the role of auto_increment, ensuring monotonically incrementing IDs that are unique across shards.\nSequences in Vitess are themselves backed by a regular MySQL table in an unsharded keyspace (docs). This is a mandatory requirement as we want a single entity to be responsible for coordinating incrementing the primary ID, and a distributed solution would introduce issues when there are network issues.\nThe VTTablet is responsible for reserving and caching a block of ids from the Sequences table, and this reduces the writes required to the underlying table by the magnitude of the cache. In our production environment, we've set this cache value to 1000. The cache value is a critical parameter that needs careful consideration. It needs to be large enough to ensure that MySQL doesn't become a bottleneck during a spike of writes and can handle small periods of downtime of the underlying MySQL. At the same time, it needs to be small enough to ensure that no large block of IDs are \"lost\" anytime the VTTablet process is restarted or stopped.\nGetting existing tables to start using Sequences was tricky. Before using Sequence tables, we needed to update the next_id column in the Sequence tables to be bigger than the current max id of each table. We did this with application layer code that was thoroughly tested and more defensive than normal. The three steps were: 1) Identifying the current max id of a table, 2) Updating the related Sequence table with max_id plus a large buffer, 3) Updating the VSchema so Vitess would start using the Sequence table for auto-incrementing.\nVindexes for maintaining global uniqueness and reducing cross shard queries\nOnce a keyspace is sharded, there are two extra considerations: Row uniqueness and cross-shard queries. If you want to ensure that a row is unique, adding a unique MySQL index is not enough anymore, as that only guarantees uniqueness per shard. Moreover, if the sharding key is not passed to a SQL query, a cross-shard query will be performed. Cross-shard queries are not bad if done infrequently, but they can cause issues at higher scale.\nLookup Vindexes are used to solve both of these problems. Lookup Vindexes are backed by MySQL tables that are maintained by Vitess. When a row is inserted, updated or deleted, Vitess also makes the corresponding changes in the lookup table. This allows Vitess to perform a lookup in the Lookup Vindex table to determine if a row is unique across shards. There are many different types of Vindex (docs) which one can pick from. \nFor our purposes, we had to pick between\u00a0 consistent_lookup_unique and the older\u00a0lookup_unique\u00a0Vindex which is slower and requires a two-phase commit.\u00a0We opted to only use consistent_lookup_unique Vindexes to enforce global uniqueness across shards. We created these Lookup Vindexes before sharding, as sharding without them would break the uniqueness guarantees.\u00a0As we gained more experience, we landed on the philosophy of avoiding Lookup Vindexes unless necessary.\u00a0\nOther than the complexity there are two major downsides: 1) Writes are slower, because Vitess does extra work to maintain the Lookup Vindex table, 2) The test suite becomes slower, as we have to run non-transactional tests when updating a column that's backed by a consistent_lookup\u00a0Vindex. This is due to a limitation with consistent_lookup Vindexes, where an insert followed by an update or delete in the same transaction for the same consistent lookup column value is not supported (docs). To avoid this issue, we considered an approach where we would use the slower\u00a0lookup_unique Vindex in the development and test environments. We opted not to do that, because we wanted parity between the development and production environments \u2013 there was a risk where code that would fail in production would not fail CI.\u00a0Running a few non-transactional tests is not ideal so we are going to revisit this approach.\nLuckily, Lookup Vindexes are not required for enforcing cross shard uniqueness for two cases: 1) The key is a randomly generated collision safe key (e.g., UUID), or 2) One column in a unique MySQL index only exists in a single shard. To illustrate, consider the example where all tables are sharded by the user_id. A unique MySQL index like [\"user_id\", \"name\"] will be globally unique, as a single shard will contain all of the rows for a distinct user_id, and the uniqueness of the \u201cname\u201d column will be guaranteed within that shard. Likewise if a \u201cProduct\u201d table is also sharded by a user_id, then a distinct product_id could only exist in the shard that contains the user. In this case, a unique MySQL index like [\"product_id\", \"line_item_id\"] will be globally unique as well. This works because all of the data for a user is kept in the same shard, and the MySQL unique index enforces uniqueness within that shard. Lookup Vindexes won\u2019t be needed here. See the example below. \n Lookup Vindexes can be created in any keyspace. We opted to create a separate sharded keyspace for the consistent_lookup_unique Vindexes because of the following reasons: 1) We wanted to gain more experience with a sharded keyspace, before actually sharding the users keyspace, 2) We wanted to enforce that all tables in the users keyspace had the correct sharding id, 3) We wanted to improve our observability and tooling, 4) We didn't want a single unsharded database to become a bottleneck for inserts into the sharded keyspace.\nAdding more shards\nWe now had the required pieces to add more shards. This last step was also the riskiest one. There were going to be many moving parts, and everything needed to go perfectly when switching from a single database to multiple shards. From our previous experience moving tables, we realized that this was not going to be easy. We were the first ones at Shopify so we would have to pay some early adopter tax.\nWe started off by organizing a week-long hackathon between all members of the project. The goal was to properly understand Vindexes, Sequences, and actually sharding the users keyspace. We went through the gauntlet but pushed through. All in all, we ran into around 25 bugs. Some of these were bugs in Vitess itself (e.g, 1, 2, 3, 4, 5, 6) while others were bugs at the application and internal-infrastructure layers. One gains a healthy skepticism for moving parts after an exercise like this.\nAs we gained more confidence we were prepared to shard the production database. Similar to the MoveTables operation, we disabled schema caches and migrations before getting started. We had disabled Vitess tablet throttler (docs) as it caused some issues while switching traffic in our tests. We then created new shards that matched the specs as the source shard. We kicked off the Reshard workflow (docs). The whole process took about a week of time. It is worth noting that we saw a lot of pauses while the data was being copied over, as the replication flow would throttle when MySQL history list length (HLL) was too high. The replication flow would resume as the HLL came back down.\nAfter copying the data we ran VDiffs to verify the integrity of the move. When executing VDiff, we saw HLL grow to over 1 million over about a 15 minute period. We also saw that VDiff would run into transient connection errors, however it typically recovers from them automatically. VDiff uses a series of single long running queries on the source and target tablets. When we ran VDiff on a larger table we ran into an error where it actually stopped the workflow. The workflow resumed after we stopped the VDiff. On a positive note, we found the workflow to be very resilient to failure (VReplication). It was able to survive an unplanned failover, and a configuration change which required replacing the instance.\nAfter completing the VDiffs, we switched replica reads to point to the new shards and looked for issues. Given our intense focus on the query verifiers, we didn\u2019t see major issues after switching reads. We waited a few hours and then finally switched primary reads and writes. At this point, writes were going to the new shards, and replication back to the original source shard. Roughly an hour later, the reverse replication stopped with the following error:\nDuplicate entry REDACTED for key 'index_name_on_table' (errno 1062) (sqlstate 23000) during query: insert into table_name(<REDACTED>) values (REDACTED)\nThis issue happened because of a complex flow, which requires updating, inserting, and deleting a record in short succession: UPDATE row_1 \n\u2192 INSERT row_2 \n\u2192 DELETE row_1.\u00a0This worked on on the single source shard, but broke when row_1 and row_2 were on separate shards. There are no guarantees about the ordering of events from different shards, so the insert could appear before the update, which violates the uniqueness constraint. Vitess has a feature to minimize the skew but that only mitigates the issue and does not eliminate it (docs). We were replicating from replica tablets and this might have reduced the skew by eliminating MySQL replication lag as a factor, but we don't know if it would have been enough to prevent the race condition entirely. Given that we were not seeing any issues with the new shards, we decided to move forward with completing the reshard operation.\nCleanup\nNow that we had sharded the users keyspace, we spent some time on cleaning up. First, we realized that running schema migrations on a sharded system has a few more edge cases that we needed to consider. Each shard would complete the schema migration at different times. To reduce the impact of this, we require that all added or removed columns are included in the ignored_columns list in Rails (docs). This ensures that these columns are not references in SQL queries. We also run schema migrations with the --singleton flag, so we only run one schema migration at a time per keyspace.\nAnother cleanup task was to remove the MySQL auto-increment from all tables. Now that Sequences are responsible for auto-increment, we did not need or want MySQL to be responsible for auto-increment. So we ran schema migrations on all tables. This reduces the chance of a very-low-probability but high-impact failure mode, where shards use the MySQL auto-increment instead of the Sequence tables for any reason.\nParting thoughts\nHorizontally scaling our primary datastore has unlocked unrestricted growth. It has also pulled us out of a loop of constant marginal optimizations to make the previous setup work. Overall, we are very happy with how Vitess has impacted our system. Schema migrations take hours instead of weeks. We no longer hit capacity issues, where background jobs would throttle when the database was pushed to its limits (replica lag, mysql threads running etc). Best of all, we now simply add more shards to further scale.\nThe cost of this is added complexity. For developers to effectively use Vitess, they do have to learn a few more abstractions. We compare Vitess abstraction to MySQL indexes. A developer building on top of MySQL should know about setting up indexes. Let\u2019s consider some of the major considerations if you are switching over to Vitess.\nMajor considerations for Vitess\nVitess has many different features (docs). If we just stick to the features that most Rails-like applications would need, that leaves us with Sequences, and Vindexes and VSchemas. These are concepts that your team would need a basic understanding of.\n\nSequences are required to coordinate auto-incrementing primary IDs across shards. Rails applications default to auto-incrementing primary IDs, so you\u2019ll most likely need these. Luckily, they are fairly simple.\n\nPrimary Vindexes define how data is distributed to shards (i.e., the sharding key). Developers will need to understand this in order to avoid cross-shard transactions.\n\nLookup Vindexes enforce uniqueness across shards and reduce cross-shard queries.\n\nVSchemas describe how each keyspace is organized. VSchemas are required for sharded keyspaces.\n\nLessons from migrating to Vitess\nIf you read through this whole article, you may be considering switching over to Vitess. Here are the core lessons that might help you along the way:\n\nPick a sharding strategy earlier than you think. After we picked ours, re-organizing the data model, and then backfilling huge tables was really tedious and time consuming. Ideally, you should set up linters in advance to enforce that all new tables must have this non-nullable sharding key.\nYou must practice important steps in a staging environment. Vitess is powerful but the failure modes are scary. Set up a great staging environment and practice every single step before attempting it in production (we had two staging environments). We discovered some bugs along the way, and it\u2019s likely that you will too. You should also have an approach for creating dummy data in the staging environment at high volumes, as this would help you identify potential issues.\nYou should heavily invest in query verifiers. Remove cross keyspace/shard transactions. Reduce cross keyspace/shard queries. Maintain a list of queries and reduce them as much as possible. Triple check the validity of your verifiers.\n\nWhat\u2019s next\nOur next steps are to stabilize and simplify. We found a few issues while switching over to Vitess. Some of these were bugs with Vitess, while the others are related to our specific setup. We are planning on ironing these out.\u00a0Once we upgrade to Rails v7.1, which\u00a0introduces composite keys (docs), we\u2019ll remove most of our custom patches\u00a0and align our approach with the future of Rails.\n\nThis blog was co-authored by: \nHammad Khalid,\u00a0Senior Staff\u00a0Developer (LinkedIn, X),\nVahe Khachikyan,\u00a0Senior Developer (LinkedIn),\u00a0\nHanying (Elsa) Huang,\u00a0Staff Developer\u00a0(LinkedIn),\u00a0\nThibault Gautriaud, Staff Developer (LinkedIn),\nAdam Renberg Tamm, Principal\u00a0Engineer (LinkedIn), and\u00a0\nBrendan Dougherty,\u00a0Staff Production Engineer (LinkedIn).\n\n\n\n  Email\n  Facebook\n  Twitter\n  LinkedIn\n\n  Get stories like this in your inbox!Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.Email addressYes, sign me up!\n\nShare your email with us and receive monthly updates.\nThanks for subscribing.\nYou\u2019ll start receiving free tips and resources soon.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch articles\n\nSearch\n\n\n  Get stories like this in your inbox!Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.Email addressYes, sign me up!\n\nShare your email with us and receive monthly updates.\nThanks for subscribing.\nYou\u2019ll start receiving free tips and resources soon.\n\n\n\n\n\n  \n    Resources\n  \n\n\n            Our Tech Stack\n Curious about what\u2019s in our tech stack.\n\n            Sponsorship\n We\u2019re looking to partner with you.\n\n            Working Anywhere at Shopify\n Learn about Digital by Design\n\n            Shopify Partner Developers\n Become a Shopify developer and earn money by building apps or working with businesses\n\n            Shopify Engineering on Twitter\n Connect with us on Twitter\n\n            Shopify Engineering YouTube\n Connect with us on YouTube\n\n\n\n\n  \n    Popular\n  \n\n\n            Migrating our Largest Mobile App to React Native\n \n            Ruby 3.2\u2019s YJIT is Production-Ready\n \n            From Ruby to Node: Overhauling Shopify\u2019s CLI for a Better Developer Experience\n \n            The 25 Percent Rule for Tackling Technical Debt\n \n            The Case Against Monkey Patching, From a Rails Core Team Member\n \n            10 Tips for Building Resilient Payment Systems\n \n            How Good Documentation Can Improve Productivity\n \n            Five Common Data Stores and When to Use Them\n \n            Deconstructing the Monolith: Designing Software that Maximizes Developer Productivity\n \n\n\n\n  \n    Latest\n  \n\n\n            Horizontally scaling the Rails backend of Shop app with Vitess\n \n            Getting Started with React Native Skia\n \n            Introducing Ruvy\n \n            Building a ShopifyQL Code Editor\n \n            Sidekick\u2019s Improved Streaming Experience\n \n            Shopify\u2019s platform is the Web platform\n \n            Contributing support for a Wasm instruction to Winch\n \n            Creating a Flexible Order Routing System with Shopify Functions\n \n            Adventures in Garbage Collection: Improving GC Performance in our Massive Monolith\n \n            How Migrating from Vanilla Redux to Redux Toolkit Improved State Management in Shopify POS\n \n\n\n\n\n\n\n\n\nReady to tackle frontend, backend, infrastructure, data, or security challenges?\n\nExplore all of our available roles\u00a0  \n\n\n\n\n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "id": "https://shopify.engineering/horizontally-scaling-the-rails-backend-of-shop-app-with-vitess",
    "guidislink": true,
    "link": "https://shopify.engineering/horizontally-scaling-the-rails-backend-of-shop-app-with-vitess",
    "published": "2024-01-17T15:34:53-05:00",
    "published_parsed": [
      2024,
      1,
      17,
      20,
      34,
      53,
      2,
      17,
      0
    ],
    "updated": "2024-01-17T15:34:53-05:00",
    "updated_parsed": [
      2024,
      1,
      17,
      20,
      34,
      53,
      2,
      17,
      0
    ],
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://shopify.engineering/horizontally-scaling-the-rails-backend-of-shop-app-with-vitess"
      }
    ],
    "title": "Horizontally scaling the Rails backend of Shop app with Vitess",
    "title_detail": {
      "type": "text/plain",
      "language": "en",
      "base": "https://shopify.engineering/blog.atom",
      "value": "Horizontally scaling the Rails backend of Shop app with Vitess"
    },
    "authors": [
      {
        "name": "Hammad Khalid"
      }
    ],
    "author_detail": {
      "name": "Hammad Khalid"
    },
    "author": "Hammad Khalid",
    "summary": "<span>Shop app horizontally scaled a Ruby on Rails app with Vitess. This blog describes Vitess and our detailed approach for introducing Vitess to a Rails app.</span><p><a class=\"read-more\" href=\"https://shopify.engineering/horizontally-scaling-the-rails-backend-of-shop-app-with-vitess\">More</a></p>",
    "summary_detail": {
      "type": "text/html",
      "language": "en",
      "base": "https://shopify.engineering/blog.atom",
      "value": "<span>Shop app horizontally scaled a Ruby on Rails app with Vitess. This blog describes Vitess and our detailed approach for introducing Vitess to a Rails app.</span><p><a class=\"read-more\" href=\"https://shopify.engineering/horizontally-scaling-the-rails-backend-of-shop-app-with-vitess\">More</a></p>"
    }
  }
}