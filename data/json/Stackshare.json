{
  "company": "Stackshare",
  "title": "Stackshare",
  "xmlUrl": "https://stackshare.io/featured-posts.atom",
  "htmlUrl": "https://stackshare.io/feed",
  "content": "\n\n\n\n\nOptimizing Pinterest\u2019s Data Ingestion Stack: Findings and Learnings - Pinterest Tech Stack\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunityEnterpriseTech Stack FileSign up/Login\n\n\n\n\n\n\n\n\n\n\n\n\nMore Stories\nMore\n\n\n\n\nFeatured\n\n\nOptimizing Pinterest\u2019s Data Ingestion Stack: Findings and Learnings\n\n\nBy\u00a0\n\nPinterest\n\n\n\n\n\n1,234\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPinterest\n\n\n\n\n\nPinterest's profile on StackShare is not actively maintained, so the information here may be out of date.\n\n\nSocial Media\nE-Commerce\nSocial Commerce\nCurated Web\n\n\nView Profile\n\n\n\n\n\n\n\nBy Ping-Min Lin | Software Engineer, Logging Platform\n\nAt Pinterest, the Logging Platform team maintains the backbone of data ingestion infrastructure that ingests terabytes of data per day. When building the services powering these pipelines, it is extremely important that we build efficient systems considering how widespread and deep in the stack the systems are. Along our journey of continuous improvement, we\u2019ve figured out basic but useful patterns and learnings that could be applied in general \u2014 and hopefully for you as well.\nMemQ: Achieving memory-efficient batch data delivery using Netty\nMemQ is the next-gen data ingestion platform built in-house and recently open-sourced by the Logging Platform team. When designing the service, we tried hard to maximize the efficiency of our resources, specifically, we focused on reducing GC by using off-heap memory. Netty was chosen as our low-level networking framework due to its great balance between flexibility, performance, and sophisticated out-of-the-box features. For example, we used ByteBuf heavily throughout the project. ByteBufs are the building blocks of data within Netty. They are similar to Java NIO ByteBuffers, but allow the developers much more control of the lifecycle of the objects by providing a \u201csmart pointer\u201d approach for customized memory management using manual reference counting. By using ByteBufs, we managed to transport messages with a single copy of data by passing off-heap network buffer pointers, further reducing cycles used on garbage collection.\n\nThe typical journey of a message in the MemQ broker: Each message received from the network will be reconstructed via a length-encoded protocol that will be allocated into a ByteBuf that is off of the JVM heap (direct memory in Netty terms), and will be the only existing copy of the payload throughout the whole pipeline. This ByteBuf reference will be passed into the topic processor and put into a Batch along with other messages that are also waiting to be uploaded to the storage destination. Once the upload constraints are met, either due to the time threshold or the size threshold, the Batch will be dispatched. In the case of uploading to a remote object store like S3, the whole batch of messages will be kept in a CompositeByteBuf (which is a virtual wrapper ByteBuf consisting of multiple ByteBufs) and uploaded to the destination using the netty-reactor library, allowing us to create no additional copies of data within the processing path. By building on top of ByteBufs and other Netty constructs, we were able to iterate rapidly without sacrificing performance and avoid reinventing the wheel.\nSinger: Leveraging asynchronous processing to reduce thread overheads\nSinger has been around at Pinterest for a long time, reliably delivering messages to PubSub backends. With more and more use cases onboarded to Singer, we\u2019ve started to hit bottlenecks on memory usage that led to frequent OOM issues and incidents. Singer has memory and CPU resources constrained on nearly all fleets at Pinterest to avoid impact on the host service e.g. our API serving layer. After inspecting the code and leveraging debugging tools such as VisualVM, Native Memory Tracking (NMT), and pmap, we noticed various potential improvements to be done, most notably reducing the number of threads. After performing NMT result analysis we noticed the number of threads and the memory used by the stack as a result of these threads (allocated due to the Singer executor and producer thread pools).\nTaking a deeper look into the source of these threads, the majority of these threads come from the thread pools for each Kafka cluster Singer publishes to. The threads in these thread pools are used to wait for Kafka to complete writing messages to a partition and then report the status of the writes. While the threads do the job, each thread in the JVM (by default) will allocate 1MB of memory used for the thread\u2019s stack.\n\nA Singer NMT report showing the different memory regions a JVM process allocates. The Thread entry represents the thread stack. Arena contains the off-heap/direct memory portion managed outside of the JVM heap.\nEven with lazy allocation of the stack memory on the underlying operating systems until the thread is actually used, this still quickly adds up to hundreds of MBs of the process\u2019 memory. When there are a lot of log streams publishing to multiple partitions on different clusters, the memory used by thread stacks can be easily comparable to the 800MB default heap size of Singer and eats into the resources of the application.\n\nEach submission of KafkaWriteTask will occupy a thread. Full code can be found here\nBy closely examining the usage of these threads, it quickly becomes clear that most of these threads are doing non-blocking operations such as updating metrics and are perfectly suitable for asynchronous processing using CompletableFutures provided starting in Java 8. The CompletableFuture allows us to resolve the blocking calls by chaining stages asynchronously, thus replacing the usage of these threads that had to wait until the results to come back from Kafka. By utilizing the callback in the KafkaProducer.send(record, callback) method, we rely on the Kafka producer\u2019s network client to be completely in control of the multiplexing of networking.\n\nA brief example of the result code after using CompletableFutures. Full code can be found here\nOnce we convert the original logic into several chained non-blocking stages, it becomes obvious to use a single common thread pool to handle them regardless of the logstream, so we use the common ForkJoinPool that is already at our disposal from JVM. This dramatically reduces the thread usage for Singer, from a couple of hundred threads to virtually no additional threads. This improvement demonstrates the power of asynchronous processing and how network-bound applications can benefit from it.\nKafka and Singer: Balancing performance and efficiency with controllable variance\nOperating our Kafka clusters has always been a delicate balance between performance, fault tolerance, and efficiency. Our logging agent Singer, at the front line of publishing messages to Kafka, is a crucial component that plays a heavy role in these factors, especially in routing the traffic by deciding which partitions we deliver data to for a topic.\nThe Default Partitioner: Evenly Distributed Traffic\nIn Singer, logs from a machine would be picked up and routed to the corresponding topic it belongs to and published to that topic in Kafka. In the early days, Singer would publish uniformly to all the partitions that topic has in a round-robin fashion using our default partitioner. For example, if there were 3000 messages on a particular host that needed to be published to a 30 partition topic, each partition would roughly receive 100 messages. This worked pretty well for most of the use cases and has a nice benefit where all partitions receive the same amount of messages, which is great for the consumers of these topics since the workload is evenly distributed amongst them.\n\nDefaultPartitioner: Producers and Partitions are fully connected\nThe Single Partition Partitioner: In Favor of the Law of Large Numbers\n\nSinglePartitionPartitioner: Ideal scenario where connections are evenly distributed\nAs Pinterest grew, we had fleets expanding to thousands of hosts, and this evenly-distributed approach started to cause some issues to our Kafka brokers: high connections counts and large amounts of produce requests started to elevate the brokers\u2019 CPU usage, and spreading out the messages means that the batch sizes are smaller for each partition, or lower efficiency of the compression, resulting in higher aggregated network traffic. To tackle this, we implemented a new partitioner: the SinglePartitionPartitioner. This partitioner solves the issue by forcing Singer to only write to one random partition per topic per host, reducing the fanout from all brokers to a single broker. This partition remains the same throughout the producer\u2019s lifetime until Singer restarts.\nFor pipelines that had a large producer fleet and relatively uniform message rates across hosts, this was extremely effective: The law of large numbers worked in our favor, and statistically, if the number of producers is significantly larger than partitions, each partition will still receive a similar amount of traffic. Connection count went down from (number of brokers serving the topic) times (number of producers) to only (number of producers), which could be up to a hundred times less for larger topics. Meanwhile, batching up all messages per producer to a single partition improved compression ratios by at least 10% in most use cases.\n\nSinglePartitionPartitioner: Skewed scenario where there are too few producers vs. partitions\nThe Fixed Partitions Partitioner: Configurable variance for adjusting trade-offs\nDespite coming up with this new solution, there were still some pipelines that lie in the middle ground where both solutions are subpar, such as when the number of producers is not large enough to outnumber the number of partitions. In this case, the SinglePartitionPartitioner would introduce significant skew between partitions: some partitions will have multiple producers writing to them, and some are assigned very few or even no producers. This skew could cause unbalanced workloads for the downstream consumers, and also increases the burden for our team to manage the cluster, especially when storage is tight. We thus recently introduced a new partitioner that can be used on these cases, and even cover the original use cases: the FixedPartitionsPartitioner, which basically allows us to not only publish to one fixed partition like the SinglePartitionPartitioner, but randomly across a fixed number of partitions.\nThis approach is somewhat similar to the concept of virtual nodes in consistent hashing, where we artificially create more \u201ceffective producers\u201d to achieve a more continuous distribution. Since the number of partitions for each host can be configured, we can tune it to the sweet spot where the efficiency and performance are both at desired levels. This partitioner could also help with \u201chot producers\u201d by spreading traffic out while still maintaining a reasonable connection count. Although a simple concept, it turns out that having the ability to configure the degree of variance could be a powerful tool to manage trade-offs.\n\nFixedPartitionsPartitioner: Less skew while still keeping connection count lower than the default\n\nRelative compression ratio and request rate skew with different numbers of fixed partitions on a 120 partition topic on 30 brokers\nConclusion and Acknowledgements\nThese learnings are just a few examples of improvements the Logging Platform team has been making. Despite their seemingly different nature, the ultimate goal of all these improvements was to achieve better results for our team and our customers. We hope that these findings are inspiring and could spark a few ideas for you.\nNone of the content in this article could have been delivered without the in-depth discussions and candid feedback from Ambud Sharma, Eric Lopez, Henry Cai, Jeff Xiang, and Vahid Hashemian on the Logging Platform team. We also deeply appreciate the great support from external teams that provided support and input on the various improvements we\u2019ve been working on. As we strive for continuous improvement within our architecture, we hope we will be able to share more interesting findings in our pursuit of perfecting our system.\n\n\n\n\n\n\nApplication and Data\n\n\n\n\n\nJava\n\n\nLanguages\n\n\n\n\n\nAmazon S3\n\n\nCloud Storage\n\n\n\n\n\nKafka\n\n\nMessage Queue\n\n\n\n\n\nNetty\n\n\nConcurrency Frameworks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPinterest\n\n\n\n\n\nPinterest's profile on StackShare is not actively maintained, so the information here may be out of date.\n\n\nView Profile\n\n\n\nSocial Media\nE-Commerce\nSocial Commerce\nCurated Web\n\n\n\n\n\n\nTools mentioned in article\n\n\n\n\n\n\n\n\n\n\nKafka\nKafka\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetty\nNetty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon S3\nAmazon S3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJava\nJava\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet the best new tools weekly\n\n\n\n\n\nSign up today and never miss out on the latest tech trends\n\n\n\nSubscribe to Newsletter\n\n\n\n\n\n\n\n\n\n\n\nOpen jobs at\nPinterest\n\n\n\n\n\nFullstack Web Engineer\n\n\n\nWarsaw, POL\n\n\n<div class=\"content-intro\"><p><strong>About Pinterest</strong><span style=\"font-weight: 400;\">:&nbsp;&nbsp;</span></p>\n<p>Millions of people across the world come to Pinterest to find new ideas every day. It\u2019s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love.&nbsp;In your role, you\u2019ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You\u2019ll grow as a person and leader in your field, all the while helping&nbsp;Pinners&nbsp;make their lives better in the positive corner of the internet.</p>\n<p>Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.</p>\n<p><em>Our new progressive work model is called PinFlex, a term that\u2019s uniquely Pinterest to describe our flexible approach to living and working. Visit our </em><a href=\"https://www.pinterestcareers.com/pinflex/\" target=\"_blank\"><em><u>PinFlex</u></em></a><em> landing page to learn more.&nbsp;</em></p></div><p><strong>What you\u2019ll do</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><strong>Impact &amp; Mission. </strong><span style=\"font-weight: 400;\">Join a small team that works to implement a wide range of techniques to deliver value for Pinners and Creators&nbsp;</span></li>\n<li style=\"font-weight: 400;\"><strong>Fast iterations</strong><span style=\"font-weight: 400;\">. Prototype through fast iteration to explore various opportunities for all of Pinterest\u2019s global Pinners and Creators.</span></li>\n<li style=\"font-weight: 400;\"><strong>Collaboration</strong><span style=\"font-weight: 400;\">. Integrate with many other internal engineering teams across Pinterest to support the end-to-end user journeys. Work in dynamic and diverse environments alongside engineering (closely with iOS/Android teams), products and designers. Evangelization across engineering to implement and adopt best practices to simplify our codebase and promote growth</span></li>\n<li style=\"font-weight: 400;\"><strong>Ownership</strong><span style=\"font-weight: 400;\">. Take ownership of product quality and release processes to deliver high quality user experiences.</span></li>\n<li style=\"font-weight: 400;\"><strong>Feature Development. </strong><span style=\"font-weight: 400;\">Work across all parts of the stack and be flexible working with different technologies. Adding a new functionality on both the backend and Web sides to bring the value for millions of users</span></li>\n<li style=\"font-weight: 400;\"><strong>Pinterest Video Actionability. </strong><span style=\"font-weight: 400;\">To bring the maximum contribution by developing features that increase our video actionability for Pinners around the world</span></li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>What we\u2019re looking for</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">3+ years of full stack development (Web/Mobile Web) building successful products and/or systems, preferably on customer-facing products.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Proficiency in common backend tech stacks for RESTful API, storage, caching and data processing.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Experience/familiarity with Javascript, Python, React, GraphQL, mysql, bash/scripting or similar</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Strong command of SQL-like query languages to be used for processing data and automated workflows</span><span style=\"font-weight: 400;\">.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Self-driven and openness to learn quickly, explore, flexibility without being afraid to dig deep</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Strong communication skills and great product intuition</span><span style=\"font-weight: 400;\">.</span></li>\n</ul>\n<p>This position is not eligible for relocation assistance.</p>\n<p>&nbsp;</p>\n<p>#LI-HYBRID</p>\n<p>#LI-DL2</p><div class=\"content-conclusion\"><p><strong>Our Commitment to Diversity:</strong></p>\n<p>Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify&nbsp;<a href=\"mailto:accessibility@pinterest.com\">accessibility@pinterest.com</a>&nbsp;for support.</p></div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n5\n\n\n\n\n\n\n\nPrincipal Machine Learning Engineer -...\n\n\n\nSan Francisco, CA, US; , CA, US\n\n\n<div class=\"content-intro\"><p><strong>About Pinterest</strong><span style=\"font-weight: 400;\">:&nbsp;&nbsp;</span></p>\n<p>Millions of people across the world come to Pinterest to find new ideas every day. It\u2019s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love.&nbsp;In your role, you\u2019ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You\u2019ll grow as a person and leader in your field, all the while helping&nbsp;Pinners&nbsp;make their lives better in the positive corner of the internet.</p>\n<p>Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.</p>\n<p><em>Our new progressive work model is called PinFlex, a term that\u2019s uniquely Pinterest to describe our flexible approach to living and working. Visit our </em><a href=\"https://www.pinterestcareers.com/pinflex/\" target=\"_blank\"><em><u>PinFlex</u></em></a><em> landing page to learn more.&nbsp;</em></p></div><p><span style=\"font-weight: 400;\">The Advanced Technologies Group (ATG) is Pinterest\u2019s advanced machine learning team. It keeps Pinterest at the forefront of machine learning technology across multiple application areas including recommendations, ranking, content understanding, and more. It is a high impact applied team that works horizontally across the company on state of the art AI and ML and works on directly bringing that technology to the product in collaboration with product engineering teams. The team publishes its work in applied research conferences, but the main contribution of the team's work is to drive top line metric impact across the company for our 400M+ monthly active users.</span></p>\n<p><strong>What you'll do:</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Lead projects that involve developing and deploying state of the art (and beyond) ML models in production systems across the company at scale for hundreds of millions of users.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Help to define and drive forward looking ML strategy for the team and across the company.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Collaborate with other engineering teams (infrastructure, user modeling, content understanding) to leverage their platforms and signals and work with them to collaborate on the adoption and evaluation of new technologies.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Mentor junior engineers on the ATG and partner teams and help to uplevel ML talent across the company.</span></li>\n</ul>\n<p><strong>What we're looking for:</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Experience with state of the art ML modeling techniques and approaches like transformers, self supervised pre-training, generative modeling, LLMs, etc.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce)</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Hands-on experience training and applying models at scale using deep learning frameworks like PyTorch or Tensorflow. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">3+ years experience leading cross-team engineering efforts.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Strong execution skills in project management</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Masters or PhD in Comp Sci or related fields</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Understanding of an object-oriented programming language (Java, C++, Python)&nbsp;</span></li>\n</ul>\n<p><strong>Desired skills:</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Experience in working on, backend and ML systems for large-scale user-facing products, and have a good understanding of how they all work.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Experience in closely collaborating with other engineering teams to ship new ML technologies to improve recommendation, content understanding, and ranking systems at scale.</span></li>\n</ul>\n<p>This position is not eligible for relocation assistance.</p>\n<p><span style=\"font-weight: 400;\">#LI-SA1</span></p>\n<p><span style=\"font-weight: 400;\">#LI-REMOTE</span></p><div class=\"content-pay-transparency\"><div class=\"pay-input\"><div class=\"description\"><p>At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.</p>\n<p><em><span style=\"font-weight: 400;\">Information regarding the culture at Pinterest and benefits available for this position can be found <a href=\"https://www.pinterestcareers.com/pinterest-life/\" target=\"_blank\">here</a>.</span></em></p></div><div class=\"title\">US based applicants only</div><div class=\"pay-range\"><span>$221,000</span><span class=\"divider\">&mdash;</span><span>$455,000 USD</span></div></div></div><div class=\"content-conclusion\"><p><strong>Our Commitment to Diversity:</strong></p>\n<p>Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify&nbsp;<a href=\"mailto:accessibility@pinterest.com\">accessibility@pinterest.com</a>&nbsp;for support.</p></div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n3\n\n\n\n\n\n\n\nSr. Staff Software Engineer, Ads ML I...\n\n\n\nSan Francisco, CA, US; , CA, US\n\n\n<div class=\"content-intro\"><p><strong>About Pinterest</strong><span style=\"font-weight: 400;\">:&nbsp;&nbsp;</span></p>\n<p>Millions of people across the world come to Pinterest to find new ideas every day. It\u2019s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love.&nbsp;In your role, you\u2019ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You\u2019ll grow as a person and leader in your field, all the while helping&nbsp;Pinners&nbsp;make their lives better in the positive corner of the internet.</p>\n<p>Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.</p>\n<p><em>Our new progressive work model is called PinFlex, a term that\u2019s uniquely Pinterest to describe our flexible approach to living and working. Visit our </em><a href=\"https://www.pinterestcareers.com/pinflex/\" target=\"_blank\"><em><u>PinFlex</u></em></a><em> landing page to learn more.&nbsp;</em></p></div><p>Pinterest is one of the fastest growing online advertising platforms. Continued success depends on the machine-learning systems, which crunch thousands of signals in a few hundred milliseconds, to identify the most relevant ads to show to pinners. You\u2019ll join a talented team with high impact, which designs high-performance and efficient ML systems, in order to power the most critical and revenue-generating models at Pinterest.</p>\n<p><strong>What you\u2019ll do</strong></p>\n<ul>\n<li>Being the technical leader of the Ads ML foundation evolution movement to 2x Pinterest revenue and 5x ad performance in next 3 years.</li>\n<li>Opportunities to use cutting edge ML technologies including GPU and LLMs to empower 100x bigger models in next 3 years.&nbsp;</li>\n<li>Tons of ambiguous problems and you will be tasked with building 0 to 1 solutions for all of them.</li>\n</ul>\n<p><strong>What we\u2019re looking for:</strong></p>\n<ul>\n<li>BS (or higher) degree in Computer Science, or a related field.</li>\n<li>10+ years of relevant industry experience in leading the design of large scale &amp; production ML infra systems.</li>\n<li>Deep knowledge with at least one state-of-art programming language (Java, C++, Python).&nbsp;</li>\n<li>Deep knowledge with building distributed systems or recommendation infrastructure</li>\n<li>Hands-on experience with at least one modeling framework (Pytorch or Tensorflow).&nbsp;</li>\n<li>Hands-on experience with model / hardware accelerator libraries (Cuda, Quantization)</li>\n<li>Strong communicator and collaborative team player.</li>\n</ul><div class=\"content-pay-transparency\"><div class=\"pay-input\"><div class=\"description\"><p>At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.</p>\n<p><em><span style=\"font-weight: 400;\">Information regarding the culture at Pinterest and benefits available for this position can be found <a href=\"https://www.pinterestcareers.com/pinterest-life/\" target=\"_blank\">here</a>.</span></em></p></div><div class=\"title\">US based applicants only</div><div class=\"pay-range\"><span>$135,150</span><span class=\"divider\">&mdash;</span><span>$278,000 USD</span></div></div></div><div class=\"content-conclusion\"><p><strong>Our Commitment to Diversity:</strong></p>\n<p>Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify&nbsp;<a href=\"mailto:accessibility@pinterest.com\">accessibility@pinterest.com</a>&nbsp;for support.</p></div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n2\n\n\n\n\n\n\n\nSoftware Engineer, Backend\n\n\n\nMexico City, MX; , MX\n\n\n<div class=\"content-intro\"><p><strong>About Pinterest</strong><span style=\"font-weight: 400;\">:&nbsp;&nbsp;</span></p>\n<p>Millions of people across the world come to Pinterest to find new ideas every day. It\u2019s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love.&nbsp;In your role, you\u2019ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You\u2019ll grow as a person and leader in your field, all the while helping&nbsp;Pinners&nbsp;make their lives better in the positive corner of the internet.</p>\n<p>Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.</p>\n<p><em>Our new progressive work model is called PinFlex, a term that\u2019s uniquely Pinterest to describe our flexible approach to living and working. Visit our </em><a href=\"https://www.pinterestcareers.com/pinflex/\" target=\"_blank\"><em><u>PinFlex</u></em></a><em> landing page to learn more.&nbsp;</em></p></div><p><span style=\"font-weight: 400;\">We are looking for inquisitive, well-rounded Backend engineers to join our Core Engineering teams. Working closely with product managers, designers, and backend engineers, you\u2019ll play an important role in enabling the newest technologies and experiences. You will build robust frameworks &amp; features. You will empower both developers and Pinners alike. You\u2019ll have the opportunity to find creative solutions to thought-provoking problems. Even better, because we covet the kind of courageous thinking that\u2019s required in order for big bets and smart risks to pay off, you\u2019ll be invited to create and drive new initiatives, seeing them from inception through to technical design, implementation, and release.</span></p>\n<p>&nbsp;</p>\n<p><strong>What you\u2019ll do:</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Write high-quality, performant Python/Java code to build out workflows leveraging API\u2019s that enable our systems that maintain Pinner trust from a Privacy and regulatory compliance perspective</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Contribute to and lead each step of the product development process, from ideation to implementation to release; from rapidly prototyping, running A/B tests, to architecting and building solutions that leverage large-scale data</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Partner with design, product, and backend teams to build end-to-end functionality</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Put on your Pinner hat to suggest new product ideas and features</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Employ automated testing to build features with a high degree of technical quality, taking responsibility for the components and features you develop</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Grow as an engineer by working with world-class peers on varied and high-impact projects</span></li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>What we\u2019re looking for:</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Deep understanding of API/workflow&nbsp; development and best practices in Python or Java</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">3+ years of industry API and backend development experience, building API endpoints for consumer or business-facing products</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Proficiency in common backend tech stacks for RESTful API, storage, caching, and data processing</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Experience in following best practices in writing reliable and maintainable code that may be used by many other engineers</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Ability to keep up-to-date with new technologies to understand what should be incorporated</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Strong collaboration and communication skills</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Experience with GraphQL large data processing is a plus</span></li>\n</ul>\n<p>This position is not eligible for relocation assistance and candidates that apply should be based and legally authorized to work in Mexico.</p>\n<p>#LI-REMOTE</p>\n<p><span data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;#LI-MM&quot;}\" data-sheets-userformat=\"{&quot;2&quot;:8705,&quot;3&quot;:{&quot;1&quot;:0},&quot;12&quot;:0,&quot;16&quot;:12}\">#LI-MM</span></p><div class=\"content-conclusion\"><p><strong>Our Commitment to Diversity:</strong></p>\n<p>Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify&nbsp;<a href=\"mailto:accessibility@pinterest.com\">accessibility@pinterest.com</a>&nbsp;for support.</p></div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView all jobs\n\n\n\n\n\n\nVerified by\n\n\n\n\n\n\nDavid Chaiken (He Him)\n\n\n\n\n\n\n\nAndrey Gusev\n\n\n\n\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\nJune 01, 2022\nat 05:03PM\n\n\nImproving Distributed Caching Performance and Efficiency at Pinterest\n\n\nBy\u00a0\n\nPinterest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n\n\n\n873\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 30, 2022\nat 05:02PM\n\n\n99% to 99.9% SLO: High Performance Kubernetes Control Plane at Pinterest\n\n\nBy\u00a0\n\nPinterest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n1479\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 09, 2022\nat 06:41AM\n\n\n3 Innovations While Unifying Pinterest\u2019s Key-Value Storage\n\n\nBy\u00a0\n\nPinterest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n923\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 26, 2022\nat 04:34AM\n\n\nCost Reduction in Goku\n\n\nBy\u00a0\n\nPinterest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n703\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTools & ServicesThe Tech Stack File Compare ToolsSearchBrowse Tool AlternativesBrowse Tool CategoriesSubmit A ToolApprove ToolsCompanyBlogJob SearchCareersOur StackContact UsFollow UsTerms\u00b7PrivacySOC 2 Type 2 CertifiedCopyright \u00a9 2024 StackShare, Inc. All rights reserved.Sitemap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "id": "tag:stackshare.io,2005:Stack/1055123",
    "guidislink": true,
    "link": "https://stackshare.io/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings",
    "published": "2022-06-29T04:48:20Z",
    "published_parsed": [
      2022,
      6,
      29,
      4,
      48,
      20,
      2,
      180,
      0
    ],
    "updated": "2023-10-24T10:51:32Z",
    "updated_parsed": [
      2023,
      10,
      24,
      10,
      51,
      32,
      1,
      297,
      0
    ],
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://stackshare.io/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings"
      },
      {
        "href": "https://stackshare.io/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings",
        "rel": "alternate",
        "type": "text/html"
      }
    ],
    "title": "Optimizing Pinterest\u2019s Data Ingestion Stack: Findings and Learnings",
    "title_detail": {
      "type": "text/plain",
      "language": "en-US",
      "base": "https://stackshare.io/featured-posts.atom",
      "value": "Optimizing Pinterest\u2019s Data Ingestion Stack: Findings and Learnings"
    },
    "content": [
      {
        "type": "text/html",
        "language": "en-US",
        "base": "https://stackshare.io/featured-posts.atom",
        "value": "<p><strong><em>By Ping-Min Lin | Software Engineer, Logging Platform</em></strong></p>\n\n<hr />\n\n<p>At <a href=\"https://stackshare.io/companies/pinterest\">Pinterest</a>, the Logging Platform team maintains the backbone of data ingestion infrastructure that ingests terabytes of data per day. When building the services powering these pipelines, it is extremely important that we build efficient systems considering how widespread and deep in the stack the systems are. Along our journey of continuous improvement, we\u2019ve figured out basic but useful patterns and learnings that could be applied in general \u2014 and hopefully for you as well.</p>\n\n<h1><strong>MemQ: Achieving memory-efficient batch data delivery using Netty</strong></h1>\n\n<p><a href=\"https://stackshare.io/pinterest/memq-an-efficient-scalable-cloud-native-pubsub-system\">MemQ</a> is the next-gen data ingestion platform built in-house and recently open-sourced by the Logging Platform team. When designing the service, we tried hard to maximize the efficiency of our resources, specifically, we focused on reducing GC by using off-heap memory. <a href=\"https://stackshare.io/netty\">Netty</a> was chosen as our low-level networking framework due to its great balance between flexibility, performance, and sophisticated out-of-the-box features. For example, we used ByteBuf heavily throughout the project. ByteBufs are the building blocks of data within Netty. They are similar to <a href=\"https://stackshare.io/java\">Java</a> NIO ByteBuffers, but allow the developers much more control of the lifecycle of the objects by providing a \u201csmart pointer\u201d approach for customized memory management using manual <a href=\"https://netty.io/wiki/reference-counted-objects.html\">reference counting</a>. By using ByteBufs, we managed to transport messages with a single copy of data by passing off-heap network buffer pointers, further reducing cycles used on garbage collection.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-000.png\" title=\"image_tooltip\" /></p>\n\n<p>The typical journey of a message in the MemQ broker: Each message received from the network will be reconstructed via a length-encoded protocol that will be allocated into a ByteBuf that is off of the JVM heap (direct memory in Netty terms), and will be the only existing copy of the payload throughout the whole pipeline. This ByteBuf reference will be passed into the topic processor and put into a Batch along with other messages that are also waiting to be uploaded to the storage destination. Once the upload constraints are met, either due to the time threshold or the size threshold, the Batch will be dispatched. In the case of uploading to a remote object store like <a href=\"https://stackshare.io/amazon-s3\">S3</a>, the whole batch of messages will be kept in a CompositeByteBuf (which is a virtual wrapper ByteBuf consisting of multiple ByteBufs) and uploaded to the destination using the netty-reactor library, allowing us to create no additional copies of data within the processing path. By building on top of ByteBufs and other Netty constructs, we were able to iterate rapidly without sacrificing performance and avoid reinventing the wheel.</p>\n\n<h1><strong>Singer: Leveraging asynchronous processing to reduce thread overheads</strong></h1>\n\n<p><a href=\"https://medium.com/pinterest-engineering/open-sourcing-singer-pinterests-performant-and-reliable-logging-agent-610fecf35566\">Singer</a> has been around at Pinterest for a long time, reliably delivering messages to PubSub backends. With more and more use cases onboarded to Singer, we\u2019ve started to hit bottlenecks on memory usage that led to frequent OOM issues and incidents. Singer has memory and CPU resources constrained on nearly all fleets at Pinterest to avoid impact on the host service e.g. our API serving layer. After inspecting the code and leveraging debugging tools such as VisualVM, Native Memory Tracking (NMT), and pmap, we noticed various potential improvements to be done, most notably reducing the number of threads. After performing NMT result analysis we noticed the number of threads and the memory used by the stack as a result of these threads (allocated due to the Singer executor and producer thread pools).</p>\n\n<p>Taking a deeper look into the source of these threads, the majority of these threads come from the thread pools for each <a href=\"https://stackshare.io/kafka\">Kafka</a> cluster Singer publishes to. The threads in these thread pools are used to wait for Kafka to complete writing messages to a partition and then report the status of the writes. While the threads do the job, each thread in the JVM (by <a href=\"https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/geninfo/diagnos/thread_basics.html#wp1094805\">default</a>) will allocate 1MB of memory used for the thread\u2019s stack.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-001.png\" title=\"image_tooltip\" /></p>\n\n<p>A Singer NMT report showing the different memory regions a JVM process allocates. The <em>Thread</em> entry represents the thread stack. <em>Arena</em> contains the off-heap/direct memory portion managed outside of the JVM heap.</p>\n\n<p>Even with lazy allocation of the stack memory on the underlying operating systems until the thread is actually used, this still quickly adds up to hundreds of MBs of the process\u2019 memory. When there are a lot of log streams publishing to multiple partitions on different clusters, the memory used by thread stacks can be easily comparable to the 800MB default heap size of Singer and eats into the resources of the application.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-002.png\" title=\"image_tooltip\" /></p>\n\n<p>Each submission of KafkaWriteTask will occupy a thread. Full code can be found <a href=\"https://github.com/kabochya/singer/blob/b45051c/singer/src/main/java/com/pinterest/singer/writer/kafka/CommittableKafkaWriter.java#L192\">here</a></p>\n\n<p>By closely examining the usage of these threads, it quickly becomes clear that most of these threads are doing non-blocking operations such as updating metrics and are perfectly suitable for asynchronous processing using <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html\">CompletableFutures</a> provided starting in <a href=\"https://stackshare.io/java\">Java 8</a>. The CompletableFuture allows us to resolve the blocking calls by chaining stages asynchronously, thus replacing the usage of these threads that had to wait until the results to come back from Kafka. By utilizing the callback in the <em>KafkaProducer.send(record, callback)</em> method, we rely on the Kafka producer\u2019s network client to be completely in control of the multiplexing of networking.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-003.png\" title=\"image_tooltip\" /></p>\n\n<p>A brief example of the result code after using CompletableFutures. Full code can be found <a href=\"https://github.com/pinterest/singer/blob/5cc504da01c4c2cd747c8e0bb6a56b94571c8a60/singer/src/main/java/com/pinterest/singer/writer/kafka/CommittableKafkaWriter.java#L213\">here</a></p>\n\n<p>Once we convert the original logic into several chained non-blocking stages, it becomes obvious to use a single common thread pool to handle them regardless of the logstream, so we use the <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#commonPool--\">common ForkJoinPool</a> that is already at our disposal from JVM. This dramatically reduces the thread usage for Singer, from a couple of hundred threads to virtually no additional threads. This improvement demonstrates the power of asynchronous processing and how network-bound applications can benefit from it.</p>\n\n<h1><strong>Kafka and Singer: Balancing performance and efficiency with controllable variance</strong></h1>\n\n<p><a href=\"https://www.confluent.io/blog/running-kafka-at-scale-at-pinterest/\">Operating our Kafka clusters</a> has always been a delicate balance between performance, fault tolerance, and efficiency. Our logging agent Singer, at the front line of publishing messages to Kafka, is a crucial component that plays a heavy role in these factors, especially in routing the traffic by deciding which partitions we deliver data to for a topic.</p>\n\n<h2><strong>The Default Partitioner: Evenly Distributed Traffic</strong></h2>\n\n<p>In Singer, logs from a machine would be picked up and routed to the corresponding topic it belongs to and published to that topic in Kafka. In the early days, Singer would publish uniformly to all the partitions that topic has in a round-robin fashion using our default partitioner. For example, if there were 3000 messages on a particular host that needed to be published to a 30 partition topic, each partition would roughly receive 100 messages. This worked pretty well for most of the use cases and has a nice benefit where all partitions receive the same amount of messages, which is great for the consumers of these topics since the workload is evenly distributed amongst them.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-004.png\" title=\"image_tooltip\" /></p>\n\n<p>DefaultPartitioner: Producers and Partitions are fully connected</p>\n\n<h2><strong>The Single Partition Partitioner: In Favor of the Law of Large Numbers</strong></h2>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-005.png\" title=\"image_tooltip\" /></p>\n\n<p>SinglePartitionPartitioner: Ideal scenario where connections are evenly distributed</p>\n\n<p>As Pinterest grew, we had fleets expanding to thousands of hosts, and this evenly-distributed approach started to cause some issues to our Kafka brokers: high connections counts and large amounts of produce requests started to elevate the brokers\u2019 CPU usage, and spreading out the messages means that the batch sizes are smaller for each partition, or lower efficiency of the compression, resulting in higher aggregated network traffic. To tackle this, we implemented a new partitioner: the SinglePartitionPartitioner. This partitioner solves the issue by forcing Singer to only write to one random partition per topic per host, reducing the fanout from all brokers to a single broker. This partition remains the same throughout the producer\u2019s lifetime until Singer restarts.</p>\n\n<p>For pipelines that had a large producer fleet and relatively uniform message rates across hosts, this was extremely effective: The law of large numbers worked in our favor, and statistically, if the number of producers is significantly larger than partitions, each partition will still receive a similar amount of traffic. Connection count went down from <strong>(number of brokers serving the topic)</strong> times <strong>(number of producers)</strong> to only <strong>(number of producers)</strong>, which could be up to a hundred times less for larger topics. Meanwhile, batching up all messages per producer to a single partition improved compression ratios by at least 10% in most use cases.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-006.png\" title=\"image_tooltip\" /></p>\n\n<p>SinglePartitionPartitioner: Skewed scenario where there are too few producers vs. partitions</p>\n\n<h2><strong>The Fixed Partitions Partitioner: Configurable variance for adjusting trade-offs</strong></h2>\n\n<p>Despite coming up with this new solution, there were still some pipelines that lie in the middle ground where both solutions are subpar, such as when the number of producers is not large enough to outnumber the number of partitions. In this case, the SinglePartitionPartitioner would introduce significant skew between partitions: some partitions will have multiple producers writing to them, and some are assigned very few or even no producers. This skew could cause unbalanced workloads for the downstream consumers, and also increases the burden for our team to manage the cluster, especially when storage is tight. We thus recently introduced a new partitioner that can be used on these cases, and even cover the original use cases: the FixedPartitionsPartitioner, which basically allows us to not only publish to one fixed partition like the SinglePartitionPartitioner, but randomly across a fixed number of partitions.</p>\n\n<p>This approach is somewhat similar to the concept of <a href=\"https://en.wikipedia.org/wiki/Consistent_hashing#Reduction_variance\">virtual nodes</a> in consistent hashing, where we artificially create more \u201ceffective producers\u201d to achieve a more continuous distribution. Since the number of partitions for each host can be configured, we can tune it to the sweet spot where the efficiency and performance are both at desired levels. This partitioner could also help with \u201chot producers\u201d by spreading traffic out while still maintaining a reasonable connection count. Although a simple concept, it turns out that having the ability to configure the degree of variance could be a powerful tool to manage trade-offs.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-007.png\" title=\"image_tooltip\" /></p>\n\n<p>FixedPartitionsPartitioner: Less skew while still keeping connection count lower than the default</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-008.png\" title=\"image_tooltip\" /></p>\n\n<p>Relative compression ratio and request rate skew with different numbers of fixed partitions on a 120 partition topic on 30 brokers</p>\n\n<h1><strong>Conclusion and Acknowledgements</strong></h1>\n\n<p>These learnings are just a few examples of improvements the Logging Platform team has been making. Despite their seemingly different nature, the ultimate goal of all these improvements was to achieve better results for our team and our customers. We hope that these findings are inspiring and could spark a few ideas for you.</p>\n\n<p>None of the content in this article could have been delivered without the in-depth discussions and candid feedback from Ambud Sharma, Eric Lopez, Henry Cai, Jeff Xiang, and Vahid Hashemian on the Logging Platform team. We also deeply appreciate the great support from external teams that provided support and input on the various improvements we\u2019ve been working on. As we strive for continuous improvement within our architecture, we hope we will be able to share more interesting findings in our pursuit of perfecting our system.</p>"
      }
    ],
    "summary": "<p><strong><em>By Ping-Min Lin | Software Engineer, Logging Platform</em></strong></p>\n\n<hr />\n\n<p>At <a href=\"https://stackshare.io/companies/pinterest\">Pinterest</a>, the Logging Platform team maintains the backbone of data ingestion infrastructure that ingests terabytes of data per day. When building the services powering these pipelines, it is extremely important that we build efficient systems considering how widespread and deep in the stack the systems are. Along our journey of continuous improvement, we\u2019ve figured out basic but useful patterns and learnings that could be applied in general \u2014 and hopefully for you as well.</p>\n\n<h1><strong>MemQ: Achieving memory-efficient batch data delivery using Netty</strong></h1>\n\n<p><a href=\"https://stackshare.io/pinterest/memq-an-efficient-scalable-cloud-native-pubsub-system\">MemQ</a> is the next-gen data ingestion platform built in-house and recently open-sourced by the Logging Platform team. When designing the service, we tried hard to maximize the efficiency of our resources, specifically, we focused on reducing GC by using off-heap memory. <a href=\"https://stackshare.io/netty\">Netty</a> was chosen as our low-level networking framework due to its great balance between flexibility, performance, and sophisticated out-of-the-box features. For example, we used ByteBuf heavily throughout the project. ByteBufs are the building blocks of data within Netty. They are similar to <a href=\"https://stackshare.io/java\">Java</a> NIO ByteBuffers, but allow the developers much more control of the lifecycle of the objects by providing a \u201csmart pointer\u201d approach for customized memory management using manual <a href=\"https://netty.io/wiki/reference-counted-objects.html\">reference counting</a>. By using ByteBufs, we managed to transport messages with a single copy of data by passing off-heap network buffer pointers, further reducing cycles used on garbage collection.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-000.png\" title=\"image_tooltip\" /></p>\n\n<p>The typical journey of a message in the MemQ broker: Each message received from the network will be reconstructed via a length-encoded protocol that will be allocated into a ByteBuf that is off of the JVM heap (direct memory in Netty terms), and will be the only existing copy of the payload throughout the whole pipeline. This ByteBuf reference will be passed into the topic processor and put into a Batch along with other messages that are also waiting to be uploaded to the storage destination. Once the upload constraints are met, either due to the time threshold or the size threshold, the Batch will be dispatched. In the case of uploading to a remote object store like <a href=\"https://stackshare.io/amazon-s3\">S3</a>, the whole batch of messages will be kept in a CompositeByteBuf (which is a virtual wrapper ByteBuf consisting of multiple ByteBufs) and uploaded to the destination using the netty-reactor library, allowing us to create no additional copies of data within the processing path. By building on top of ByteBufs and other Netty constructs, we were able to iterate rapidly without sacrificing performance and avoid reinventing the wheel.</p>\n\n<h1><strong>Singer: Leveraging asynchronous processing to reduce thread overheads</strong></h1>\n\n<p><a href=\"https://medium.com/pinterest-engineering/open-sourcing-singer-pinterests-performant-and-reliable-logging-agent-610fecf35566\">Singer</a> has been around at Pinterest for a long time, reliably delivering messages to PubSub backends. With more and more use cases onboarded to Singer, we\u2019ve started to hit bottlenecks on memory usage that led to frequent OOM issues and incidents. Singer has memory and CPU resources constrained on nearly all fleets at Pinterest to avoid impact on the host service e.g. our API serving layer. After inspecting the code and leveraging debugging tools such as VisualVM, Native Memory Tracking (NMT), and pmap, we noticed various potential improvements to be done, most notably reducing the number of threads. After performing NMT result analysis we noticed the number of threads and the memory used by the stack as a result of these threads (allocated due to the Singer executor and producer thread pools).</p>\n\n<p>Taking a deeper look into the source of these threads, the majority of these threads come from the thread pools for each <a href=\"https://stackshare.io/kafka\">Kafka</a> cluster Singer publishes to. The threads in these thread pools are used to wait for Kafka to complete writing messages to a partition and then report the status of the writes. While the threads do the job, each thread in the JVM (by <a href=\"https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/geninfo/diagnos/thread_basics.html#wp1094805\">default</a>) will allocate 1MB of memory used for the thread\u2019s stack.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-001.png\" title=\"image_tooltip\" /></p>\n\n<p>A Singer NMT report showing the different memory regions a JVM process allocates. The <em>Thread</em> entry represents the thread stack. <em>Arena</em> contains the off-heap/direct memory portion managed outside of the JVM heap.</p>\n\n<p>Even with lazy allocation of the stack memory on the underlying operating systems until the thread is actually used, this still quickly adds up to hundreds of MBs of the process\u2019 memory. When there are a lot of log streams publishing to multiple partitions on different clusters, the memory used by thread stacks can be easily comparable to the 800MB default heap size of Singer and eats into the resources of the application.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-002.png\" title=\"image_tooltip\" /></p>\n\n<p>Each submission of KafkaWriteTask will occupy a thread. Full code can be found <a href=\"https://github.com/kabochya/singer/blob/b45051c/singer/src/main/java/com/pinterest/singer/writer/kafka/CommittableKafkaWriter.java#L192\">here</a></p>\n\n<p>By closely examining the usage of these threads, it quickly becomes clear that most of these threads are doing non-blocking operations such as updating metrics and are perfectly suitable for asynchronous processing using <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html\">CompletableFutures</a> provided starting in <a href=\"https://stackshare.io/java\">Java 8</a>. The CompletableFuture allows us to resolve the blocking calls by chaining stages asynchronously, thus replacing the usage of these threads that had to wait until the results to come back from Kafka. By utilizing the callback in the <em>KafkaProducer.send(record, callback)</em> method, we rely on the Kafka producer\u2019s network client to be completely in control of the multiplexing of networking.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-003.png\" title=\"image_tooltip\" /></p>\n\n<p>A brief example of the result code after using CompletableFutures. Full code can be found <a href=\"https://github.com/pinterest/singer/blob/5cc504da01c4c2cd747c8e0bb6a56b94571c8a60/singer/src/main/java/com/pinterest/singer/writer/kafka/CommittableKafkaWriter.java#L213\">here</a></p>\n\n<p>Once we convert the original logic into several chained non-blocking stages, it becomes obvious to use a single common thread pool to handle them regardless of the logstream, so we use the <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html#commonPool--\">common ForkJoinPool</a> that is already at our disposal from JVM. This dramatically reduces the thread usage for Singer, from a couple of hundred threads to virtually no additional threads. This improvement demonstrates the power of asynchronous processing and how network-bound applications can benefit from it.</p>\n\n<h1><strong>Kafka and Singer: Balancing performance and efficiency with controllable variance</strong></h1>\n\n<p><a href=\"https://www.confluent.io/blog/running-kafka-at-scale-at-pinterest/\">Operating our Kafka clusters</a> has always been a delicate balance between performance, fault tolerance, and efficiency. Our logging agent Singer, at the front line of publishing messages to Kafka, is a crucial component that plays a heavy role in these factors, especially in routing the traffic by deciding which partitions we deliver data to for a topic.</p>\n\n<h2><strong>The Default Partitioner: Evenly Distributed Traffic</strong></h2>\n\n<p>In Singer, logs from a machine would be picked up and routed to the corresponding topic it belongs to and published to that topic in Kafka. In the early days, Singer would publish uniformly to all the partitions that topic has in a round-robin fashion using our default partitioner. For example, if there were 3000 messages on a particular host that needed to be published to a 30 partition topic, each partition would roughly receive 100 messages. This worked pretty well for most of the use cases and has a nice benefit where all partitions receive the same amount of messages, which is great for the consumers of these topics since the workload is evenly distributed amongst them.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-004.png\" title=\"image_tooltip\" /></p>\n\n<p>DefaultPartitioner: Producers and Partitions are fully connected</p>\n\n<h2><strong>The Single Partition Partitioner: In Favor of the Law of Large Numbers</strong></h2>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-005.png\" title=\"image_tooltip\" /></p>\n\n<p>SinglePartitionPartitioner: Ideal scenario where connections are evenly distributed</p>\n\n<p>As Pinterest grew, we had fleets expanding to thousands of hosts, and this evenly-distributed approach started to cause some issues to our Kafka brokers: high connections counts and large amounts of produce requests started to elevate the brokers\u2019 CPU usage, and spreading out the messages means that the batch sizes are smaller for each partition, or lower efficiency of the compression, resulting in higher aggregated network traffic. To tackle this, we implemented a new partitioner: the SinglePartitionPartitioner. This partitioner solves the issue by forcing Singer to only write to one random partition per topic per host, reducing the fanout from all brokers to a single broker. This partition remains the same throughout the producer\u2019s lifetime until Singer restarts.</p>\n\n<p>For pipelines that had a large producer fleet and relatively uniform message rates across hosts, this was extremely effective: The law of large numbers worked in our favor, and statistically, if the number of producers is significantly larger than partitions, each partition will still receive a similar amount of traffic. Connection count went down from <strong>(number of brokers serving the topic)</strong> times <strong>(number of producers)</strong> to only <strong>(number of producers)</strong>, which could be up to a hundred times less for larger topics. Meanwhile, batching up all messages per producer to a single partition improved compression ratios by at least 10% in most use cases.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-006.png\" title=\"image_tooltip\" /></p>\n\n<p>SinglePartitionPartitioner: Skewed scenario where there are too few producers vs. partitions</p>\n\n<h2><strong>The Fixed Partitions Partitioner: Configurable variance for adjusting trade-offs</strong></h2>\n\n<p>Despite coming up with this new solution, there were still some pipelines that lie in the middle ground where both solutions are subpar, such as when the number of producers is not large enough to outnumber the number of partitions. In this case, the SinglePartitionPartitioner would introduce significant skew between partitions: some partitions will have multiple producers writing to them, and some are assigned very few or even no producers. This skew could cause unbalanced workloads for the downstream consumers, and also increases the burden for our team to manage the cluster, especially when storage is tight. We thus recently introduced a new partitioner that can be used on these cases, and even cover the original use cases: the FixedPartitionsPartitioner, which basically allows us to not only publish to one fixed partition like the SinglePartitionPartitioner, but randomly across a fixed number of partitions.</p>\n\n<p>This approach is somewhat similar to the concept of <a href=\"https://en.wikipedia.org/wiki/Consistent_hashing#Reduction_variance\">virtual nodes</a> in consistent hashing, where we artificially create more \u201ceffective producers\u201d to achieve a more continuous distribution. Since the number of partitions for each host can be configured, we can tune it to the sweet spot where the efficiency and performance are both at desired levels. This partitioner could also help with \u201chot producers\u201d by spreading traffic out while still maintaining a reasonable connection count. Although a simple concept, it turns out that having the ability to configure the degree of variance could be a powerful tool to manage trade-offs.</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-007.png\" title=\"image_tooltip\" /></p>\n\n<p>FixedPartitionsPartitioner: Less skew while still keeping connection count lower than the default</p>\n\n<p><img alt=\"\" src=\"https://img.stackshare.io/featured_posts/pinterest/optimizing-pinterests-data-ingestion-stack-findings-and-learnings/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-008.png\" title=\"image_tooltip\" /></p>\n\n<p>Relative compression ratio and request rate skew with different numbers of fixed partitions on a 120 partition topic on 30 brokers</p>\n\n<h1><strong>Conclusion and Acknowledgements</strong></h1>\n\n<p>These learnings are just a few examples of improvements the Logging Platform team has been making. Despite their seemingly different nature, the ultimate goal of all these improvements was to achieve better results for our team and our customers. We hope that these findings are inspiring and could spark a few ideas for you.</p>\n\n<p>None of the content in this article could have been delivered without the in-depth discussions and candid feedback from Ambud Sharma, Eric Lopez, Henry Cai, Jeff Xiang, and Vahid Hashemian on the Logging Platform team. We also deeply appreciate the great support from external teams that provided support and input on the various improvements we\u2019ve been working on. As we strive for continuous improvement within our architecture, we hope we will be able to share more interesting findings in our pursuit of perfecting our system.</p>"
  }
}