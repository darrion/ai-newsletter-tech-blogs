{
  "company": "Sky Betting & Gaming",
  "title": "Sky Betting & Gaming",
  "xmlUrl": "https://technology.skybettingandgaming.com/feed.xml",
  "htmlUrl": "http://engineering.skybettingandgaming.com/",
  "content": "\n\n\n\nFireDrill GameDays at Sky Betting & Gaming - Sky Betting & Gaming Technology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                  Sky Betting & Gaming Technology\n                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJobs\n\n\nEvents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFireDrill GameDays at Sky Betting & Gaming\n\n\n\n16\nOct '23\n\n\n\nOperations\n\n\nThe FireDrill GameDay\nHere at Sky Betting & Gaming, we\u2019ve had great success running fire drills against our production systems. By running these failure scenarios we\u2019ve been able to increase confidence in our ability to support live service, enhance the knowledge of support staff and highlight weaknesses - be that with the services themselves or our support processes.\nLooking to build on this success, I put forward the idea of running similarly structured events pre-go-live in the form of a GameDay. The aim was to gain similar learnings but by doing it before the system went live we sort capture issues in their infancy, before they have a chance to impact customers, and help make a judgement call on the service\u2019s readiness to go live - thus the \u201cFireDrill GameDay\u201d was born. In this post, I\u2019ll summarise some of the key elements that made these events a success and highlight some of the lessons learned from running these over the last two years.\nA FireDrill GameDay brings together two activities in Chaos Engineering :\n\nGameDay: purposefully creating major failures with the goal of increasing reliability. Typically, they last between 2-4 hours and involve a team of engineers who either develop an application or support it.\n\u00a0\u00a0\nFire drill: routine and random testing of the failure modes of systems you build and the processes you use to support and secure what you have built. Deliberately disrupt part of a functioning system to see whether, and how, it recovers.\n\nA Firedrill GameDay is essentially a GameDay run using the tried and tested format we\u2019ve developed for fire drills; as such, I\u2019ll refer to them as GameDays for the remainder of this post. What makes the format slightly different from our standard fire drills is that they are less formal, have multiple scenarios, and in our case performed on pre-live systems.\nIn regards to planning and execution, we found it useful to include the following elements:\nScope\nBy clearly defining the scope of the GameDay we can ensure that we focus on the correct areas of the service with the appropriate participants. The scope would generally cover all components of a given service but you may choose to focus on areas of a service where the risk of failure is high or the state of readiness (to go live) is questionable.\nParticipants\nThose involved in the GameDay should be notified at the earliest opportunity to ensure they are available and preparation can start at the earliest oppotunity. Generally, involvement would be limited to those who are to be responsible for supporting the service and those involved in its development. The support team would primarily be involved in the investigation and resolution of incidents and the development team involved in planning and reviewing outcomes.\nOne or more Excon\u2019s (exercise coordinator) will be required to run the incidents and it\u2019s also useful to assign someone to the roles of SLM (Service Lifecycle Manager) and IC (Incident Commander).\nTimetable\nIn advance of the GameDay publish a timeline of how the day will be broken down. Ensure that sufficient time is provided to investigate each issues, attempt to find a solution, and confirm that everything is back to normal. It is also advisable to have a break in between each scenario to reset the environment and give those running the GameDay a break as well as time to prep for the next scenario.\n\nIntro / Overview\nKick off the GameDay by providing a run down of the day\u2019s plans and an overview of the system architecture. This is particularly useful where the target system is yet to go live and the support personnel are unfamiliar with it. Having the architect or lead developer provide this overview may be particularly beneficial as is the provision of links to useful resources such as runbooks and monitoring.\nScenarios\nMultiple failure scenarios to be devised and and tested well in advance, while ensuring they are kept hidden from those resposible for resolving the problem on the day. Devising the scenarios should be a team effort involving analysis of the system architecture and failure mode. Issues experienced during development should be used to pinpoint potential problem areas. Ultimately, a list of scenarios should be broken down and detailed as shown here:\n\nThe selected scenarios should be thoroughly tested to obtain a clear understanding of how the simulated incident will play out and the best candidates taken forward to the GameDay.\nCertain scenarios may benefit from simulated transactions being run against the service. This adds to the realism, with logs being populated with associated errors/warns, while also helping sign-posting the issue and its consequences.\nExecution\nDuring execution be clear on any rules that might apply e.g :\n\nThis GameDay will take place in the DR environment and be treated as if a live incident though all participants will be aware it is just a test exercise.\u00a0\nResponders will be called out using Slack, assuming nothing automatically calls them out.\nThe Excon may drop hints where necessary and produce updates on the time remaining.\n\nDuring the exercise, it is good to keep reminding the participants of the questions we are seeking answers to :\n\nIs system behaviour as expected?\nDo you have the required access to services to investigate the issue?\nIs the monitoring sufficient to detect the problem?\nDo the monitoring dashboards aid insight and investigation?\nDo alerts trigger as required and resolve following the incident?\nDo you have visibility of the logs and did they help highlight the issue?\nHow good is the supporting documentation e.g architectural, support, etc?\nAre there any issues with the incident management process?\nAre there any areas we can change to improve the supportability of the service(s)?\nIs there a clear understanding of the impact of the incident esp on the customer?\nDoes the service recover gracefully with acceptable impact?\n\nRetrospective\nOnce all the scenarios have been executed those involved should be given time to take a break and consider any issues encountered especially in response to the questions asked in the previous section.\u00a0\nA retro board similar to the following may be used to decide which issues are to be fixed now and those which can be left until later:\n\nWe also made use of a Readiness-O-Meter to get a quick view of where people thought we were in regard to our ability to support the service.\n\nLessons Learned\nHaving now run several of these events - here are some of the key lessons learned:\n\n\nEarly involvement is key - start early in the project and get people involved from the outset. The GameDay should be treated as a project deliverable in its own right.\n\n\nChoose scenarios wisely for maximum learnings on incidents that are likely to occur. This ensures you provide valuable insights into the service and how it is supported.\n\n\nDevise multiple scenarios - not only will you need backups but the more scenarios, the more you\u2019ll learn about the service and the greater the potential for uncovering issues.\n\n\nResist efforts to include tests that should be done elsewhere; the GameDay is a complement not a substitute for more traditional forms of Operations Acceptance Testing like failure testing, backup recovery, and DR testing.\n\n\nDuring the exercise do not lose sight of the end goal - keep reminding the participants of the questions we are seeking answers to e.g. Is system behaviour as expected? Are there any areas we can change to improve the supportability of the service(s)?\n\n\nMake time for a closure-type event and associated activities ensuring that nothing gets left unresolved and without an owner.\n\n\nFinally, the day of the GameDay is just a part of it - as important are the conversation, analysis and testing that happen in the lead-up.\n\n\n\n\nWant to read more about Operations? Check out these related articles...\n\n\n\n\n21\nAug '20\n\n\nZero-Downtime Kubernetes Deployments\nWhen migrating services to shiny new cloud-native infrastructure, special care must be taken to ensure that releases that were zero-downtime continue to be so. When said service is the login system for your entire customer-facing product offering, a little extra effort is probably needed\n\n\nAuthor:\n\nOliver Leaver-Smith\n\n\n\nTime:\n10 minute read\n\n\n\n\n\n\n7\nFeb '20\n\n\nRising from the Ashes\nWe\u2019ve always enjoyed running incident response drills, but they were becoming stale. This post covers how we addressed the problems with our fire drills and iterated upon them\n\n\nAuthor:\n\nOliver Leaver-Smith\n\n\n\nTime:\n8 minute read\n\n\n\n\n\n\n10\nJul '18\n\n\nKafka on NFS\nThere is a general recommendation against running Apache Kafka on NFS storage, but nobody really gives a good explanation as to why.  In this post we look at some broker crashes we have seen happening on Kafka clusters which use NFS storage and why they were happening.\n\n\nAuthor:\n\nAlice Kaerast\n\n\n\nTime:\n5 minute read\n\n\n\n\n\n\n26\nMay '18\n\n\nJMX Metrics in Kafka Connect\nThe use of JMX metrics in Java applications is often poorly documented and is a feature that people are often unaware of.  In this post we explore how to use the JMX metrics provided by Kafka Connect.\n\n\nAuthor:\n\nAlice Kaerast\n\n\n\nTime:\n11 minute read\n\n\n\n\n\n\n4\nMay '18\n\n\nCrash! Bang! Wallop! Practice makes perfect\nEngineered Chaos, breaking production, and getting away with it. How the Core Tribe in Sky Betting and Gaming break stuff to make things better\n\n\nAuthor:\n\nOliver Leaver-Smith\n\n\n\nTime:\n14 minute read\n\n\n\n\n\n\n9\nAug '16\n\n\nCSI Skybet\nA tale of football, nodejs, and rabbits.\n\n\nAuthor:\n\nColin Ameigh\n\n\n\nTime:\n15 minute read\n\n\n\n\n\n\n\n\nMeet the author\n\n\n\n\n\n\n            Paul Whitehead\n        \nSenior Performance Test Engineer \u00b7 Core Tribe\nWorking at Sky Betting & Gaming since 2016\nSpecialist in Performance Testing and Operational Acceptance Testing. PerfOps &  #Perfmatters\n\nResponse_Times\n\n\n\n\n\nTweets by @SBGTechTeam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Sky Betting & Gaming\nOur applications are written in all manner of languages, including Node.js, NextJS, PHP, Golang, and a few JVM-based variations. We utilise a number of open source projects including Kubernetes, MySQL, Redis, RabbitMQ, Kafka, and Prometheus. Our infrastructure is made up of over 4000 virtual and physical machines spread across five data centres and managed using Chef & Ansible, as well as extensive use of public and private cloud vendors managed using Terraform.\n\n\nBefore you go\u2026\n\nWant more? Browse all articles by SB&G\nFollow us by subscribing to our ATOM feed\nReady for a challenge? Get in touch, we\u2019re hiring\nView our GitHub page\n\n\n\n\n\n\n\n\nOur Products\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "title": "FireDrill GameDays at Sky Betting &amp; Gaming",
    "title_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://sbg.technology/feed.xml",
      "value": "FireDrill GameDays at Sky Betting &amp; Gaming"
    },
    "links": [
      {
        "href": "https://sbg.technology/2023/10/16/firedrill-gamedays/",
        "rel": "alternate",
        "type": "text/html",
        "title": "FireDrill GameDays at Sky Betting &amp; Gaming"
      }
    ],
    "link": "https://sbg.technology/2023/10/16/firedrill-gamedays/",
    "published": "2023-10-16T00:00:00+00:00",
    "published_parsed": [
      2023,
      10,
      16,
      0,
      0,
      0,
      0,
      289,
      0
    ],
    "updated": "2023-10-16T00:00:00+00:00",
    "updated_parsed": [
      2023,
      10,
      16,
      0,
      0,
      0,
      0,
      289,
      0
    ],
    "id": "https://sbg.technology/2023/10/16/firedrill-gamedays",
    "guidislink": false,
    "content": [
      {
        "type": "text/html",
        "language": null,
        "base": "https://sbg.technology/2023/10/16/firedrill-gamedays/",
        "value": "<h2 id=\"the-firedrill-gameday\">The FireDrill GameDay</h2>\n\n<p>Here at Sky Betting &amp; Gaming, we\u2019ve had great success running <a href=\"https://sbg.technology/2018/05/04/firedrills-in-core/\">fire drills</a> against our production systems. By running these failure scenarios we\u2019ve been able to increase confidence in our ability to support live service, enhance the knowledge of support staff and highlight weaknesses - be that with the services themselves or our support processes.</p>\n\n<p>Looking to build on this success, I put forward the idea of running similarly structured events pre-go-live in the form of a GameDay. The aim was to gain similar learnings but by doing it before the system went live we sort capture issues in their infancy, before they have a chance to impact customers, and help make a judgement call on the service\u2019s readiness to go live - thus the \u201cFireDrill GameDay\u201d was born. In this post, I\u2019ll summarise some of the key elements that made these events a success and highlight some of the lessons learned from running these over the last two years.</p>\n\n<p>A FireDrill GameDay brings together two activities in Chaos Engineering :</p>\n\n<ul>\n  <li>GameDay: purposefully creating major failures with the goal of increasing reliability. Typically, they last between 2-4 hours and involve a team of engineers who either develop an application or support it.\n\u00a0\u00a0</li>\n  <li>Fire drill: routine and random testing of the failure modes of systems you build and the processes you use to support and secure what you have built. Deliberately disrupt part of a functioning system to see whether, and how, it recovers.</li>\n</ul>\n\n<p>A Firedrill GameDay is essentially a GameDay run using the tried and tested format we\u2019ve developed for fire drills; as such, I\u2019ll refer to them as GameDays for the remainder of this post. What makes the format slightly different from our standard fire drills is that they are less formal, have multiple scenarios, and in our case performed on pre-live systems.</p>\n\n<p>In regards to planning and execution, we found it useful to include the following elements:</p>\n\n<h3 id=\"scope\">Scope</h3>\n<p>By clearly defining the scope of the GameDay we can ensure that we focus on the correct areas of the service with the appropriate participants. The scope would generally cover all components of a given service but you may choose to focus on areas of a service where the risk of failure is high or the state of readiness (to go live) is questionable.</p>\n\n<h3 id=\"participants\">Participants</h3>\n<p>Those involved in the GameDay should be notified at the earliest opportunity to ensure they are available and preparation can start at the earliest oppotunity. Generally, involvement would be limited to those who are to be responsible for supporting the service and those involved in its development. The support team would primarily be involved in the investigation and resolution of incidents and the development team involved in planning and reviewing outcomes.</p>\n\n<p>One or more Excon\u2019s (exercise coordinator) will be required to run the incidents and it\u2019s also useful to assign someone to the roles of SLM (Service Lifecycle Manager) and IC (Incident Commander).</p>\n\n<h3 id=\"timetable\">Timetable</h3>\n<p>In advance of the GameDay publish a timeline of how the day will be broken down. Ensure that sufficient time is provided to investigate each issues, attempt to find a solution, and confirm that everything is back to normal. It is also advisable to have a break in between each scenario to reset the environment and give those running the GameDay a break as well as time to prep for the next scenario.</p>\n\n<p><img alt=\"Time Table\" src=\"https://sbg.technology/images/gameday_image1.png\" /></p>\n\n<h3 id=\"intro--overview\">Intro / Overview</h3>\n<p>Kick off the GameDay by providing a run down of the day\u2019s plans and an overview of the system architecture. This is particularly useful where the target system is yet to go live and the support personnel are unfamiliar with it. Having the architect or lead developer provide this overview may be particularly beneficial as is the provision of links to useful resources such as runbooks and monitoring.</p>\n\n<h3 id=\"scenarios\">Scenarios</h3>\n<p>Multiple failure scenarios to be devised and and tested well in advance, while ensuring they are kept hidden from those resposible for resolving the problem on the day. Devising the scenarios should be a team effort involving analysis of the system architecture and failure mode. Issues experienced during development should be used to pinpoint potential problem areas. Ultimately, a list of scenarios should be broken down and detailed as shown here:</p>\n\n<p><img alt=\"Scenarios\" src=\"https://sbg.technology/images/gameday_image2.png\" /></p>\n\n<p>The selected scenarios should be thoroughly tested to obtain a clear understanding of how the simulated incident will play out and the best candidates taken forward to the GameDay.</p>\n\n<p>Certain scenarios may benefit from simulated transactions being run against the service. This adds to the realism, with logs being populated with associated errors/warns, while also helping sign-posting the issue and its consequences.</p>\n\n<h3 id=\"execution\">Execution</h3>\n<p>During execution be clear on any rules that might apply e.g :</p>\n\n<ul>\n  <li>This GameDay will take place in the DR environment and be treated as if a live incident though all participants will be aware it is just a test exercise.\u00a0</li>\n  <li>Responders will be called out using Slack, assuming nothing automatically calls them out.</li>\n  <li>The Excon may drop hints where necessary and produce updates on the time remaining.</li>\n</ul>\n\n<p>During the exercise, it is good to keep reminding the participants of the questions we are seeking answers to :</p>\n\n<ul>\n  <li>Is system behaviour as expected?</li>\n  <li>Do you have the required access to services to investigate the issue?</li>\n  <li>Is the monitoring sufficient to detect the problem?</li>\n  <li>Do the monitoring dashboards aid insight and investigation?</li>\n  <li>Do alerts trigger as required and resolve following the incident?</li>\n  <li>Do you have visibility of the logs and did they help highlight the issue?</li>\n  <li>How good is the supporting documentation e.g architectural, support, etc?</li>\n  <li>Are there any issues with the incident management process?</li>\n  <li>Are there any areas we can change to improve the supportability of the service(s)?</li>\n  <li>Is there a clear understanding of the impact of the incident esp on the customer?</li>\n  <li>Does the service recover gracefully with acceptable impact?</li>\n</ul>\n\n<h3 id=\"retrospective\">Retrospective</h3>\n<p>Once all the scenarios have been executed those involved should be given time to take a break and consider any issues encountered especially in response to the questions asked in the previous section.\u00a0</p>\n\n<p>A retro board similar to the following may be used to decide which issues are to be fixed now and those which can be left until later:</p>\n\n<p><img alt=\"Retrospective\" src=\"https://sbg.technology/images/gameday_image3.png\" /></p>\n\n<p>We also made use of a Readiness-O-Meter to get a quick view of where people thought we were in regard to our ability to support the service.</p>\n\n<p><img alt=\"Readiness\" src=\"https://sbg.technology/images/gameday_image4.png\" /></p>\n\n<h2 id=\"lessons-learned\">Lessons Learned</h2>\n\n<p>Having now run several of these events - here are some of the key lessons learned:</p>\n\n<ul>\n  <li>\n    <p>Early involvement is key - start early in the project and get people involved from the outset. The GameDay should be treated as a project deliverable in its own right.</p>\n  </li>\n  <li>\n    <p>Choose scenarios wisely for maximum learnings on incidents that are likely to occur. This ensures you provide valuable insights into the service and how it is supported.</p>\n  </li>\n  <li>\n    <p>Devise multiple scenarios - not only will you need backups but the more scenarios, the more you\u2019ll learn about the service and the greater the potential for uncovering issues.</p>\n  </li>\n  <li>\n    <p>Resist efforts to include tests that should be done elsewhere; the GameDay is a complement not a substitute for more traditional forms of Operations Acceptance Testing like failure testing, backup recovery, and DR testing.</p>\n  </li>\n  <li>\n    <p>During the exercise do not lose sight of the end goal - keep reminding the participants of the questions we are seeking answers to e.g. Is system behaviour as expected? Are there any areas we can change to improve the supportability of the service(s)?</p>\n  </li>\n  <li>\n    <p>Make time for a closure-type event and associated activities ensuring that nothing gets left unresolved and without an owner.</p>\n  </li>\n  <li>\n    <p>Finally, the day of the GameDay is just a part of it - as important are the conversation, analysis and testing that happen in the lead-up.</p>\n  </li>\n</ul>"
      }
    ],
    "summary": "The FireDrill GameDay",
    "authors": [
      {
        "name": "paul_whitehead"
      }
    ],
    "author_detail": {
      "name": "paul_whitehead"
    },
    "author": "paul_whitehead",
    "tags": [
      {
        "term": "Operations",
        "scheme": null,
        "label": null
      },
      {
        "term": "devops,",
        "scheme": null,
        "label": null
      },
      {
        "term": "chaos",
        "scheme": null,
        "label": null
      },
      {
        "term": "engineering,",
        "scheme": null,
        "label": null
      },
      {
        "term": "fire",
        "scheme": null,
        "label": null
      },
      {
        "term": "drill,",
        "scheme": null,
        "label": null
      },
      {
        "term": "testing",
        "scheme": null,
        "label": null
      }
    ],
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://sbg.technology/feed.xml",
      "value": "The FireDrill GameDay"
    }
  }
}