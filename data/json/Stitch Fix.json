{
  "company": "Stitch Fix",
  "title": "Stitch Fix",
  "xmlUrl": "https://multithreaded.stitchfix.com/feed.xml",
  "htmlUrl": "http://multithreaded.stitchfix.com/blog/",
  "content": "\n\n\n\n\n\nTowards Service Deployment Agility | Stitch Fix Technology \u00e2\u0080\u0093 Multithreaded\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Engineering\n          \n\n\n\n            Algorithms\n          \n\n\n\n            Algorithms Tour\n          \n\n\n\n            Tour\n          \n\n\n\n            Careers\n          \n\n\n\n            Blog\n          \n\n\n\n\n\n\n\n\n            Engineering\n          \n\n            Algorithms\n          \n\n            Algorithms Tour\n          \n\n            Tour\n          \n\n            Careers\n          \n\n            Blog\n          \n\n\n\n\n\nTowards Service Deployment Agility\n\n\n\n\n\n\n\n\n\n\n\n                \n                Justin Lee, Kurt Bollacker, Oz Raza, Ujjwal Sarin, and Alex Milowski\n                \n              \n\n\n\n\n\n\n\n\n\nSeptember 19, 2023\n - San Francisco, California \n\n\n\n\n\nTweet this post!\n\n\n\nPost on LinkedIn\n\n\n\n\n\n\n\nIntroduction\nAt Stitch Fix, our data platform is designed to be self-service, with our users taking ownership of their own ETL, models, and microservices. To support this approach, the platform team prioritizes user autonomy and end-to-end tooling in their tooling designs, minimizing the involvement of platform engineers in day-to-day engineering and data science workflows.\nWe regularly evaluate our infrastructure against new frameworks to assess the costs and benefits of potential updates. Last year, we transitioned our EMR-based Spark infrastructure to EKS to take advantage of its benefits.  The next logical step was to re-examine how our core platform services were deployed and updated, especially those still deployed directly to instances.\nWe also recognized that Kubernetes, a popular open-source system for handling the deployment of containerized applications, could provide benefits for microservice orchestration beyond just batch compute infrastructure.  Our existing instance-based service deployment framework was beginning to show its age in areas that were critical for Stitch Fix, such as the development lifecycle\u00e2\u0080\u0099s velocity. As we saw an opportunity to leverage Kubernetes to address these issues, we identified pain points in our service deployment ecosystem.\nPain Points\n\n\nBuilding and deploying services can be a time-consuming process, involving multiple steps and dependencies. We needed a solution that would streamline this process and enable us to deploy services more efficiently.\n\n\nPolyglot environments, where multiple programming languages are used within the same system, can present a challenge for deployment standardization. At Stitch Fix, we use a variety of languages, including Python, Golang, Nodejs, and JVM (Java and Scala), making it difficult to establish consistent deployment practices across the board. We needed a way to deploy polyglot environments in a standardized and efficient manner.\n\n\nAutoscaling and rollback capabilities are essential for managing service deployment at scale. At Stitch Fix, we found that our existing infrastructure had limited capabilities in these areas. We needed a solution that would enable us to easily scale services up and down as needed, and quickly rollback changes if necessary.\n\n\nTraffic segmentation is an important aspect of service deployment, enabling us to direct traffic to different parts of our infrastructure based on various factors. However, we found that we were using different solutions for traffic segmentation across our various services, which created inconsistencies and added complexity to our infrastructure. We needed a solution that would enable us to manage traffic segmentation in a consistent and efficient manner.\n\n\nWe evaluated industry options and were pleased to realize that we could address our pain points by adopting open source tools and applying a dash of Stitch Fix flavor. And, so we did: introducing the Service Operator Service (SOS)!\nIntroducing: Service Operator Service (SOS)\nThe Philosophy\nSOS is a service deployment framework based on Kubernetes, designed to enable Stitch Fix data platform engineers, data engineers, and data scientists to manage their own deployment lifecycle through a self-service approach.\nWith SOS, end users can interact with a streamlined set of tools, without the need to learn or be aware of Kubernetes. Meanwhile, maintainers can remain agile with industry-standard tools by leveraging cloud-native open-source software such as Knative and building on top of a thin layer of opinionated APIs. Knative offers a simplified abstraction that streamlines certain aspects of the Kubernetes application deployment process, including the management of network configurations.\n\n\nSimplified diagram of the SOS architecture presenting the main components of the system.\n\nAddressing the Pain Points\nThe design of SOS focused on a set of key features centered around addressing our previously-detailed pain points:\nFast Deployments\nDeployments in SOS are lightning-fast, thanks to two design decisions: we use Knative to reduce overhead, and we run all services as artifacts on standardized Docker images.\nKnative\u00e2\u0080\u0099s quick pod provisioning is a major advantage, as we\u00e2\u0080\u0099ve discovered firsthand - our deployments can be up and running in under 10 seconds. One of the ways Knative achieves this is by using a shared load balancer for all deployments, which reduces the overhead of deploying a new service.\nIn SOS, each revision of a service is a self-contained artifact that runs on a curated Docker image specific to the service\u00e2\u0080\u0099s language - Python, NodeJS, Golang, or JVM. This approach, while standard for Golang binaries and JVM jars, can be more challenging with Python due to its less-robust dependency management and portability issues.\nWith this in mind, we set up a remote build process for each supported language that can compile an artifact on the same Docker image that the artifact would be run on. Each of the languages artifacts are defined as follows:\n\nGolang - Golang binary\nJVM - executable assembly jar file\nPython - shiv archive creates self-contained Python zipapps\nNodeJS - ncc compiles a NodeJS project into a single file\n\nBy bundling all dependencies within the artifact, we save significant time - anywhere from a few seconds to several minutes - that would otherwise be spent installing dependencies during runtime. As a result, services can enter a ready state more quickly. For services that have a more complex set of Python dependencies, we\u00e2\u0080\u0099ve seen time savings of more than 5 minutes!\nStandardized Build and Deployment Processes\nBuild Process\nIn addition to infrastructure work, we realized that for SOS to succeed, it was essential to offer a streamlined and straightforward method for users to create their artifacts. Without a standardized build and deploy process, users would have to sift through documentation to set up their local environments and handle processor architectures. We wanted to alleviate these overheads and provide users a least-resistance path to start experiencing SOS \u00e2\u0080\u0093  batteries are included!\nTo address this issue, we established a command-line tool where running sos build in your project directory will automatically detect project metadata (such as if it\u00e2\u0080\u0099s a Python or Golang deployment), perform integrity checks, and launch a remote build job that creates the SOS artifact on the same Docker image it would run on.\n\n\nWe standardized the build process with a command-line tool that initiates remote build jobs, reducing user overhead in setting up environments and mitigating compatibility issues between runtime environments.\n\nDeploy Process\nWe offer two interfaces for creating and updating service deployments in SOS: a Python client and a web-based UI built with React.\nThe Python client allows users to declare their services as a DSL in a familiar Python ecosystem. To ensure consistent types, we synchronize data models between the SOS client and the SOS API using shared protobuf files. We will cover the Python client\u00e2\u0080\u0099s user interface in more detail later.\nOur web-based UI allows users to easily review revision histories and adjust traffic policies. We have ensured feature parity between the Python client and the web UI, enabling our users to choose the interface that suits them best.\n\n\nOur UI provides an alternative way to easily manage deployments. This screenshot shows how the UI can be used to adjust the traffic segmentation between different revisions.\n\nTraffic Segmentation and Responsive Autoscaling Over Multiple Axes\nSOS included traffic segmentation and responsive scaling as a key feature for several reasons. These features enabled faster deployments and rollbacks, which facilitated shorter development cycles. Moreover, because all metrics and logs are namespaced by the traffic segmentation, we gained much better insights into service behavior. This feature also made gradual traffic cutovers possible and allowed for fast autoscaling to handle traffic bursts while reducing costs by scaling down unused instances.\nFor example, by setting services to use a minimum of zero instances, it is possible to make SOS services serverless. No pods would be provisioned until they receive traffic. Implementing this feature for our services allowed us to save significant costs on staging deployments and production services that receive less constant traffic.\nThe SOS User Experience\nBlissful Ignorance\nUsers don\u00e2\u0080\u0099t need to be aware that SOS runs on Kubernetes - all they need to do to start deploying services is install the SOS Python client!\nDecoupled Application and System\nSOS ensures that each user\u00e2\u0080\u0099s new code deployment is immutable and has a unique ID, tied to an SOS artifact with its own dependencies. This decouples the system and application, setting clear boundaries and avoiding the issue of Nginx configs and secrets management bundled into an application\u00e2\u0080\u0099s source code that we faced with our previous instance-based deployment.\nWe wanted to avoid a scenario where users inherit and patch and layers of Docker images until they become unmaintainable black boxes. By running all service artifacts on top of curated Docker images, we avoided the problem of users managing their own Docker images. This made it easier to implement platform-wide changes over time, as we could patch and improve the Docker images without relying on users to rebuild their applications. This also allowed us to efficiently tweak and fine-tune the observability agents on the system.\nSingle Source of Truth\nThe SOS service itself is the source of truth for each deployment, so users don\u00e2\u0080\u0099t need to manage multiple configurations for the same service. When deploying a service, the SOS service will merge any necessary files at the service level, which prevents confusion that can arise when configuration files are stored in different branches or exist only in a user\u00e2\u0080\u0099s local environment.\nThe Experience\nThe SOS Python client is the tool that our users use to interact with SOS. Below are some examples of the SOS Python client and how a user would use it to define their service deployment.\nService Creation\nEach SOS service correlates with a service that\u00e2\u0080\u0099s created in Kubernetes.\nservice = Service(\n  id=\"sfix_service\",\n  owner_id=\":team:sfix_team_name\",\n  env=\"prod\",\n)\n\nCreating a Revision\nIn SOS, a service can have multiple revisions, with each revision representing an immutable version of the service that can be deployed independently.\nThe example below shows the minimal configurations to create a revision that deploys a Python artifact. The Artifact is a self-contained Python artifact that has been deployed to Artifactory, which we use as our sink of artifacts.\nrevision = Revision(\n  id=\"sfix_service-5\",\n  artifact=Artifact(name=\"sfix_service\", version=\"v0.0.16\"),\n  python_config=PythonConfig(\n          python_version=\"3.9\",\n          gunicorn_config=GunicornConfig(worker_class=\"gevent\"),\n          app_module=\"sfix_service.server:app\",\n  ),\n)\n\nlogging.info(service.add_revision(revision))\n\nAdditional settings can be configured for more complex deployments, such as port number, environment variables, memory and CPU allocation, health checks, and thread and worker counts. Here\u00e2\u0080\u0099s an example of how to set these configurations:\nrevision = Revision(\n  id=\"sfix_service-5\",\n  artifact=Artifact(name=\"sfix_service\", version=\"v0.0.16\"),\n  container=Container(\n          port=5001,\n          env_vars=[EnvVar(id=\"env_var\", value=\"env_value\")],\n          resources=Resources(memory=\"2G\", cpu=\"1000m\"),\n  ),\n  health_check=HealthCheck(liveness_probe=\"/\", readiness_probe=\"/\"),\n  sf_env=\"prod\",\n  python_config=PythonConfig(\n          python_version=\"3.9\",\n          gunicorn_config=GunicornConfig(worker_count=4, thread_count=8, worker_class=\"gevent\"),\n          app_module=\"sfix_service.server:app\",\n  ),\n  autoscaling=Autoscaling(min_scale=6),\n)\n\nlogging.info(service.add_revision(revision))\n\nSet Traffic for New Revision\nWe recognized the importance of providing robust support for traffic segmentation, as users should have confidence that deploying new code wouldn\u00e2\u0080\u0099t negatively impact their entire system. By giving them complete control over the flow of traffic, they could confidently iterate on changes without fear of breaking everything. This was crucial to enabling faster development cycles and smoother rollbacks, as well as improving our understanding of service behavior through fine-grained traffic analysis.\nThe following example shows how the newly created revision can be set to receive 100% traffic. However, to roll out changes more safely, traffic can be segmented between multiple revisions by adding more Route objects to the TrafficRoute.routes configuration. SOS\u00e2\u0080\u0099s observability tooling can be used to monitor the traffic and ensure a smooth transition.\nservice.add_traffic(\n    Traffic(\n        traffic_route=TrafficRoute(\n            routes=[\n                Route(\n                    percent=100,\n                    revision_id=\"sfix_service-5\",\n                    dns_record_suffix=\"active\",\n                ),\n            ]\n        ),\n    )\n)\n\nThe Present and the Future\nMost data platform services have already been migrated to SOS.  It hosts many critical services, including the components of our batch processor, job execution engine, our metastore, configuration management service, Kafka connect cluster, ownership discovery service, and more! While there are some data science services deployed on SOS, most of the currently-deployed services are focused on powering our data platform.\nSOS aims to streamline the service lifecycle, yet its opinionated approach might not suit all deployments. Certain open-source tools align better with standard industry patterns. For instance, we deploy Trino directly onto Kubernetes and use a Helm Chart for Airflow. While SOS isn\u00e2\u0080\u0099t a one-size-fits-all solution, it remains highly effective for the majority of our existing and future deployments.\nWe are regularly improving the user experience of SOS by migrating existing services and building new ones on top of it. Our goal is to reduce the complexity of service deployment and continue to empower our data scientists and engineers at Stitch Fix to be more efficient and effective.\n\n\n\n\n\n\nTweet this post!\n\n\n\nPost on LinkedIn\n\n\n\n\n\n\n\nCome Work with Us!\n\n      We\u00e2\u0080\u0099re a diverse team dedicated to building great products, and we\u00e2\u0080\u0099d love your help. Do you want to build\n      amazing products with amazing peers? Join us!\n    \n\nAll Careers at Stitch Fix\n\n\n \n\n\n\n\nStitch Fix and Fix aretrademarks of Stitch Fix, Inc.\n\n\n\n\nStitch Fix Home\nFAQ\nPress\n\n\nMaternity\nBig and tall\nJeans\nBusiness Casual\nPetite\nPlus\n\n\nTech Blog\nTech Careers\nTerms of Use\nPrivacy Policy\n\n\n\n\nFollow Us!\n\n\n\n\n\n\n\nFollow Us!\n\n\n\n\n\nTech Blog\nTech Careers\nStitch Fix Home\nFAQ\nPress\nTerms of Use\nPrivacy Policy\n\n\nStitch Fix and Fix are trademarks of Stitch Fix, Inc.\n\n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "title": "Towards Service Deployment Agility",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://multithreaded.stitchfix.com/feed.xml",
      "value": "Towards Service Deployment Agility"
    },
    "summary": "<h1 id=\"introduction\">Introduction</h1>\n<p>At Stitch Fix, our data platform is designed to be self-service, with our users taking ownership of their own ETL, models, and microservices. To support this approach, the platform team prioritizes user autonomy and end-to-end tooling in their tooling designs, minimizing the involvement of platform engineers in day-to-day engineering and data science workflows.</p>\n\n<p>We regularly evaluate our infrastructure against new frameworks to assess the costs and benefits of potential updates. Last year, we transitioned our <a href=\"https://multithreaded.stitchfix.com/blog/2022/03/14/spark-eks/\">EMR-based Spark infrastructure to EKS</a> to take advantage of its benefits.  The next logical step was to re-examine how our core platform services were deployed and updated, especially those still deployed directly to instances.</p>\n\n<p>We also recognized that Kubernetes, a popular open-source system for handling the deployment of containerized applications, could provide benefits for microservice orchestration beyond just batch compute infrastructure.  Our existing instance-based service deployment framework was beginning to show its age in areas that were critical for Stitch Fix, such as the development lifecycle\u2019s velocity. As we saw an opportunity to leverage Kubernetes to address these issues, we identified pain points in our service deployment ecosystem.</p>\n\n<h2 id=\"pain-points\">Pain Points</h2>\n<ol>\n  <li>\n    <p><strong>Building and deploying services can be a time-consuming process</strong>, involving multiple steps and dependencies. We needed a solution that would streamline this process and enable us to deploy services more efficiently.</p>\n  </li>\n  <li>\n    <p><strong>Polyglot environments</strong>, where multiple programming languages are used within the same system, can present a challenge for deployment standardization. At Stitch Fix, we use a variety of languages, including Python, Golang, Nodejs, and JVM (Java and Scala), making it difficult to establish consistent deployment practices across the board. We needed a way to deploy polyglot environments in a standardized and efficient manner.</p>\n  </li>\n  <li>\n    <p><strong>Autoscaling and rollback capabilities</strong> are essential for managing service deployment at scale. At Stitch Fix, we found that our existing infrastructure had limited capabilities in these areas. We needed a solution that would enable us to easily scale services up and down as needed, and quickly rollback changes if necessary.</p>\n  </li>\n  <li>\n    <p><strong>Traffic segmentation</strong> is an important aspect of service deployment, enabling us to direct traffic to different parts of our infrastructure based on various factors. However, we found that we were using different solutions for traffic segmentation across our various services, which created inconsistencies and added complexity to our infrastructure. We needed a solution that would enable us to manage traffic segmentation in a consistent and efficient manner.</p>\n  </li>\n</ol>\n\n<p>We evaluated industry options and were pleased to realize that we could address our pain points by adopting open source tools and applying a dash of Stitch Fix flavor. And, so we did: introducing the <strong>Service Operator Service (SOS)</strong>!</p>\n\n<h1 id=\"introducing-service-operator-service-sos\">Introducing: Service Operator Service (SOS)</h1>\n<h2 id=\"the-philosophy\">The Philosophy</h2>\n<p>SOS is a service deployment framework based on Kubernetes, designed to enable Stitch Fix data platform engineers, data engineers, and data scientists to manage their own deployment lifecycle through a self-service approach.</p>\n\n<p>With SOS, end users can interact with a streamlined set of tools, without the need to learn or be aware of Kubernetes. Meanwhile, maintainers can remain agile with industry-standard tools by leveraging cloud-native open-source software such as <a href=\"https://knative.dev/\">Knative</a> and building on top of a thin layer of opinionated APIs. Knative offers a simplified abstraction that streamlines certain aspects of the Kubernetes application deployment process, including the management of network configurations.</p>\n\n<figure>\n    <img src=\"https://multithreaded.stitchfix.com/assets/posts/2023-09-19-towards-service-deployment-agility/sos-bov-architecture.png\" style=\"width: 100%;\" />\n    <figcaption style=\"text-align: center;\">Simplified diagram of the SOS architecture presenting the main components of the system.</figcaption>\n</figure>\n\n<h2 id=\"addressing-the-pain-points\">Addressing the Pain Points</h2>\n<p>The design of SOS focused on a set of key features centered around addressing our previously-detailed pain points:</p>\n\n<h3 id=\"fast-deployments\">Fast Deployments</h3>\n<p>Deployments in SOS are lightning-fast, thanks to two design decisions: we use Knative to reduce overhead, and we run all services as artifacts on standardized Docker images.</p>\n\n<p>Knative\u2019s quick pod provisioning is a major advantage, as we\u2019ve discovered firsthand - our deployments can be up and running in under 10 seconds. One of the ways Knative achieves this is by using a shared load balancer for all deployments, which reduces the overhead of deploying a new service.</p>\n\n<p>In SOS, each revision of a service is a self-contained artifact that runs on a curated Docker image specific to the service\u2019s language - Python, NodeJS, Golang, or JVM. This approach, while standard for Golang binaries and JVM jars, can be more challenging with Python due to its less-robust dependency management and portability issues.</p>\n\n<p>With this in mind, we set up a remote build process for each supported language that can compile an artifact on the same Docker image that the artifact would be run on. Each of the languages artifacts are defined as follows:</p>\n<ul>\n  <li>Golang - Golang binary</li>\n  <li>JVM - executable assembly jar file</li>\n  <li>Python - <a href=\"https://shiv.readthedocs.io/en/latest/\">shiv archive</a> creates self-contained Python zipapps</li>\n  <li>NodeJS - <a href=\"https://github.com/vercel/ncc\">ncc</a> compiles a NodeJS project into a single file</li>\n</ul>\n\n<p>By bundling all dependencies within the artifact, we save significant time - anywhere from a few seconds to several minutes - that would otherwise be spent installing dependencies during runtime. As a result, services can enter a ready state more quickly. For services that have a more complex set of Python dependencies, we\u2019ve seen time savings of more than 5 minutes!</p>\n\n<h3 id=\"standardized-build-and-deployment-processes\">Standardized Build and Deployment Processes</h3>\n<h4 id=\"build-process\">Build Process</h4>\n<p>In addition to infrastructure work, we realized that for SOS to succeed, it was essential to offer a streamlined and straightforward method for users to create their artifacts. Without a standardized build and deploy process, users would have to sift through documentation to set up their local environments and handle processor architectures. We wanted to alleviate these overheads and provide users a least-resistance path to start experiencing SOS \u2013  batteries <em>are</em> included!</p>\n\n<p>To address this issue, we established a command-line tool where running <code class=\"language-plaintext highlighter-rouge\">sos build</code> in your project directory will automatically detect project metadata (such as if it\u2019s a Python or Golang deployment), perform integrity checks, and launch a remote build job that creates the SOS artifact on the same Docker image it would run on.</p>\n\n<figure>\n    <img src=\"https://multithreaded.stitchfix.com/assets/posts/2023-09-19-towards-service-deployment-agility/sos-build-deploy.png\" style=\"width: 100%;\" />\n    <figcaption style=\"text-align: center;\">We standardized the build process with a command-line tool that initiates remote build jobs, reducing user overhead in setting up environments and mitigating compatibility issues between runtime environments.</figcaption>\n</figure>\n\n<h4 id=\"deploy-process\">Deploy Process</h4>\n<p>We offer two interfaces for creating and updating service deployments in SOS: a Python client and a web-based UI built with React.</p>\n\n<p>The Python client allows users to declare their services as a DSL in a familiar Python ecosystem. To ensure consistent types, we synchronize data models between the SOS client and the SOS API using shared protobuf files. We will cover the Python client\u2019s user interface in more detail later.</p>\n\n<p>Our web-based UI allows users to easily review revision histories and adjust traffic policies. We have ensured feature parity between the Python client and the web UI, enabling our users to choose the interface that suits them best.</p>\n\n<figure>\n    <img src=\"https://multithreaded.stitchfix.com/assets/posts/2023-09-19-towards-service-deployment-agility/sos-ui.png\" style=\"width: 100%;\" />\n    <figcaption style=\"text-align: center;\">Our UI provides an alternative way to easily manage deployments. This screenshot shows how the UI can be used to adjust the traffic segmentation between different revisions.</figcaption>\n</figure>\n\n<h3 id=\"traffic-segmentation-and-responsive-autoscaling-over-multiple-axes\">Traffic Segmentation and Responsive Autoscaling Over Multiple Axes</h3>\n<p>SOS included traffic segmentation and responsive scaling as a key feature for several reasons. These features enabled faster deployments and rollbacks, which facilitated shorter development cycles. Moreover, because all metrics and logs are namespaced by the traffic segmentation, we gained much better insights into service behavior. This feature also made gradual traffic cutovers possible and allowed for fast autoscaling to handle traffic bursts while reducing costs by scaling down unused instances.</p>\n\n<p>For example, by setting services to use a minimum of zero instances, it is possible to make SOS services serverless. No pods would be provisioned until they receive traffic. Implementing this feature for our services allowed us to save significant costs on staging deployments and production services that receive less constant traffic.</p>\n\n<h1 id=\"the-sos-user-experience\">The SOS User Experience</h1>\n<h2 id=\"blissful-ignorance\">Blissful Ignorance</h2>\n<p>Users don\u2019t need to be aware that SOS runs on Kubernetes - all they need to do to start deploying services is install the SOS Python client!</p>\n\n<h2 id=\"decoupled-application-and-system\">Decoupled Application and System</h2>\n<p>SOS ensures that each user\u2019s new code deployment is immutable and has a unique ID, tied to an SOS artifact with its own dependencies. This decouples the system and application, setting clear boundaries and avoiding the issue of Nginx configs and secrets management bundled into an application\u2019s source code that we faced with our previous instance-based deployment.</p>\n\n<p>We wanted to avoid a scenario where users inherit and patch and layers of Docker images until they become unmaintainable black boxes. By running all service artifacts on top of curated Docker images, we avoided the problem of users managing their own Docker images. This made it easier to implement platform-wide changes over time, as we could patch and improve the Docker images without relying on users to rebuild their applications. This also allowed us to efficiently tweak and fine-tune the observability agents on the system.</p>\n\n<h2 id=\"single-source-of-truth\">Single Source of Truth</h2>\n<p>The SOS service itself is the source of truth for each deployment, so users don\u2019t need to manage multiple configurations for the same service. When deploying a service, the SOS service will merge any necessary files at the service level, which prevents confusion that can arise when configuration files are stored in different branches or exist only in a user\u2019s local environment.</p>\n\n<h2 id=\"the-experience\">The Experience</h2>\n<p>The SOS Python client is the tool that our users use to interact with SOS. Below are some examples of the SOS Python client and how a user would use it to define their service deployment.</p>\n\n<h3 id=\"service-creation\">Service Creation</h3>\n<p>Each SOS service correlates with a service that\u2019s created in Kubernetes.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">service</span> <span class=\"o\">=</span> <span class=\"n\">Service</span><span class=\"p\">(</span>\n  <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service\"</span><span class=\"p\">,</span>\n  <span class=\"n\">owner_id</span><span class=\"o\">=</span><span class=\"s\">\":team:sfix_team_name\"</span><span class=\"p\">,</span>\n  <span class=\"n\">env</span><span class=\"o\">=</span><span class=\"s\">\"prod\"</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h3 id=\"creating-a-revision\">Creating a Revision</h3>\n<p>In SOS, a service can have multiple revisions, with each revision representing an immutable version of the service that can be deployed independently.</p>\n\n<p>The example below shows the minimal configurations to create a revision that deploys a Python artifact. The <code class=\"language-plaintext highlighter-rouge\">Artifact</code> is a self-contained Python artifact that has been deployed to Artifactory, which we use as our sink of artifacts.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">revision</span> <span class=\"o\">=</span> <span class=\"n\">Revision</span><span class=\"p\">(</span>\n  <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service-5\"</span><span class=\"p\">,</span>\n  <span class=\"n\">artifact</span><span class=\"o\">=</span><span class=\"n\">Artifact</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service\"</span><span class=\"p\">,</span> <span class=\"n\">version</span><span class=\"o\">=</span><span class=\"s\">\"v0.0.16\"</span><span class=\"p\">),</span>\n  <span class=\"n\">python_config</span><span class=\"o\">=</span><span class=\"n\">PythonConfig</span><span class=\"p\">(</span>\n          <span class=\"n\">python_version</span><span class=\"o\">=</span><span class=\"s\">\"3.9\"</span><span class=\"p\">,</span>\n          <span class=\"n\">gunicorn_config</span><span class=\"o\">=</span><span class=\"n\">GunicornConfig</span><span class=\"p\">(</span><span class=\"n\">worker_class</span><span class=\"o\">=</span><span class=\"s\">\"gevent\"</span><span class=\"p\">),</span>\n          <span class=\"n\">app_module</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service.server:app\"</span><span class=\"p\">,</span>\n  <span class=\"p\">),</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">service</span><span class=\"p\">.</span><span class=\"n\">add_revision</span><span class=\"p\">(</span><span class=\"n\">revision</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<p>Additional settings can be configured for more complex deployments, such as port number, environment variables, memory and CPU allocation, health checks, and thread and worker counts. Here\u2019s an example of how to set these configurations:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">revision</span> <span class=\"o\">=</span> <span class=\"n\">Revision</span><span class=\"p\">(</span>\n  <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service-5\"</span><span class=\"p\">,</span>\n  <span class=\"n\">artifact</span><span class=\"o\">=</span><span class=\"n\">Artifact</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service\"</span><span class=\"p\">,</span> <span class=\"n\">version</span><span class=\"o\">=</span><span class=\"s\">\"v0.0.16\"</span><span class=\"p\">),</span>\n  <span class=\"n\">container</span><span class=\"o\">=</span><span class=\"n\">Container</span><span class=\"p\">(</span>\n          <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">5001</span><span class=\"p\">,</span>\n          <span class=\"n\">env_vars</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">EnvVar</span><span class=\"p\">(</span><span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"env_var\"</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"s\">\"env_value\"</span><span class=\"p\">)],</span>\n          <span class=\"n\">resources</span><span class=\"o\">=</span><span class=\"n\">Resources</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s\">\"2G\"</span><span class=\"p\">,</span> <span class=\"n\">cpu</span><span class=\"o\">=</span><span class=\"s\">\"1000m\"</span><span class=\"p\">),</span>\n  <span class=\"p\">),</span>\n  <span class=\"n\">health_check</span><span class=\"o\">=</span><span class=\"n\">HealthCheck</span><span class=\"p\">(</span><span class=\"n\">liveness_probe</span><span class=\"o\">=</span><span class=\"s\">\"/\"</span><span class=\"p\">,</span> <span class=\"n\">readiness_probe</span><span class=\"o\">=</span><span class=\"s\">\"/\"</span><span class=\"p\">),</span>\n  <span class=\"n\">sf_env</span><span class=\"o\">=</span><span class=\"s\">\"prod\"</span><span class=\"p\">,</span>\n  <span class=\"n\">python_config</span><span class=\"o\">=</span><span class=\"n\">PythonConfig</span><span class=\"p\">(</span>\n          <span class=\"n\">python_version</span><span class=\"o\">=</span><span class=\"s\">\"3.9\"</span><span class=\"p\">,</span>\n          <span class=\"n\">gunicorn_config</span><span class=\"o\">=</span><span class=\"n\">GunicornConfig</span><span class=\"p\">(</span><span class=\"n\">worker_count</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">thread_count</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">worker_class</span><span class=\"o\">=</span><span class=\"s\">\"gevent\"</span><span class=\"p\">),</span>\n          <span class=\"n\">app_module</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service.server:app\"</span><span class=\"p\">,</span>\n  <span class=\"p\">),</span>\n  <span class=\"n\">autoscaling</span><span class=\"o\">=</span><span class=\"n\">Autoscaling</span><span class=\"p\">(</span><span class=\"n\">min_scale</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">),</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">service</span><span class=\"p\">.</span><span class=\"n\">add_revision</span><span class=\"p\">(</span><span class=\"n\">revision</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<h3 id=\"set-traffic-for-new-revision\">Set Traffic for New Revision</h3>\n<p>We recognized the importance of providing robust support for traffic segmentation, as users should have confidence that deploying new code wouldn\u2019t negatively impact their entire system. By giving them complete control over the flow of traffic, they could confidently iterate on changes without fear of breaking everything. This was crucial to enabling faster development cycles and smoother rollbacks, as well as improving our understanding of service behavior through fine-grained traffic analysis.</p>\n\n<p>The following example shows how the newly created revision can be set to receive 100% traffic. However, to roll out changes more safely, traffic can be segmented between multiple revisions by adding more Route objects to the TrafficRoute.routes configuration. SOS\u2019s observability tooling can be used to monitor the traffic and ensure a smooth transition.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">service</span><span class=\"p\">.</span><span class=\"n\">add_traffic</span><span class=\"p\">(</span>\n    <span class=\"n\">Traffic</span><span class=\"p\">(</span>\n        <span class=\"n\">traffic_route</span><span class=\"o\">=</span><span class=\"n\">TrafficRoute</span><span class=\"p\">(</span>\n            <span class=\"n\">routes</span><span class=\"o\">=</span><span class=\"p\">[</span>\n                <span class=\"n\">Route</span><span class=\"p\">(</span>\n                    <span class=\"n\">percent</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n                    <span class=\"n\">revision_id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service-5\"</span><span class=\"p\">,</span>\n                    <span class=\"n\">dns_record_suffix</span><span class=\"o\">=</span><span class=\"s\">\"active\"</span><span class=\"p\">,</span>\n                <span class=\"p\">),</span>\n            <span class=\"p\">]</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h1 id=\"the-present-and-the-future\">The Present and the Future</h1>\n<p>Most data platform services have already been migrated to SOS.  It hosts many critical services, including the components of our batch processor, <a href=\"https://github.com/stitchfix/flotilla-os\">job execution engine</a>, our metastore, configuration management service, Kafka connect cluster, ownership discovery service, and more! While there are some data science services deployed on SOS, most of the currently-deployed services are focused on powering our data platform.</p>\n\n<p>SOS aims to streamline the service lifecycle, yet its opinionated approach might not suit all deployments. Certain open-source tools align better with standard industry patterns. For instance, we deploy Trino directly onto Kubernetes and use a Helm Chart for Airflow. While SOS isn\u2019t a one-size-fits-all solution, it remains highly effective for the majority of our existing and future deployments.</p>\n\n<p>We are regularly improving the user experience of SOS by migrating existing services and building new ones on top of it. Our goal is to reduce the complexity of service deployment and continue to empower our data scientists and engineers at Stitch Fix to be more efficient and effective.</p>",
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://multithreaded.stitchfix.com/feed.xml",
      "value": "<h1 id=\"introduction\">Introduction</h1>\n<p>At Stitch Fix, our data platform is designed to be self-service, with our users taking ownership of their own ETL, models, and microservices. To support this approach, the platform team prioritizes user autonomy and end-to-end tooling in their tooling designs, minimizing the involvement of platform engineers in day-to-day engineering and data science workflows.</p>\n\n<p>We regularly evaluate our infrastructure against new frameworks to assess the costs and benefits of potential updates. Last year, we transitioned our <a href=\"https://multithreaded.stitchfix.com/blog/2022/03/14/spark-eks/\">EMR-based Spark infrastructure to EKS</a> to take advantage of its benefits.  The next logical step was to re-examine how our core platform services were deployed and updated, especially those still deployed directly to instances.</p>\n\n<p>We also recognized that Kubernetes, a popular open-source system for handling the deployment of containerized applications, could provide benefits for microservice orchestration beyond just batch compute infrastructure.  Our existing instance-based service deployment framework was beginning to show its age in areas that were critical for Stitch Fix, such as the development lifecycle\u2019s velocity. As we saw an opportunity to leverage Kubernetes to address these issues, we identified pain points in our service deployment ecosystem.</p>\n\n<h2 id=\"pain-points\">Pain Points</h2>\n<ol>\n  <li>\n    <p><strong>Building and deploying services can be a time-consuming process</strong>, involving multiple steps and dependencies. We needed a solution that would streamline this process and enable us to deploy services more efficiently.</p>\n  </li>\n  <li>\n    <p><strong>Polyglot environments</strong>, where multiple programming languages are used within the same system, can present a challenge for deployment standardization. At Stitch Fix, we use a variety of languages, including Python, Golang, Nodejs, and JVM (Java and Scala), making it difficult to establish consistent deployment practices across the board. We needed a way to deploy polyglot environments in a standardized and efficient manner.</p>\n  </li>\n  <li>\n    <p><strong>Autoscaling and rollback capabilities</strong> are essential for managing service deployment at scale. At Stitch Fix, we found that our existing infrastructure had limited capabilities in these areas. We needed a solution that would enable us to easily scale services up and down as needed, and quickly rollback changes if necessary.</p>\n  </li>\n  <li>\n    <p><strong>Traffic segmentation</strong> is an important aspect of service deployment, enabling us to direct traffic to different parts of our infrastructure based on various factors. However, we found that we were using different solutions for traffic segmentation across our various services, which created inconsistencies and added complexity to our infrastructure. We needed a solution that would enable us to manage traffic segmentation in a consistent and efficient manner.</p>\n  </li>\n</ol>\n\n<p>We evaluated industry options and were pleased to realize that we could address our pain points by adopting open source tools and applying a dash of Stitch Fix flavor. And, so we did: introducing the <strong>Service Operator Service (SOS)</strong>!</p>\n\n<h1 id=\"introducing-service-operator-service-sos\">Introducing: Service Operator Service (SOS)</h1>\n<h2 id=\"the-philosophy\">The Philosophy</h2>\n<p>SOS is a service deployment framework based on Kubernetes, designed to enable Stitch Fix data platform engineers, data engineers, and data scientists to manage their own deployment lifecycle through a self-service approach.</p>\n\n<p>With SOS, end users can interact with a streamlined set of tools, without the need to learn or be aware of Kubernetes. Meanwhile, maintainers can remain agile with industry-standard tools by leveraging cloud-native open-source software such as <a href=\"https://knative.dev/\">Knative</a> and building on top of a thin layer of opinionated APIs. Knative offers a simplified abstraction that streamlines certain aspects of the Kubernetes application deployment process, including the management of network configurations.</p>\n\n<figure>\n    <img src=\"https://multithreaded.stitchfix.com/assets/posts/2023-09-19-towards-service-deployment-agility/sos-bov-architecture.png\" style=\"width: 100%;\" />\n    <figcaption style=\"text-align: center;\">Simplified diagram of the SOS architecture presenting the main components of the system.</figcaption>\n</figure>\n\n<h2 id=\"addressing-the-pain-points\">Addressing the Pain Points</h2>\n<p>The design of SOS focused on a set of key features centered around addressing our previously-detailed pain points:</p>\n\n<h3 id=\"fast-deployments\">Fast Deployments</h3>\n<p>Deployments in SOS are lightning-fast, thanks to two design decisions: we use Knative to reduce overhead, and we run all services as artifacts on standardized Docker images.</p>\n\n<p>Knative\u2019s quick pod provisioning is a major advantage, as we\u2019ve discovered firsthand - our deployments can be up and running in under 10 seconds. One of the ways Knative achieves this is by using a shared load balancer for all deployments, which reduces the overhead of deploying a new service.</p>\n\n<p>In SOS, each revision of a service is a self-contained artifact that runs on a curated Docker image specific to the service\u2019s language - Python, NodeJS, Golang, or JVM. This approach, while standard for Golang binaries and JVM jars, can be more challenging with Python due to its less-robust dependency management and portability issues.</p>\n\n<p>With this in mind, we set up a remote build process for each supported language that can compile an artifact on the same Docker image that the artifact would be run on. Each of the languages artifacts are defined as follows:</p>\n<ul>\n  <li>Golang - Golang binary</li>\n  <li>JVM - executable assembly jar file</li>\n  <li>Python - <a href=\"https://shiv.readthedocs.io/en/latest/\">shiv archive</a> creates self-contained Python zipapps</li>\n  <li>NodeJS - <a href=\"https://github.com/vercel/ncc\">ncc</a> compiles a NodeJS project into a single file</li>\n</ul>\n\n<p>By bundling all dependencies within the artifact, we save significant time - anywhere from a few seconds to several minutes - that would otherwise be spent installing dependencies during runtime. As a result, services can enter a ready state more quickly. For services that have a more complex set of Python dependencies, we\u2019ve seen time savings of more than 5 minutes!</p>\n\n<h3 id=\"standardized-build-and-deployment-processes\">Standardized Build and Deployment Processes</h3>\n<h4 id=\"build-process\">Build Process</h4>\n<p>In addition to infrastructure work, we realized that for SOS to succeed, it was essential to offer a streamlined and straightforward method for users to create their artifacts. Without a standardized build and deploy process, users would have to sift through documentation to set up their local environments and handle processor architectures. We wanted to alleviate these overheads and provide users a least-resistance path to start experiencing SOS \u2013  batteries <em>are</em> included!</p>\n\n<p>To address this issue, we established a command-line tool where running <code class=\"language-plaintext highlighter-rouge\">sos build</code> in your project directory will automatically detect project metadata (such as if it\u2019s a Python or Golang deployment), perform integrity checks, and launch a remote build job that creates the SOS artifact on the same Docker image it would run on.</p>\n\n<figure>\n    <img src=\"https://multithreaded.stitchfix.com/assets/posts/2023-09-19-towards-service-deployment-agility/sos-build-deploy.png\" style=\"width: 100%;\" />\n    <figcaption style=\"text-align: center;\">We standardized the build process with a command-line tool that initiates remote build jobs, reducing user overhead in setting up environments and mitigating compatibility issues between runtime environments.</figcaption>\n</figure>\n\n<h4 id=\"deploy-process\">Deploy Process</h4>\n<p>We offer two interfaces for creating and updating service deployments in SOS: a Python client and a web-based UI built with React.</p>\n\n<p>The Python client allows users to declare their services as a DSL in a familiar Python ecosystem. To ensure consistent types, we synchronize data models between the SOS client and the SOS API using shared protobuf files. We will cover the Python client\u2019s user interface in more detail later.</p>\n\n<p>Our web-based UI allows users to easily review revision histories and adjust traffic policies. We have ensured feature parity between the Python client and the web UI, enabling our users to choose the interface that suits them best.</p>\n\n<figure>\n    <img src=\"https://multithreaded.stitchfix.com/assets/posts/2023-09-19-towards-service-deployment-agility/sos-ui.png\" style=\"width: 100%;\" />\n    <figcaption style=\"text-align: center;\">Our UI provides an alternative way to easily manage deployments. This screenshot shows how the UI can be used to adjust the traffic segmentation between different revisions.</figcaption>\n</figure>\n\n<h3 id=\"traffic-segmentation-and-responsive-autoscaling-over-multiple-axes\">Traffic Segmentation and Responsive Autoscaling Over Multiple Axes</h3>\n<p>SOS included traffic segmentation and responsive scaling as a key feature for several reasons. These features enabled faster deployments and rollbacks, which facilitated shorter development cycles. Moreover, because all metrics and logs are namespaced by the traffic segmentation, we gained much better insights into service behavior. This feature also made gradual traffic cutovers possible and allowed for fast autoscaling to handle traffic bursts while reducing costs by scaling down unused instances.</p>\n\n<p>For example, by setting services to use a minimum of zero instances, it is possible to make SOS services serverless. No pods would be provisioned until they receive traffic. Implementing this feature for our services allowed us to save significant costs on staging deployments and production services that receive less constant traffic.</p>\n\n<h1 id=\"the-sos-user-experience\">The SOS User Experience</h1>\n<h2 id=\"blissful-ignorance\">Blissful Ignorance</h2>\n<p>Users don\u2019t need to be aware that SOS runs on Kubernetes - all they need to do to start deploying services is install the SOS Python client!</p>\n\n<h2 id=\"decoupled-application-and-system\">Decoupled Application and System</h2>\n<p>SOS ensures that each user\u2019s new code deployment is immutable and has a unique ID, tied to an SOS artifact with its own dependencies. This decouples the system and application, setting clear boundaries and avoiding the issue of Nginx configs and secrets management bundled into an application\u2019s source code that we faced with our previous instance-based deployment.</p>\n\n<p>We wanted to avoid a scenario where users inherit and patch and layers of Docker images until they become unmaintainable black boxes. By running all service artifacts on top of curated Docker images, we avoided the problem of users managing their own Docker images. This made it easier to implement platform-wide changes over time, as we could patch and improve the Docker images without relying on users to rebuild their applications. This also allowed us to efficiently tweak and fine-tune the observability agents on the system.</p>\n\n<h2 id=\"single-source-of-truth\">Single Source of Truth</h2>\n<p>The SOS service itself is the source of truth for each deployment, so users don\u2019t need to manage multiple configurations for the same service. When deploying a service, the SOS service will merge any necessary files at the service level, which prevents confusion that can arise when configuration files are stored in different branches or exist only in a user\u2019s local environment.</p>\n\n<h2 id=\"the-experience\">The Experience</h2>\n<p>The SOS Python client is the tool that our users use to interact with SOS. Below are some examples of the SOS Python client and how a user would use it to define their service deployment.</p>\n\n<h3 id=\"service-creation\">Service Creation</h3>\n<p>Each SOS service correlates with a service that\u2019s created in Kubernetes.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">service</span> <span class=\"o\">=</span> <span class=\"n\">Service</span><span class=\"p\">(</span>\n  <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service\"</span><span class=\"p\">,</span>\n  <span class=\"n\">owner_id</span><span class=\"o\">=</span><span class=\"s\">\":team:sfix_team_name\"</span><span class=\"p\">,</span>\n  <span class=\"n\">env</span><span class=\"o\">=</span><span class=\"s\">\"prod\"</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h3 id=\"creating-a-revision\">Creating a Revision</h3>\n<p>In SOS, a service can have multiple revisions, with each revision representing an immutable version of the service that can be deployed independently.</p>\n\n<p>The example below shows the minimal configurations to create a revision that deploys a Python artifact. The <code class=\"language-plaintext highlighter-rouge\">Artifact</code> is a self-contained Python artifact that has been deployed to Artifactory, which we use as our sink of artifacts.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">revision</span> <span class=\"o\">=</span> <span class=\"n\">Revision</span><span class=\"p\">(</span>\n  <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service-5\"</span><span class=\"p\">,</span>\n  <span class=\"n\">artifact</span><span class=\"o\">=</span><span class=\"n\">Artifact</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service\"</span><span class=\"p\">,</span> <span class=\"n\">version</span><span class=\"o\">=</span><span class=\"s\">\"v0.0.16\"</span><span class=\"p\">),</span>\n  <span class=\"n\">python_config</span><span class=\"o\">=</span><span class=\"n\">PythonConfig</span><span class=\"p\">(</span>\n          <span class=\"n\">python_version</span><span class=\"o\">=</span><span class=\"s\">\"3.9\"</span><span class=\"p\">,</span>\n          <span class=\"n\">gunicorn_config</span><span class=\"o\">=</span><span class=\"n\">GunicornConfig</span><span class=\"p\">(</span><span class=\"n\">worker_class</span><span class=\"o\">=</span><span class=\"s\">\"gevent\"</span><span class=\"p\">),</span>\n          <span class=\"n\">app_module</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service.server:app\"</span><span class=\"p\">,</span>\n  <span class=\"p\">),</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">service</span><span class=\"p\">.</span><span class=\"n\">add_revision</span><span class=\"p\">(</span><span class=\"n\">revision</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<p>Additional settings can be configured for more complex deployments, such as port number, environment variables, memory and CPU allocation, health checks, and thread and worker counts. Here\u2019s an example of how to set these configurations:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">revision</span> <span class=\"o\">=</span> <span class=\"n\">Revision</span><span class=\"p\">(</span>\n  <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service-5\"</span><span class=\"p\">,</span>\n  <span class=\"n\">artifact</span><span class=\"o\">=</span><span class=\"n\">Artifact</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service\"</span><span class=\"p\">,</span> <span class=\"n\">version</span><span class=\"o\">=</span><span class=\"s\">\"v0.0.16\"</span><span class=\"p\">),</span>\n  <span class=\"n\">container</span><span class=\"o\">=</span><span class=\"n\">Container</span><span class=\"p\">(</span>\n          <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">5001</span><span class=\"p\">,</span>\n          <span class=\"n\">env_vars</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">EnvVar</span><span class=\"p\">(</span><span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s\">\"env_var\"</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"s\">\"env_value\"</span><span class=\"p\">)],</span>\n          <span class=\"n\">resources</span><span class=\"o\">=</span><span class=\"n\">Resources</span><span class=\"p\">(</span><span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s\">\"2G\"</span><span class=\"p\">,</span> <span class=\"n\">cpu</span><span class=\"o\">=</span><span class=\"s\">\"1000m\"</span><span class=\"p\">),</span>\n  <span class=\"p\">),</span>\n  <span class=\"n\">health_check</span><span class=\"o\">=</span><span class=\"n\">HealthCheck</span><span class=\"p\">(</span><span class=\"n\">liveness_probe</span><span class=\"o\">=</span><span class=\"s\">\"/\"</span><span class=\"p\">,</span> <span class=\"n\">readiness_probe</span><span class=\"o\">=</span><span class=\"s\">\"/\"</span><span class=\"p\">),</span>\n  <span class=\"n\">sf_env</span><span class=\"o\">=</span><span class=\"s\">\"prod\"</span><span class=\"p\">,</span>\n  <span class=\"n\">python_config</span><span class=\"o\">=</span><span class=\"n\">PythonConfig</span><span class=\"p\">(</span>\n          <span class=\"n\">python_version</span><span class=\"o\">=</span><span class=\"s\">\"3.9\"</span><span class=\"p\">,</span>\n          <span class=\"n\">gunicorn_config</span><span class=\"o\">=</span><span class=\"n\">GunicornConfig</span><span class=\"p\">(</span><span class=\"n\">worker_count</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">thread_count</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">worker_class</span><span class=\"o\">=</span><span class=\"s\">\"gevent\"</span><span class=\"p\">),</span>\n          <span class=\"n\">app_module</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service.server:app\"</span><span class=\"p\">,</span>\n  <span class=\"p\">),</span>\n  <span class=\"n\">autoscaling</span><span class=\"o\">=</span><span class=\"n\">Autoscaling</span><span class=\"p\">(</span><span class=\"n\">min_scale</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">),</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">service</span><span class=\"p\">.</span><span class=\"n\">add_revision</span><span class=\"p\">(</span><span class=\"n\">revision</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<h3 id=\"set-traffic-for-new-revision\">Set Traffic for New Revision</h3>\n<p>We recognized the importance of providing robust support for traffic segmentation, as users should have confidence that deploying new code wouldn\u2019t negatively impact their entire system. By giving them complete control over the flow of traffic, they could confidently iterate on changes without fear of breaking everything. This was crucial to enabling faster development cycles and smoother rollbacks, as well as improving our understanding of service behavior through fine-grained traffic analysis.</p>\n\n<p>The following example shows how the newly created revision can be set to receive 100% traffic. However, to roll out changes more safely, traffic can be segmented between multiple revisions by adding more Route objects to the TrafficRoute.routes configuration. SOS\u2019s observability tooling can be used to monitor the traffic and ensure a smooth transition.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">service</span><span class=\"p\">.</span><span class=\"n\">add_traffic</span><span class=\"p\">(</span>\n    <span class=\"n\">Traffic</span><span class=\"p\">(</span>\n        <span class=\"n\">traffic_route</span><span class=\"o\">=</span><span class=\"n\">TrafficRoute</span><span class=\"p\">(</span>\n            <span class=\"n\">routes</span><span class=\"o\">=</span><span class=\"p\">[</span>\n                <span class=\"n\">Route</span><span class=\"p\">(</span>\n                    <span class=\"n\">percent</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n                    <span class=\"n\">revision_id</span><span class=\"o\">=</span><span class=\"s\">\"sfix_service-5\"</span><span class=\"p\">,</span>\n                    <span class=\"n\">dns_record_suffix</span><span class=\"o\">=</span><span class=\"s\">\"active\"</span><span class=\"p\">,</span>\n                <span class=\"p\">),</span>\n            <span class=\"p\">]</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h1 id=\"the-present-and-the-future\">The Present and the Future</h1>\n<p>Most data platform services have already been migrated to SOS.  It hosts many critical services, including the components of our batch processor, <a href=\"https://github.com/stitchfix/flotilla-os\">job execution engine</a>, our metastore, configuration management service, Kafka connect cluster, ownership discovery service, and more! While there are some data science services deployed on SOS, most of the currently-deployed services are focused on powering our data platform.</p>\n\n<p>SOS aims to streamline the service lifecycle, yet its opinionated approach might not suit all deployments. Certain open-source tools align better with standard industry patterns. For instance, we deploy Trino directly onto Kubernetes and use a Helm Chart for Airflow. While SOS isn\u2019t a one-size-fits-all solution, it remains highly effective for the majority of our existing and future deployments.</p>\n\n<p>We are regularly improving the user experience of SOS by migrating existing services and building new ones on top of it. Our goal is to reduce the complexity of service deployment and continue to empower our data scientists and engineers at Stitch Fix to be more efficient and effective.</p>"
    },
    "published": "Tue, 19 Sep 2023 15:00:00 +0000",
    "published_parsed": [
      2023,
      9,
      19,
      15,
      0,
      0,
      1,
      262,
      0
    ],
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://multithreaded.stitchfix.com/blog/2023/09/19/towards-service-deployment-agility/"
      }
    ],
    "link": "https://multithreaded.stitchfix.com/blog/2023/09/19/towards-service-deployment-agility/",
    "id": "https://multithreaded.stitchfix.com/blog/2023/09/19/towards-service-deployment-agility/",
    "guidislink": false
  }
}