{
  "company": "Slack",
  "title": "Slack",
  "xmlUrl": "https://slack.engineering/feed",
  "htmlUrl": "https://slack.engineering/",
  "content": "\n\n\n\n\n\n\n\n\n\nThe Scary Thing About Automating Deploys - Slack Engineering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to content\n\n\n\n\n \n\n\n\n\nSearch for:\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n\nScared Robot \n\nThe Scary Thing About Automating Deploys \n\n\n\n\n\n\n\nSean McIlroy\nSr Software Engineer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 14 minutes \u2022 Written yesterday\n\n\n\nMost of Slack runs on a monolithic service simply called \u201cThe Webapp\u201d. It\u2019s big \u2013 hundreds of developers create hundreds of changes every week.\nDeploying at this scale is a unique challenge. When people talk about continuous deployment, they\u2019re often thinking about deploying to systems as soon as changes are ready. They talk about microservices and 2-pizza teams (~8 people). But what does continuous deployment mean when you\u2019re looking at 150 changes on a normal day? That\u2019s a lot of pizzas\u2026\nChanges per day.\n\u00a0\nContinuous deployments are preferable to large, one-off deployments.\n\nWe want our customers to see the work of our developers as fast as possible so that we can iterate quickly. This allows us to respond quickly to customer feedback, whether that feedback is a feature request or bug reports.\nWe don\u2019t want to release a ton of changes at once. There\u2019s a higher likelihood of errors and those errors are more difficult to debug within a sea of changes.\n\nSo we need to move fast \u2013 and we do move fast. We deploy from our Webapp repository 30-40 times a day to our production fleet, with a median deploy size of 3 PRs. We manage a reasonable PR-to-deploy ratio despite the scale of our system\u2019s inputs.\n\n\u00a0\nWe manage these deployment speeds and sizes using our ReleaseBot. It runs 24/7, continually deploying new builds. But it wasn\u2019t always like this. We used to schedule Deploy Commanders (DCs), recruiting them from our Webapp developers. DCs would work a 2 hour shift where they\u2019d walk Webapp through its deployment steps, watching dashboards and executing manual tests along the way.\nThe Release Engineering team managed the deployment tooling, dashboards, and the DC schedule. The strongest, most frequent, feedback Release Engineering heard from DCs was that they weren\u2019t confident making decisions. It\u2019s difficult to monitor the deployment of a system this large. DCs were on a rotation with hundreds of other developers. How do you get comfortable with a system that you may only interact with every few months? What\u2019s normal? What do you do if something goes wrong? We had training and documentation, but it\u2019s impossible to cover every edge case.\nSo Release Engineering started thinking about how we could give DCs better signals. Fully automating deployments wasn\u2019t on the radar at this point. We just wanted to give DCs higher-level, clearer \u201cgo/no-go\u201d signals.\nWe worked on the ReleaseBot for a quarter and let it run alongside DCs for a quarter before realizing that ReleaseBot could be trusted to handle deployments by itself. It caught issues faster and more consistently than humans, so why not put it in the driver\u2019s seat?\nThe heart of ReleaseBot is its anomaly detection and monitoring. This is both the scariest and most important piece in any automated deployment system. Bots move faster than humans, meaning you\u2019re one bug and a very short period of time away from bringing down production.\nThe risks that come with automation are worth it for 2 reasons:\n\nIt\u2019s safer if you can get the monitoring right. Computers are both faster and more vigilant than humans.\nHuman time is our most valuable, constrained resource. How many hours do your company\u2019s engineers spend staring at dashboards?\n\n\nMonitoring never feels \u201cdone\u201d\nAny engineer that\u2019s been on-call will know this cycle:\n\nYou monitor everything with tight thresholds.\nThese tight thresholds, combined with a noisy service, lead to frequent pages.\nFrustrated and tired, you delete a few alerts and increase some thresholds\nYou finally get some sleep.\nAn incident occurs because that noisy service actually broke something but you didn\u2019t get paged.\nSomeone in an incident review asks why you weren\u2019t monitoring something.\nGo to step 1.\n\n\u00a0\nThis cycle stops a lot of teams from implementing automated deployments. I\u2019ve been in meetings like this multiple times throughout my career:\n\nPerson 1: \u201cWhy don\u2019t we just automate deployments?\u201d\nEveryone: *Nods*\nPerson 2: \u201cWhat if something breaks?\u201d\nEveryone: *Looks sad*\n\n\u00a0\nThe conversation doesn\u2019t make it past this point. Everyone is convinced it won\u2019t work because it feels like we don\u2019t have a solid hold on our alarms as-is \u2013 and that\u2019s with humans in the loop!\nEven if you have solid alerting and a reasonable on-call burden, you probably find yourself making small tweaks to alerts every few months. Complex systems experience a low hum of background errors and everything from performance characteristics, to dependencies, to the systems themselves change over time. Defining a particular number as \u201cbad\u201d for a complex system is open to subjective interpretation. It\u2019s a judgment call. Is 100 errors bad? What about a 200 millisecond average latency?\u00a0 Is one bad data point enough to page someone or should we wait a few minutes? Will your answers be the same in a month?\nGiven these constraints, writing a program we trust to handle deployments can seem insurmountable but, in some ways, it\u2019s easier than monitoring in general.\nHow deployments are different\nThe number of errors a system experiences in a steady-state isn\u2019t necessarily relevant to a deployment. If both version 1 and version 2 of an application emit 100 errors per second, then version 2 didn\u2019t introduce any new, breaking changes. By comparing the state of version 1 and version 2 and determining that the state of the system did not change, we can be confident that version 2 is a \u201cgood\u201d deployment.\nYou are mostly concerned with anomalies in the system when deploying. This necessitates a different approach.\nThis is intuitive if you think about how you watch a dashboard during a deployment. Imagine you just deployed some new code. You\u2019re looking at a dashboard. Which of these two graphs catches your attention?\n\n\u00a0\nClearly, the graph with a spike is concerning. We don\u2019t even know what this metric represents. Maybe it\u2019s a good spike! Either way, you know to look for those spikes. They\u2019re an indication something is tangibly different. And you\u2019re good at it. You can just scan the dashboard, ignoring specific numbers, looking for anomalies. It\u2019s easier and faster than watching for thresholds on every individual graph.\nSo how do we teach a computer to do this?\n\n\u00a0\nLuckily for us, defining \u201canomalous\u201d is mathematically simple. If a normal alert threshold is a judgment call involving tradeoffs between under and over alerting, a deployment threshold is a statistical question. We don\u2019t need to define \u201cbad\u201d in absolute terms. If we can see that the new version of the code has an anomalous error rate, we can assume that\u2019s bad \u2013 even if we don\u2019t know anything else about the system.\nIn short, you probably have all the metrics you need to start automating your deployments today. You just need to look at them a little differently.\nOur focus on \u201canomalous\u201d is, of course, a little overfit. Monitoring hard thresholds during a deployment is reasonable. That information is available, and a simple threshold provides us the signal that we\u2019re looking for most of the time, so why wouldn\u2019t we use it? However, you can get signals on-par with a human scanning a dashboard if you can implement anomaly detection.\nThe nitty-gritty\nLet\u2019s get into the details of anomaly detection. We have 2 ways of detecting anomalous behavior: z scores and dynamic thresholds.\nYour new best friend, the z score\nThe simplest mathematical way to find an anomaly is a z score. A z score represents the number of standard deviations from the mean for a particular data point (if that all sounds too math-y, I promise it gets better). The larger the number, the larger the outlier.\n\n\u00a0\nBasically, we\u2019re mathematically detecting a spike in a graph.\nThis can be a little intimidating if you\u2019re not familiar with statistics or z scores, but that\u2019s why we\u2019re here! Read on to find out how we do it, how you might implement it, and a few lessons we learned along the way.\nFirst, what is a z score? The actual equation for determining the z score for a particular data point is ((data point \u2013 mean) / standard deviation).\nUsing the above equation, we can calculate the z scores for every data point in a particular time interval.\nThankfully, calculating a z score is computationally simple. ReleaseBot is a Python application. Here\u2019s our implementation of z scores in Python, using scipy\u2019s stats library:\nfrom scipy import stats\n\ndef calculate_zscores(self) -> list[float]:\n\t# Grab our data points\n\tvalues = ChartHelper.all_values_in_automation_metrics(\n\t\tself.automation_metrics\n\t)\n\t# Calculate zscores\n\treturn list(stats.zscore(values))\nYou can do the same thing in Prometheus, Graphite, and in most other monitoring tools. These tools usually have built-in functions for calculating the mean and the standard deviation of datapoints. Here\u2019s a z score calculation for the last 5 minutes of data points in PromQL:\nabs(\n\tavg_over_time(metric[5m])\n\t- \n\tavg_over_time(metric[3h])\n)\n/ stddev_over_time(metric[3h])\nNow that ReleaseBot has the z scores, we check for z score threshold breaches and send a signal to our automation. ReleaseBot will automatically stop deployments and notify a Slack channel.\nAlmost all of our z score thresholds are 3 and/or -3 (-3 detects a drop in the graph). A z score of 3 generally represents a datapoint above the 99th percentile. I say \u201cgenerally\u201d because this really depends on the shape of your data. A z score of 3 can easily be the 99.7th percentile for a dataset.\nSo a z score of 3 is a large outlier, but it doesn\u2019t need to be a large difference in absolute terms. Here\u2019s an example in Python:\n>>> from scipy import stats\n# List representing a metric that alternates between \n# 1 and 3 for 3 hours (180 minutes)\n>>> x = [1 if i % 2 == 0 else 3 for i in range(180)]\n# Our most recent datapoint jumps to 5.5\n>>> x.append(5.5)\n# Calculate our zscores and grab the score for the 5.5 datapoint\n>>> score = stats.zscore(x)[-1]\n>>> score\n3.377882555133357\nThe same situation, in graph form:\n\n\u00a0\nSo if we have a graph that\u2019s been hanging out between 1 and 3 for 3 hours, a jump to 5.5 would have a z score of 3.37. This is a threshold breach. Our metric only increased by 2.5 in absolute numerical terms, but that jump was a huge statistical outlier. It wasn\u2019t a big jump, but it was definitely an unusual jump.\nThis is exactly the type of pattern that\u2019s obvious to a human scanning a dashboard, but could be missed by a static threshold because the actual change in value is so low.\nIt\u2019s really that simple. You can use built-in functions in the tool of your choice to calculate the z score and now you can detect anomalies instead of wrestling with hard-coded thresholds.\nSome extra tips:\n\nWe\u2019ve found a z score threshold of 3 is a good starting point. We use 3 for the majority of our metrics.\nYour standard deviation will be 0 if all of your numbers are the same. The z score equation requires dividing by the standard deviation. You can\u2019t divide by 0. Make sure your system handles this.\n\nIn our Python application, scipy.stats.zscore will return \u201cnan\u201d (not a number) in this scenario. So we just overwrite \u201cnan\u201d with 0. There was no variation in the metric \u2013 the line was flat \u2013 so we treat it like a z score of 0.\n\n\nYou might want to ignore either negative or positive z scores for some metrics. Do you care if errors or latency go down? Maybe! But give it some thought.\nYou may want to monitor things that don\u2019t traditionally indicate issues with the system. We, for example, monitor total log volume for anomalies. You probably wouldn\u2019t page an on-call because of increased informational log messages, but this could indicate some unexpected change in behavior during a deployment. (There\u2019s more on this later.)\nSnoozing z score metrics is a killer feature. Sometimes a change in a metric is an anomaly based on historical data, but you know it\u2019s going to be the new \u201cnormal\u201d. If that\u2019s the case, you\u2019ll want to snooze your z scores for whatever interval you use to calculate z scores. ReleaseBot looks at the last 3 hours of data, so the ReleaseBot UI has a \u201cSnooze for 3 Hours\u201d button next to each metric.\n\nHow Slack uses z scores\nWe consider z scores \u201chigh confidence\u201d signals. We know something has definitely changed and someone needs to take a look.\nAt Slack, we have a standard system of using white, blue, or red circle emojis within Slack messages to denote the urgency of a request, with white being the lowest urgency and red the highest.\n\n\u00a0\nA single z score threshold breach is a blue circle. Imagine you saw one graph spike on the dashboard. That\u2019s not good but you might do some investigation before raising any alarms.\nMultiple z score threshold breaches are a red circle. You know something bad just happened if you see multiple graphs jump at the same time. It\u2019s reasonable to take remediation actions before digging into a root cause.\nWe monitor the typical metrics you\u2019d expect (errors, 500\u2019s, latency, etc \u2013 see Google\u2019s The Four Golden Signals), but here are some potentially interesting ones:\n\n\n\nMetric\nHigh z score\nLow z score\nNotes\n\n\nPHPErrors\n1.5\n\u2013\nWe choose to be especially sensitive to error logs.\n\n\nStatusSlackCom\n3\n-3\nThis is the number of requests to https://status.slack.com \u2013 the site users access to check if Slack is having problems. A lot of people suddenly curious about the status of Slack is a good indication that something is broken.\n\n\nWebsocketEventsVolume\n\u2013\n-3\nA high number of client connections doesn\u2019t necessarily mean that we\u2019re overloaded. But an unexpected drop in client connections could mean we\u2019ve released something especially bad on the backend.\n\n\nLogVolume\n3\n\u2013\nSeparate from error logs. Are we creating many more logs than usual? Why? Can our logging system handle the volume?\n\n\nEnvoyPanicRouting\n3\n\u2013\nEnvoy routes traffic to the Webapp hosts. It starts \u201cpanic routing\u201d when it can\u2019t locate enough hosts. Are hosts stopping but not restarting during the deployment? Are we deploying too quickly \u2013 taking down too many hosts at once?\n\n\n\n\u00a0\nBeyond the z score, dynamic thresholds\nWe still monitor static thresholds but we consider them \u201clow confidence\u201d alarms (they\u2019re a white circle). We set static thresholds for some key metrics but Releasebot also calculates its own dynamic threshold, using the higher of the two.\nImagine the database team deploys some component every Wednesday at 3pm. When this deployment happens, database errors temporarily spike above your alert threshold, but your application handles it gracefully. Since the application handles it gracefully, users don\u2019t see the errors and thus we obviously don\u2019t need to stop deployments in this situation.\nSo how can we monitor a metric using a static threshold while filtering out otherwise \u201cnormal\u201d behavior? We use an average derived from historical data.\n\u201cHistorical data\u201d deserves some explanation here. Slack is used by enterprises. Our product is mostly used during the typical workday, 9am to 5pm, Monday through Friday. So we don\u2019t just grab a larger, continuous window of data when we\u2019re thinking about historical relevance. We sample data from similar time periods.\nLet\u2019s say we\u2019re running this calculation at 6pm on Wednesday. We\u2019ll pull data from:\n\n12pm-6pm Wednesday (today).\n12pm-6pm Tuesday.\n12pm-6pm last Wednesday.\n\nWe pool all of these windows together and calculate a simple average. Here\u2019s how you could achieve the same result with PromQL:\n(\n\tsum(metric[6h])\n\t+ sum(metric[6h] offset 1d)\n\t+ sum(metric[6h] offset 1w)\n ) / 3\nAgain, this is a fairly simple algorithm:\n\nGather historical data and calculate the average.\nTake the larger of \u201cthe average historical data\u201d and \u201chard-coded threshold\u201d.\nStop deployments and alarm if the last 5 data points breach the chosen threshold.\n\nIn simple terms: We watch thresholds but we\u2019re willing to ignore a breach if historical data indicates it\u2019s normal.\nDynamic thresholds are a nice-to-have, but not strictly required, feature of ReleaseBot. Static thresholds may be a bit more noisy, but don\u2019t carry any additional risks to your production systems.\nEmbrace the fear\nFear of breaking production holds many teams back from automating their deployments, but understanding how deployment monitoring differs from normal monitoring opens the door to simple, effective tools.\nIt\u2019ll still be scary. We took a careful, iterative approach to ease our fears. We added z score monitoring to our ReleaseBot platform and compared its results to the humans running deployments and watching graphs. The results of ReleaseBot were far better than we expected; to the point where it seemed irresponsible to not put ReleaseBot in the driver\u2019s seat for deployments.\nSo throw some z scores on a dashboard and see how they work. You might just accidentally help your coworkers avoid staring at dashboards all day.\n\n\n\t\t\t\tWant to come help us build Slack (and/or fun robots?!) \t\t\t\tApply now\n\n\nautomationbackendci-cddeploymentdevopsengineeringinfrastructureobservability\n\n\n\n\n \n\n\nSearch for:\n\n\n\n\n\n\n\n\nSearch\n\n \n\t\t\tPost Types\t\t\n\n\n\n\t\t\t\t\t\tPost\u00a0(160)\t\t\t\t\t\n\n\n\n\t\t\tCategories\t\t\n\n\n\n\t\t\t\t\t\tUncategorized\u00a0(139)\t\t\t\t\t\n\n\n\n\t\t\tTags\t\t\n\n\n\n\t\t\t\t\t\tinfrastructure\u00a0(23)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\tjavascript\u00a0(20)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\tandroid\u00a0(13)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\tperformance\u00a0(13)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\tsoftware-development\u00a0(13)\t\t\t\t\t\n\n\n\n\t\t\tYear\t\t\n\n\n\n\t\t\t\t\t\t2024\u00a0(1)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\t2023\u00a0(16)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\t2022\u00a0(21)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\t2021\u00a0(24)\t\t\t\t\t\n\n\n\n\t\t\t\t\t\t2020\u00a0(26)\t\t\t\t\t\n\n\n\nMost Recent\n\n\n\n\nOur Journey Migrating to AWS IMDSv2 \n\nOur Journey Migrating to AWS IMDSv2@Archie Gunasekara \n\n\n\n\n\n \n\nBuilding Custom Animations in the Workflow Builder@Christina Mudarth \n\n\n\n\n\n \n\nManaging Slack Connect@Yuriy Loginov \n\n\n\n\n\n\n@SlackEng how can I stay up-to-date on what's happening over there?\n\n\nFollow us on Twitter\n\n\n \n\n\n\n\n\n\nPost navigation\nPrevious post Previous post: Our Journey Migrating to AWS IMDSv2\n\n\n\n\n\n\nRecommended Reading\n\n\n\n\n\n \n\nSlack\u2019s Migration to a Cellular Architecture@Cooper Bethea \n\n\n\n\n\n \n\nExecuting Cron Scripts Reliably At Scale@Claire Adams \n\n\n\n\n\n \n\nBuilding Custom Animations in the Workflow Builder@Christina Mudarth \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScroll to top\n\n\n \n \n\n\n\n\n\n\n\n",
  "latestPost": {
    "title": "The Scary Thing About Automating Deploys",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://slack.engineering/feed/",
      "value": "The Scary Thing About Automating Deploys"
    },
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://slack.engineering/the-scary-thing-about-automating-deploys/"
      }
    ],
    "link": "https://slack.engineering/the-scary-thing-about-automating-deploys/",
    "authors": [
      {
        "name": "Sean McIlroy"
      }
    ],
    "author": "Sean McIlroy",
    "author_detail": {
      "name": "Sean McIlroy"
    },
    "published": "Thu, 18 Jan 2024 18:29:03 +0000",
    "published_parsed": [
      2024,
      1,
      18,
      18,
      29,
      3,
      3,
      18,
      0
    ],
    "tags": [
      {
        "term": "Uncategorized",
        "scheme": null,
        "label": null
      },
      {
        "term": "automation",
        "scheme": null,
        "label": null
      },
      {
        "term": "backend",
        "scheme": null,
        "label": null
      },
      {
        "term": "ci-cd",
        "scheme": null,
        "label": null
      },
      {
        "term": "deployment",
        "scheme": null,
        "label": null
      },
      {
        "term": "devops",
        "scheme": null,
        "label": null
      },
      {
        "term": "engineering",
        "scheme": null,
        "label": null
      },
      {
        "term": "infrastructure",
        "scheme": null,
        "label": null
      },
      {
        "term": "observability",
        "scheme": null,
        "label": null
      }
    ],
    "id": "https://slack.engineering/?p=16533",
    "guidislink": false,
    "summary": "<p>Most of Slack runs on a monolithic service simply called \u201cThe Webapp\u201d. It\u2019s big &#8211; hundreds of developers create hundreds of changes every week. Deploying at this scale is a unique challenge. When people talk about continuous deployment, they\u2019re often thinking about deploying to systems as soon as changes are ready. They talk about microservices [&#8230;]</p>\n<p>The post <a href=\"https://slack.engineering/the-scary-thing-about-automating-deploys/\" rel=\"nofollow\">The Scary Thing About Automating Deploys</a> appeared first on <a href=\"https://slack.engineering\" rel=\"nofollow\">Slack Engineering</a>.</p>",
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://slack.engineering/feed/",
      "value": "<p>Most of Slack runs on a monolithic service simply called \u201cThe Webapp\u201d. It\u2019s big &#8211; hundreds of developers create hundreds of changes every week. Deploying at this scale is a unique challenge. When people talk about continuous deployment, they\u2019re often thinking about deploying to systems as soon as changes are ready. They talk about microservices [&#8230;]</p>\n<p>The post <a href=\"https://slack.engineering/the-scary-thing-about-automating-deploys/\" rel=\"nofollow\">The Scary Thing About Automating Deploys</a> appeared first on <a href=\"https://slack.engineering\" rel=\"nofollow\">Slack Engineering</a>.</p>"
    },
    "content": [
      {
        "type": "text/html",
        "language": null,
        "base": "https://slack.engineering/feed/",
        "value": "<p>Most of Slack runs on a monolithic service simply called \u201cThe Webapp\u201d. It\u2019s <i>big</i> &#8211; hundreds of developers create hundreds of changes every week.</p>\n<p>Deploying at this scale is a unique challenge. When people talk about continuous deployment, they\u2019re often thinking about deploying to systems as soon as changes are ready. They talk about microservices and 2-pizza teams (~8 people). But what does continuous deployment mean when you\u2019re looking at 150 changes on a normal day? That\u2019s a lot of pizzas\u2026</p>\n<figure class=\"wp-caption aligncenter\" id=\"attachment_16534\"><img alt=\"Graph showing changes opened, merged, and deployed per day, from October 16th to October 20th. Changes deployed is between 150 and 190.\" class=\"wp-image-16534 size-medium\" height=\"488\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.05.49\u202fAM.png?w=506\" width=\"506\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-16534\">Changes per day.</figcaption></figure>\n<p>&nbsp;</p>\n<p>Continuous deployments are preferable to large, one-off deployments.</p>\n<ol>\n<li>We want our customers to see the work of our developers as fast as possible so that we can iterate quickly. This allows us to respond quickly to customer feedback, whether that feedback is a feature request or bug reports.</li>\n<li>We don\u2019t want to release a ton of changes at once. There\u2019s a higher likelihood of errors and those errors are more difficult to debug within a sea of changes.</li>\n</ol>\n<p>So we need to move fast &#8211; and we <i>do</i> move fast. We deploy from our Webapp repository 30-40 times a day to our production fleet, with a median deploy size of 3 PRs. We manage a reasonable PR-to-deploy ratio despite the scale of our system\u2019s inputs.</p>\n<p><img alt=\"A graph showing deploys per day, from October 16th to October 20th. The number bounces between 32 and 37.\" class=\"aligncenter wp-image-16535 size-medium\" height=\"509\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.06.49\u202fAM.png?w=514\" width=\"514\" /></p>\n<p>&nbsp;</p>\n<p>We manage these deployment speeds and sizes using our ReleaseBot. It runs 24/7, continually deploying new builds. But it wasn\u2019t always like this. We used to schedule Deploy Commanders (DCs), recruiting them from our Webapp developers. DCs would work a 2 hour shift where they\u2019d walk Webapp through its deployment steps, watching dashboards and executing manual tests along the way.</p>\n<p>The Release Engineering team managed the deployment tooling, dashboards, and the DC schedule. The strongest, most frequent, feedback Release Engineering heard from DCs was that they weren\u2019t confident making decisions. It\u2019s difficult to monitor the deployment of a system this large. DCs were on a rotation with hundreds of other developers. How do you get comfortable with a system that you may only interact with every few months? What\u2019s normal? What do you do if something goes wrong? We had training and documentation, but it\u2019s impossible to cover every edge case.</p>\n<p>So Release Engineering started thinking about how we could give DCs better signals. Fully automating deployments wasn\u2019t on the radar at this point. We just wanted to give DCs higher-level, clearer \u201cgo/no-go\u201d signals.</p>\n<p>We worked on the ReleaseBot for a quarter and let it run alongside DCs for a quarter before realizing that ReleaseBot could be trusted to handle deployments by itself. It caught issues faster and more consistently than humans, so why not put it in the driver&#8217;s seat?</p>\n<p>The heart of ReleaseBot is its anomaly detection and monitoring. This is both the scariest and most important piece in any automated deployment system. Bots move faster than humans, meaning you\u2019re one bug and a very short period of time away from bringing down production.</p>\n<p>The risks that come with automation are worth it for 2 reasons:</p>\n<ol>\n<li>It\u2019s safer if you can get the monitoring right. Computers are both faster and more vigilant than humans.</li>\n<li>Human time is our most valuable, constrained resource. How many hours do your company\u2019s engineers spend staring at dashboards?</li>\n</ol>\n<p><img alt=\"Screenshot of Slack Message from Release Bot saying &quot;ReleaseBot started for webapp&quot;\" class=\"aligncenter wp-image-16536 size-medium\" height=\"75\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.09.41\u202fAM.png?w=579\" width=\"579\" /></p>\n<h2>Monitoring never feels \u201cdone\u201d</h2>\n<p>Any engineer that\u2019s been on-call will know this cycle:</p>\n<ol>\n<li>You monitor everything with tight thresholds.</li>\n<li>These tight thresholds, combined with a noisy service, lead to frequent pages.</li>\n<li>Frustrated and tired, you delete a few alerts and increase some thresholds</li>\n<li>You finally get some sleep.</li>\n<li>An incident occurs because that noisy service actually broke something but you didn\u2019t get paged.</li>\n<li>Someone in an incident review asks why you weren\u2019t monitoring something.</li>\n<li>Go to step 1.</li>\n</ol>\n<p>&nbsp;</p>\n<p>This cycle stops a lot of teams from implementing automated deployments. I\u2019ve been in meetings like this multiple times throughout my career:</p>\n<ul>\n<li>Person 1: \u201cWhy don\u2019t we just automate deployments?\u201d</li>\n<li>Everyone: *Nods*</li>\n<li>Person 2: \u201cWhat if something breaks?\u201d</li>\n<li>Everyone: *Looks sad*</li>\n</ul>\n<p>&nbsp;</p>\n<p>The conversation doesn\u2019t make it past this point. Everyone is <i>convinced</i> it won\u2019t work because it feels like we don\u2019t have a solid hold on our alarms as-is &#8211; and that\u2019s with humans in the loop!</p>\n<p>Even if you have solid alerting and a reasonable on-call burden, you probably find yourself making small tweaks to alerts every few months. Complex systems experience a low hum of background errors and everything from performance characteristics, to dependencies, to the systems themselves change over time. Defining a particular number as \u201cbad\u201d for a complex system is open to subjective interpretation. It\u2019s a judgment call. Is 100 errors bad? What about a 200 millisecond average latency?\u00a0 Is one bad data point enough to page someone or should we wait a few minutes? Will your answers be the same in a month?</p>\n<p>Given these constraints, writing a program we trust to handle deployments can seem insurmountable but, in some ways, it\u2019s easier than monitoring in general.</p>\n<h2>How deployments are different</h2>\n<p>The number of errors a system experiences in a steady-state isn\u2019t necessarily relevant to a deployment. If both version 1 and version 2 of an application emit 100 errors per second, then version 2 didn\u2019t introduce any new, breaking changes. By comparing the state of version 1 and version 2 and determining that the state of the system did not change, we can be confident that version 2 is a \u201cgood\u201d deployment.</p>\n<p>You are mostly concerned with <i>anomalies</i> in the system when deploying. This necessitates a different approach.</p>\n<p>This is intuitive if you think about how you watch a dashboard during a deployment. Imagine you just deployed some new code. You\u2019re looking at a dashboard. Which of these two graphs catches your attention?</p>\n<p><img alt=\"Two graphs with a line on each denoting a deployment. The left graph is at 1, then spikes to 10 and 15 immediately after the deployment. The right graph is a flat line at 100 before and after the deployment.\" class=\"aligncenter wp-image-16537\" height=\"178\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.11.02\u202fAM.png?w=640\" width=\"594\" /></p>\n<p>&nbsp;</p>\n<p>Clearly, the graph with a spike is concerning. We don\u2019t even know what this metric represents. Maybe it\u2019s a good spike! Either way, you know to look for those spikes. They\u2019re an indication something is tangibly different. And you\u2019re <i>good</i> at it. You can just scan the dashboard, ignoring specific numbers, looking for anomalies. It\u2019s easier and faster than watching for thresholds on every individual graph.</p>\n<p>So how do we teach a computer to do this?</p>\n<p><img alt=\"Picture of a robot emoji with a robot cat in a thought bubble. They are in front of a graph in the rough shape of a cat. The text reads &quot;It's easy for humans to spot anomalies in data. For example, this PHP Errors chart resembles my cat&quot;.\" class=\"aligncenter wp-image-16538 size-medium\" height=\"359\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.12.30\u202fAM.png?w=640\" width=\"639\" /></p>\n<p>&nbsp;</p>\n<p>Luckily for us, defining \u201canomalous\u201d is mathematically simple. If a normal alert threshold is a judgment call involving tradeoffs between under and over alerting, a deployment threshold is a statistical question. We don\u2019t need to define \u201cbad\u201d in absolute terms. If we can see that the new version of the code has an anomalous error rate, we can assume that\u2019s bad \u2013 even if we don\u2019t know anything else about the system.</p>\n<p>In short, you probably have all the metrics you need to start automating your deployments today. You just need to look at them a little differently.</p>\n<p>Our focus on \u201canomalous\u201d is, of course, a little overfit. Monitoring hard thresholds during a deployment is reasonable. That information is available, and a simple threshold provides us the signal that we\u2019re looking for most of the time, so why wouldn\u2019t we use it? However, you can get signals on-par with a human scanning a dashboard if you can implement anomaly detection.</p>\n<h2>The nitty-gritty</h2>\n<p>Let\u2019s get into the details of anomaly detection. We have 2 ways of detecting anomalous behavior: z scores and dynamic thresholds.</p>\n<h3>Your new best friend, the <i>z</i> score</h3>\n<p>The simplest mathematical way to find an anomaly is a <i>z</i> score. A <i>z</i> score represents the number of standard deviations from the mean for a particular data point (if that all sounds too math-y, I promise it gets better). The larger the number, the larger the outlier.</p>\n<p><img alt=\"A picture of a robot emoji with sunglasses on the cover of Kenny Loggins Danger Zone, in front of a graph show a normal distribution with standard deviations. The text reads &quot;A z-score tells us how far a value is from the mean, measured in terms of standard deviation. For example, a z-score of 2.5 or -2.5 means that the value is between 2 to 3 standard deviations from the mean.\" class=\"aligncenter wp-image-16539 size-medium\" height=\"360\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.13.32\u202fAM.png?w=640\" width=\"640\" /></p>\n<p>&nbsp;</p>\n<p>Basically, we\u2019re mathematically detecting a spike in a graph.</p>\n<p>This can be a little intimidating if you\u2019re not familiar with statistics or <i>z</i> scores, but that\u2019s why we\u2019re here! Read on to find out how we do it, how you might implement it, and a few lessons we learned along the way.</p>\n<p>First, what is a z score? The actual equation for determining the z score for a particular data point is ((data point &#8211; mean) / standard deviation).</p>\n<p>Using the above equation, we can calculate the <i>z</i> scores for every data point in a particular time interval.</p>\n<p>Thankfully, calculating a z score is computationally simple. ReleaseBot is a Python application. Here\u2019s our implementation of z scores in Python, using <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html#scipy-stats-zscore\">scipy\u2019s stats library</a>:</p>\n<pre><code class=\"language-python\">from scipy import stats\n\ndef calculate_zscores(self) -&gt; list[float]:\n\t# Grab our data points\n\tvalues = ChartHelper.all_values_in_automation_metrics(\n\t\tself.automation_metrics\n\t)\n\t# Calculate zscores\n\treturn list(stats.zscore(values))</code></pre>\n<p>You can do the same thing in Prometheus, Graphite, and in most other monitoring tools. These tools usually have built-in functions for calculating the mean and the standard deviation of datapoints. Here\u2019s a z score calculation for the last 5 minutes of data points in PromQL:</p>\n<pre><code class=\"language-python\">abs(\n\tavg_over_time(metric[5m])\n\t- \n\tavg_over_time(metric[3h])\n)\n/ stddev_over_time(metric[3h])</code></pre>\n<p>Now that ReleaseBot has the <i>z</i> scores, we check for z score threshold breaches and send a signal to our automation. ReleaseBot will automatically stop deployments and notify a Slack channel.</p>\n<p>Almost all of our z score thresholds are 3 and/or -3 (-3 detects a drop in the graph). A z score of 3 generally represents a datapoint above the 99th percentile. I say \u201cgenerally\u201d because this really depends on the shape of your data. A <i>z</i> score of 3 can easily be the 99.7th percentile for a dataset.</p>\n<p>So a <i>z</i> score of 3 is a large outlier, but it doesn\u2019t need to be a large difference in absolute terms. Here\u2019s an example in Python:</p>\n<pre><code class=\"language-python\">&gt;&gt;&gt; from scipy import stats\n# List representing a metric that alternates between \n# 1 and 3 for 3 hours (180 minutes)\n&gt;&gt;&gt; x = [1 if i % 2 == 0 else 3 for i in range(180)]\n# Our most recent datapoint jumps to 5.5\n&gt;&gt;&gt; x.append(5.5)\n# Calculate our zscores and grab the score for the 5.5 datapoint\n&gt;&gt;&gt; score = stats.zscore(x)[-1]\n&gt;&gt;&gt; score\n3.377882555133357</code></pre>\n<p>The same situation, in graph form:</p>\n<p><img alt=\"A graph that bounces between 1 and 3 continually, then jumps to 5.5 at the last datapoint. A red arrow points to 5.5 with &quot;z score = 3.37&quot;.\" class=\"aligncenter wp-image-16540 size-medium\" height=\"382\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.18.38\u202fAM.png?w=640\" width=\"640\" /></p>\n<p>&nbsp;</p>\n<p>So if we have a graph that\u2019s been hanging out between 1 and 3 for 3 hours, a jump to 5.5 would have a <i>z</i> score of 3.37. This is a threshold breach. Our metric only increased by 2.5 in absolute numerical terms, but that jump was a huge statistical outlier. It wasn\u2019t a big jump, but it was definitely an <i>unusual</i> jump.</p>\n<p>This is exactly the type of pattern that\u2019s obvious to a human scanning a dashboard, but could be missed by a static threshold because the actual change in value is so low.</p>\n<p>It\u2019s really that simple. You can use built-in functions in the tool of your choice to calculate the <i>z</i> score and now you can detect anomalies instead of wrestling with hard-coded thresholds.</p>\n<p>Some extra tips:</p>\n<ol>\n<li>We\u2019ve found a <i>z</i> score threshold of 3 is a good starting point. We use 3 for the majority of our metrics.</li>\n<li>Your standard deviation will be 0 if all of your numbers are the same. The<i> z</i> score equation requires dividing by the standard deviation. You can\u2019t divide by 0. Make sure your system handles this.\n<ol>\n<li>In our Python application, scipy.stats.zscore will return \u201cnan\u201d (not a number) in this scenario. So we just overwrite \u201cnan\u201d with 0. There was no variation in the metric &#8211; the line was flat &#8211; so we treat it like a <i>z</i> score of 0.</li>\n</ol>\n</li>\n<li>You might want to ignore either negative or positive <i>z</i> scores for some metrics. Do you care if errors or latency go <i>down</i>? Maybe! But give it some thought.</li>\n<li>You may want to monitor things that don\u2019t traditionally indicate issues with the system. We, for example, monitor total log volume for anomalies. You probably wouldn\u2019t page an on-call because of increased informational log messages, but this could indicate some unexpected change in behavior during a deployment. (There\u2019s more on this later.)</li>\n<li>Snoozing z score metrics is a killer feature. Sometimes a change in a metric is an anomaly based on historical data, but you know it\u2019s going to be the new \u201cnormal\u201d. If that\u2019s the case, you\u2019ll want to snooze your z scores for whatever interval you use to calculate z scores. ReleaseBot looks at the last 3 hours of data, so the ReleaseBot UI has a \u201cSnooze for 3 Hours\u201d button next to each metric.</li>\n</ol>\n<h3>How Slack uses <em>z</em> scores</h3>\n<p>We consider <i>z</i> scores \u201chigh confidence\u201d signals. We know something has definitely changed and someone needs to take a look.</p>\n<p>At Slack, we have a standard system of using white, blue, or red circle emojis within Slack messages to denote the urgency of a request, with white being the lowest urgency and red the highest.</p>\n<p><img alt=\"A screenshot of a Slack message from Release Bot. The message is a blue circle emoji with text, &quot;Webapp event #2528 opened for char Five Hundred Errors, in tier dogfood and az use1-az2&quot;.\" class=\"aligncenter wp-image-16542 size-medium\" height=\"220\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.22.08\u202fAM.png?w=640\" width=\"637\" /></p>\n<p>&nbsp;</p>\n<p>A single <i>z</i> score threshold breach is a blue circle. Imagine you saw one graph spike on the dashboard. That\u2019s not good but you might do some investigation before raising any alarms.</p>\n<p>Multiple <i>z</i> score threshold breaches are a red circle. You know something bad just happened if you see multiple graphs jump at the same time. It\u2019s reasonable to take remediation actions before digging into a root cause.</p>\n<p>We monitor the typical metrics you\u2019d expect (errors, 500\u2019s, latency, etc &#8211; see Google\u2019s <a href=\"https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals\">The Four Golden Signals</a>), but here are some potentially interesting ones:</p>\n<table style=\"height: 988px;\" width=\"552\">\n<tbody>\n<tr>\n<td>Metric</td>\n<td>High z score</td>\n<td>Low z score</td>\n<td>Notes</td>\n</tr>\n<tr>\n<td>PHPErrors</td>\n<td>1.5</td>\n<td>&#8211;</td>\n<td>We choose to be especially sensitive to error logs.</td>\n</tr>\n<tr>\n<td>StatusSlackCom</td>\n<td>3</td>\n<td>-3</td>\n<td>This is the number of requests to <a href=\"https://status.slack.com/\">https://status.slack.com</a> &#8211; the site users access to check if Slack is having problems. A lot of people suddenly curious about the status of Slack is a good indication that something is broken.</td>\n</tr>\n<tr>\n<td>WebsocketEventsVolume</td>\n<td>&#8211;</td>\n<td>-3</td>\n<td>A high number of client connections doesn\u2019t necessarily mean that we\u2019re overloaded. But an unexpected drop in client connections could mean we\u2019ve released something especially bad on the backend.</td>\n</tr>\n<tr>\n<td>LogVolume</td>\n<td>3</td>\n<td>&#8211;</td>\n<td>Separate from error logs. Are we creating many more logs than usual? Why? Can our logging system handle the volume?</td>\n</tr>\n<tr>\n<td>EnvoyPanicRouting</td>\n<td>3</td>\n<td>&#8211;</td>\n<td>Envoy routes traffic to the Webapp hosts. It starts \u201cpanic routing\u201d when it can\u2019t locate enough hosts. Are hosts stopping but not restarting during the deployment? Are we deploying too quickly &#8211; taking down too many hosts at once?</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h3>Beyond the <i>z</i> score, dynamic thresholds</h3>\n<p>We still monitor static thresholds but we consider them \u201clow confidence\u201d alarms (they\u2019re a white circle). We set static thresholds for some key metrics but Releasebot also calculates its own dynamic threshold, using the <b>higher</b> of the two.</p>\n<p>Imagine the database team deploys some component every Wednesday at 3pm. When this deployment happens, database errors temporarily spike above your alert threshold, but your application handles it gracefully. Since the application handles it gracefully, users don\u2019t see the errors and thus we obviously don\u2019t need to stop deployments in this situation.</p>\n<p>So how can we monitor a metric using a static threshold while filtering out otherwise \u201cnormal\u201d behavior? We use an average derived from historical data.</p>\n<p>\u201cHistorical data\u201d deserves some explanation here. Slack is used by enterprises. Our product is mostly used during the typical workday, 9am to 5pm, Monday through Friday. So we don\u2019t just grab a larger, continuous window of data when we\u2019re thinking about historical relevance. We sample data from similar time periods.</p>\n<p>Let\u2019s say we\u2019re running this calculation at 6pm on Wednesday. We\u2019ll pull data from:</p>\n<ul>\n<li>12pm-6pm Wednesday (today).</li>\n<li>12pm-6pm Tuesday.</li>\n<li>12pm-6pm last Wednesday.</li>\n</ul>\n<p>We pool all of these windows together and calculate a simple average. Here\u2019s how you could achieve the same result with PromQL:</p>\n<pre><code class=\"language-python\">(\n\tsum(metric[6h])\n\t+ sum(metric[6h] offset 1d)\n\t+ sum(metric[6h] offset 1w)\n ) / 3</code></pre>\n<p>Again, this is a fairly simple algorithm:</p>\n<ol>\n<li>Gather historical data and calculate the average.</li>\n<li>Take the larger of \u201cthe average historical data\u201d and \u201chard-coded threshold\u201d.</li>\n<li>Stop deployments and alarm if the last 5 data points breach the chosen threshold.</li>\n</ol>\n<p>In simple terms: We watch thresholds but we\u2019re willing to ignore a breach if historical data indicates it\u2019s normal.</p>\n<p>Dynamic thresholds are a nice-to-have, but not strictly required, feature of ReleaseBot. Static thresholds may be a bit more noisy, but don\u2019t carry any additional risks to your production systems.</p>\n<h2>Embrace the fear</h2>\n<p>Fear of breaking production holds many teams back from automating their deployments, but understanding how deployment monitoring differs from normal monitoring opens the door to simple, effective tools.</p>\n<p>It\u2019ll still be scary. We took a careful, iterative approach to ease our fears. We added <i>z</i> score monitoring to our ReleaseBot platform and compared its results to the humans running deployments and watching graphs. The results of ReleaseBot were far better than we expected; to the point where it seemed irresponsible to <i>not</i> put ReleaseBot in the driver\u2019s seat for deployments.</p>\n<p>So throw some <i>z</i> scores on a dashboard and see how they work. You might just accidentally help your coworkers avoid staring at dashboards all day.</p>\n<p><img alt=\"A screenshot of a message from ReleaseBot with the text &quot;Release Bot has called 'all clear' on that deploy!&quot;\" class=\"aligncenter wp-image-16544 size-medium\" height=\"83\" src=\"https://slack.engineering/wp-content/uploads/sites/7/2024/01/Screenshot-2024-01-18-at-9.24.21\u202fAM.png?w=640\" width=\"639\" /></p>\n\t\t\t<p class=\"hiring\">\n\t\t\t\tWant to come help us build Slack (and/or fun robots?!) \t\t\t\t<a class=\"\" href=\"https://slack.com/jobs/dept/engineering\" target=\"_blank\">Apply now</a>\n\t\t\t</p>\n\t\t\n<p>The post <a href=\"https://slack.engineering/the-scary-thing-about-automating-deploys/\" rel=\"nofollow\">The Scary Thing About Automating Deploys</a> appeared first on <a href=\"https://slack.engineering\" rel=\"nofollow\">Slack Engineering</a>.</p>"
      }
    ],
    "post-id": "16533"
  }
}