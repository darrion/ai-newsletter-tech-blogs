{
  "company": "Heroku",
  "title": "Heroku",
  "xmlUrl": "https://blog.heroku.com/engineering/feed",
  "htmlUrl": "https://blog.heroku.com/engineering",
  "content": "\n\n\n\n\n\nHow to Use pgvector for Similarity Search on Heroku Postgres | Heroku\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip Navigation\n\n\n\n\n      Show nav\n      \n\n\n\n\n\n\nHeroku\n\n\n\n\nProducts\n\nHeroku Platform\n\nHeroku DX\n\nHeroku Flow\nContinuous Delivery\nContinuous Integration\n\n\nHeroku OpEx\nHeroku Runtime\n\nHeroku Dynos\n\n\n\n\n\nHeroku Data Services\n\nHeroku Postgres\nHeroku Data for Redis\nApache Kafka on Heroku\n\n\n\nHeroku Enterprise\n\nHeroku Private Spaces\nHeroku Connect\nHeroku Shield\n\n\nHeroku Teams\n\nSalesforce\n\nSales Cloud\nService Cloud\nMarketing Cloud\n\n\n\n\n\nMarketplace\n\nAdd-ons\nButtons\nBuildpacks\nAbout\n\n\nPricing\nDocumentation\nSupport\n\nMore\n\n\nResources\n\nWhat is Heroku?\nHelp\nCustomers\nCareers\nEvents\nPodcasts\nCompliance Center\n\n\n\nHeroku is for\n\nDevelopers\nCTOs\nTeam Collaboration\nStartups\nEnterprises\nAgencies\nStudents\nSee More\n\n\n\nLanguages\n\nNode.js\nRuby\nJava\nPHP\nPython\nGo\nScala\nClojure\nSee More\n\n\n\nLatest News from the Heroku Blog\nHeroku Blog\n\nFind out what's new with Heroku on our blog.\nMore news\nView all blog posts\n\n\n\n\n\n\n\n\nSearch: \n\n\n\n\n\n\nLog in\nor\nSign up\n\n\n\n\n\n\n\nHeroku Blog\n\n\n\n\n            Latest\n \n\n\n\n              News\n \n\n\n\n              Engineering\n \n\n\n\n              Ecosystem\n \n\n\n\n              Life\n \n\n\n\n\n\n\n\n\n\n\nBlog\nEngineering\nHow to Use pgvector for Similarity Search on Heroku Postgres\n\n\n\n\n\n\n\n|||\n\nVideo Transcript\n\n\n\n\n\n\n\nClose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Use pgvector for Similarity Search on Heroku Postgres\n\n\n\n\nPosted by Valerie Woolard\n\nNovember 15, 2023\n\n\n\n\n\nIntroducing pgvector for Heroku Postgres\n\nOver the past few weeks, we worked on adding pgvector as an extension on Heroku Postgres. We're excited to release this feature, and based on the feedback on our public roadmap, many of you are too. We want to share a bit more about how you can use it and how it may be helpful to you. \nAll Standard-tier or higher databases running Postgres 15 now support the pgvector extension. You can get started by running CREATE EXTENSION vector; in a client session. Postgres 15 has been the default version on Heroku Postgres since March 2023.  If you're on an older version and want to use pgvector, upgrade to Postgres 15.\nThe extension adds the vector data type to Heroku Postgres along with additional functions to work with it. Vectors are important for working with large language models and other machine learning applications, as the embeddings generated by these models are often output in vector format. Working with vectors lets you implement things like similarity search across these embeddings. See our launch blog for more background into what pgvector is, its significance, and ideas for how to use this new data type.\n\nAn Example: Word Vector Similarity Search\n\nTo show a simple example of how to generate and save vector data to your Heroku database, I'm using the Wikipedia2Vec pretrained embeddings. However, you can train your own embeddings or use other models providing embeddings via API, like HuggingFace or OpenAI. The model you want to use depends on the type of data you're working with. There are models for tasks like computing sentence similarities, searching large texts, or performing image classification. Wikipedia2Vec uses a Word2vec algorithm to generate vectors for individual words, which maps similar words close to each other in a continuous vector space. \nI like animals, so I want to use Wikipedia2Vec to group similar animals. I\u2019m using the vector embeddings of each animal and the distance between them to find animals that are alike.\nIf I want to get the embedding for a word from Wikipedia2Vec, I need to use a model. I downloaded one from the pretrained embeddings on their website. Then I can use their Python module and the function get_word_vector as follows:\nfrom wikipedia2vec import Wikipedia2Vec\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nwiki2vec.get_word_vector('llama')\n\nThe output of the vector looks like this:\nmemmap([-0.15647224,  0.04055957,  0.48439676, -0.22689971, -0.04544162,\n        -0.06538601,  0.22609918, -0.26075622, -0.7195759 , -0.24022003,\n         0.1050799 , -0.5550985 ,  0.4054564 ,  0.14180332,  0.19856507,\n         0.09962048,  0.38372937, -1.1912689 , -0.93939453, -0.28067762,\n         0.04410955,  0.43394643, -0.3429818 ,  0.22209083, -0.46317756,\n        -0.18109794,  0.2775289 , -0.21939017, -0.27015808,  0.72002393,\n        -0.01586861, -0.23480305,  0.365697  ,  0.61743397, -0.07460125,\n        -0.10441436, -0.6537417 ,  0.01339269,  0.06189647, -0.17747395,\n         0.2669941 , -0.03428648, -0.8533792 , -0.09588563, -0.7616592 ,\n        -0.11528812, -0.07127796,  0.28456485, -0.12986512, -0.8063386 ,\n        -0.04875885, -0.27353695, -0.32921   , -0.03807172,  0.10544889,\n         0.49989182, -0.03783042, -0.37752548, -0.19257008,  0.06255971,\n         0.25994852, -0.81092316, -0.15077794,  0.00658835,  0.02033841,\n        -0.32411653, -0.03033727, -0.64633304, -0.43443972, -0.30764043,\n        -0.11036412,  0.04134453, -0.26934972, -0.0289086 , -0.50319433,\n        -0.0204528 , -0.00278326,  0.36589545,  0.5446438 , -0.10852882,\n         0.09699931, -0.01168614,  0.08618425, -0.28925297, -0.25445923,\n         0.63120073,  0.52186656,  0.3439454 ,  0.6686451 ,  0.1076297 ,\n        -0.34688494,  0.05976971, -0.3720558 ,  0.20328045, -0.485623  ,\n        -0.2222396 , -0.22480975,  0.4386788 , -0.7506131 ,  0.14270408],\n       dtype=float32)\n\nTo get your vector data into your database:\n\nGenerate the embeddings.\nAdd a column to your database to store your embeddings.\nSave the embeddings to the database.\n\nI already have the embeddings from Wikipedia2Vec, so let\u2019s walk through preparing my database and saving them. When creating a vector column, it's necessary to declare a length for it, so check and see the length of the embedding the model outputs. In my case, the embeddings are 100 numbers long, so I add that column to my table.\nCREATE TABLE animals(id serial PRIMARY KEY, name VARCHAR(100), embedding VECTOR(100));\n\nFrom there, save the items you're interested in to your database. You can do it directly in SQL:\nINSERT INTO animals(name, embedding) VALUES ('llama', '[-0.15647223591804504, \n\u2026\n-0.7506130933761597, 0.1427040845155716]');\n\nBut you can also use your favorite programming language along with a Postgres client and a pgvector library. For this example, I used Python, psycopg, and pgvector-python. Here I'm using the pretrained embedding file to generate embeddings for a list of animals I made, valeries-animals.txt,  and save them to my database.\nimport psycopg\nfrom pathlib import Path\nfrom pgvector.psycopg import register_vector\nfrom wikipedia2vec import Wikipedia2Vec\n\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nanimals = Path('valeries-animals.txt').read_text().split('\\n')\n\nwith psycopg.connect(DATABASE_URL, sslmode='require', autocommit=True) as conn:\n    register_vector(conn)\n    cur = conn.cursor()\n    for animal in animals:\n        cur.execute(\"INSERT INTO animals(name, embedding) VALUES (%s, %s)\", (animal, wiki2vec.get_word_vector(animal)))\n\nNow that I have the embeddings in my database, I can use pgvector's functions to query them. The extension includes functions to calculate Euclidean distance (<->), cosine distance (<=>), and inner product (<#>). You can use all three for calculating similarity between vectors. Which one you use depends on your data as well as your use case.\nHere I'm using Euclidean distance to find the five animals closest to a shark:\n=> SELECT name FROM animals WHERE name != 'shark' ORDER BY embedding <-> (SELECT embedding FROM animals WHERE name = 'shark') LIMIT 5;\n name \n-----------\n crocodile\n dolphin\n whale\n turtle\n alligator\n(5 rows)\n\nIt works! It's worth noting that the model that we used is based on words appearing together in Wikipedia articles, and different models or source corpuses likely yield different results. The results here are also limited to the hundred or so animals that I added to my database.\n\npgvector Optimization and Performance Considerations\n\nAs you add more vector data to your database, you may notice performance issues or slowness in performing queries. You can index vector data like other columns in Postgres, and pgvector provides a few ways to do so, but there are some important considerations to keep in mind:\n\nAdding an index causes pgvector to switch to using approximate nearest neighbor search instead of exact nearest neighbor search, possibly causing a difference in query results.\nIndexing functions are based on distance calculations, so create one based on the calculation you plan to rely on the most in your application.\nThere are two index types supported, IVFFlat and HNSW. Before you add an IVFFlat index, make sure you have some data in your table for better recall.\n\nCheck out the pgvector documentation for more information on indexing and other performance considerations.\n\nCollaborate and Share Your pgvector Projects\n\nNow that pgvector for Heroku Postgres is out in the world, we're really excited to hear what you do with it! One of pgvector's great advantages is that it lets vector data live alongside all the other data you might already have in Postgres. You can add an embedding column to your existing tables and start experimenting. Our launch blog for this feature includes a lot of ideas and possible use cases for how to use this new tool, and I'm sure you can come up with many more. If you have questions, our Support team is available to assist. Don't forget you can share your solutions using the Heroku Button on your repo. If you feel like blogging on your success, tag us on social media and we would love to read about it!\n\n\n\n\n\n\n\nTweet\nShare\nShare\n\n\n\n\n\n\n\n    Browse the archives for\n    engineering\n    or\n    all blogs\n\n    Subscribe to the RSS feed for\n    engineering\n    or\n    all blogs.\n  \n\n\n\n\n\n\n\nProducts\n\nHeroku Platform\nHeroku Data Services\nHeroku Postgres\nHeroku Data for Redis\nApache Kafka on Heroku\nHeroku Enterprise\nHeroku Private Spaces\nHeroku Connect\nHeroku Shield\nHeroku Teams\nElements Marketplace\nPricing\n\n\n\nResources\n\nDocumentation\nCompliance Center\nTraining & Education\nBlog\nGet Started\n\n\n\nAbout\n\nAbout Us\nWhat is Heroku\nHeroku & Salesforce\nOur Customers\nCareers\nPartners\nElements Marketplace\n\n\n\nSupport\n\nHelp Center\nStatus\nContact Us\n\n\n\n\nSubscribe to our monthly newsletter\n\n\n\nYour email address:\n\n\n\n\n\n\n RSS\n\n\nHeroku Blog\nHeroku News Blog\nHeroku Engineering Blog\nDev Center Articles\nDev Center Changelog\nHeroku Status\n\n\n\n\n Twitter\n\n\nHeroku\nDev Center Articles\nDev Center Changelog\nHeroku Status\n\n\n\nGitHub\nLinkedIn\n\n\n\n\n\n\nHeroku is a  company\n\n\n\nheroku.com\nTerms of Service\nPrivacy\nCookies\nCookie Preferences\n Your Privacy Choices\n\n            \u00a9\n            2024\n            Salesforce.com\n          \n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "title": "How to Use pgvector for Similarity Search on Heroku Postgres",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://blog.heroku.com/engineering/feed",
      "value": "How to Use pgvector for Similarity Search on Heroku Postgres"
    },
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://blog.heroku.com/pgvector-for-similarity-search-on-heroku-postgres"
      }
    ],
    "link": "https://blog.heroku.com/pgvector-for-similarity-search-on-heroku-postgres",
    "published": "Wed, 15 Nov 2023 18:42:51 GMT",
    "published_parsed": [
      2023,
      11,
      15,
      18,
      42,
      51,
      2,
      319,
      0
    ],
    "id": "https://blog.heroku.com/pgvector-for-similarity-search-on-heroku-postgres",
    "guidislink": false,
    "summary": "<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#introducing-pgvector-for-heroku-postgres\" name=\"introducing-pgvector-for-heroku-postgres\">Introducing pgvector for Heroku Postgres</a>\n</h2>\n\n<p>Over the past few weeks, we worked on adding <a href=\"https://github.com/pgvector/pgvector\">pgvector</a> as an extension on Heroku Postgres. We're excited to release this feature, and based on the feedback on <a href=\"https://github.com/heroku/roadmap/issues/156\">our public roadmap</a>, many of you are too. We want to share a bit more about how you can use it and how it may be helpful to you. </p>\n\n<p>All <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-plans#plan-tiers\">Standard-tier or higher</a> databases running Postgres 15 now support the <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-extensions-postgis-full-text-search#pgvector\"><code>pgvector</code> extension</a>. You can get started by running <code>CREATE EXTENSION vector;</code> in a client session. Postgres 15 has been the default version on Heroku Postgres since March 2023.  If you're on an older version and want to use pgvector, <a href=\"https://devcenter.heroku.com/articles/upgrading-heroku-postgres-databases\">upgrade</a> to Postgres 15.</p>\n\n<p>The extension adds the vector data type to Heroku Postgres along with additional functions to work with it. Vectors are important for working with large language models and other machine learning applications, as the <a href=\"https://huggingface.co/blog/getting-started-with-embeddings#understanding-embeddings\">embeddings</a> generated by these models are often output in vector format. Working with vectors lets you implement things like similarity search across these embeddings. See our <a href=\"https://blog.heroku.com/pgvector-launch#understanding-pgvector-and-its-significance\">launch blog</a> for more background into what pgvector is, its significance, and ideas for how to use this new data type.</p>\n<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#an-example-word-vector-similarity-search\" name=\"an-example-word-vector-similarity-search\">An Example: Word Vector Similarity Search</a>\n</h2>\n\n<p>To show a simple example of how to generate and save vector data to your Heroku database, I'm using the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/\">Wikipedia2Vec</a> pretrained embeddings. However, you can train your own embeddings or use other models providing embeddings via API, like <a href=\"https://huggingface.co/blog/getting-started-with-embeddings\">HuggingFace</a> or <a href=\"https://openai.com/\">OpenAI</a>. The model you want to use depends on the type of data you're working with. There are models for tasks like computing sentence similarities, searching large texts, or performing image classification. Wikipedia2Vec uses a <a href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2vec</a> algorithm to generate vectors for individual words, which maps similar words close to each other in a continuous vector space. </p>\n\n<p>I like animals, so I want to use Wikipedia2Vec to group similar animals. I\u2019m using the vector embeddings of each animal and the distance between them to find animals that are alike.</p>\n\n<p>If I want to get the embedding for a word from Wikipedia2Vec, I need to use a model. I downloaded one from the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\">pretrained embeddings</a> on their website. Then I can use their Python module and the function <code>get_word_vector</code> as follows:</p>\n\n<pre><code>from wikipedia2vec import Wikipedia2Vec\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nwiki2vec.get_word_vector('llama')\n</code></pre>\n\n<p>The output of the vector looks like this:</p>\n\n<pre><code>memmap([-0.15647224,  0.04055957,  0.48439676, -0.22689971, -0.04544162,\n        -0.06538601,  0.22609918, -0.26075622, -0.7195759 , -0.24022003,\n         0.1050799 , -0.5550985 ,  0.4054564 ,  0.14180332,  0.19856507,\n         0.09962048,  0.38372937, -1.1912689 , -0.93939453, -0.28067762,\n         0.04410955,  0.43394643, -0.3429818 ,  0.22209083, -0.46317756,\n        -0.18109794,  0.2775289 , -0.21939017, -0.27015808,  0.72002393,\n        -0.01586861, -0.23480305,  0.365697  ,  0.61743397, -0.07460125,\n        -0.10441436, -0.6537417 ,  0.01339269,  0.06189647, -0.17747395,\n         0.2669941 , -0.03428648, -0.8533792 , -0.09588563, -0.7616592 ,\n        -0.11528812, -0.07127796,  0.28456485, -0.12986512, -0.8063386 ,\n        -0.04875885, -0.27353695, -0.32921   , -0.03807172,  0.10544889,\n         0.49989182, -0.03783042, -0.37752548, -0.19257008,  0.06255971,\n         0.25994852, -0.81092316, -0.15077794,  0.00658835,  0.02033841,\n        -0.32411653, -0.03033727, -0.64633304, -0.43443972, -0.30764043,\n        -0.11036412,  0.04134453, -0.26934972, -0.0289086 , -0.50319433,\n        -0.0204528 , -0.00278326,  0.36589545,  0.5446438 , -0.10852882,\n         0.09699931, -0.01168614,  0.08618425, -0.28925297, -0.25445923,\n         0.63120073,  0.52186656,  0.3439454 ,  0.6686451 ,  0.1076297 ,\n        -0.34688494,  0.05976971, -0.3720558 ,  0.20328045, -0.485623  ,\n        -0.2222396 , -0.22480975,  0.4386788 , -0.7506131 ,  0.14270408],\n       dtype=float32)\n</code></pre>\n\n<p>To get your vector data into your database:</p>\n\n<ol>\n<li>Generate the embeddings.</li>\n<li>Add a column to your database to store your embeddings.</li>\n<li>Save the embeddings to the database.</li>\n</ol>\n\n<p>I already have the embeddings from Wikipedia2Vec, so let\u2019s walk through preparing my database and saving them. When creating a vector column, it's necessary to declare a length for it, so check and see the length of the embedding the model outputs. In my case, the embeddings are 100 numbers long, so I add that column to my table.</p>\n\n<pre><code>CREATE TABLE animals(id serial PRIMARY KEY, name VARCHAR(100), embedding VECTOR(100));\n</code></pre>\n\n<p>From there, save the items you're interested in to your database. You can do it directly in SQL:</p>\n\n<pre><code>INSERT INTO animals(name, embedding) VALUES ('llama', '[-0.15647223591804504, \n\u2026\n-0.7506130933761597, 0.1427040845155716]');\n</code></pre>\n\n<p>But you can also use your <a href=\"https://devcenter.heroku.com/articles/connecting-heroku-postgres\">favorite programming language</a> along with a Postgres client and a <a href=\"https://github.com/pgvector/pgvector#languages\">pgvector library</a>. For this example, I used Python, <a href=\"https://github.com/psycopg/psycopg\">psycopg</a>, and <a href=\"https://github.com/pgvector/pgvector-python\">pgvector-python</a>. Here I'm using the pretrained embedding file to generate embeddings for a list of animals I made, <code>valeries-animals.txt</code>,  and save them to my database.</p>\n\n<pre><code>import psycopg\nfrom pathlib import Path\nfrom pgvector.psycopg import register_vector\nfrom wikipedia2vec import Wikipedia2Vec\n\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nanimals = Path('valeries-animals.txt').read_text().split('\\n')\n\nwith psycopg.connect(DATABASE_URL, sslmode='require', autocommit=True) as conn:\n    register_vector(conn)\n    cur = conn.cursor()\n    for animal in animals:\n        cur.execute(\"INSERT INTO animals(name, embedding) VALUES (%s, %s)\", (animal, wiki2vec.get_word_vector(animal)))\n</code></pre>\n\n<p>Now that I have the embeddings in my database, I can use pgvector's functions to query them. The extension includes functions to calculate Euclidean distance (<code>&lt;-&gt;</code>), cosine distance (<code>&lt;=&gt;</code>), and inner product (<code>&lt;#&gt;</code>). You can use all three for <a href=\"https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity\">calculating similarity</a> between vectors. Which one you use depends on <a href=\"https://cmry.github.io/notes/euclidean-v-cosine\">your data as well as your use case</a>.</p>\n\n<p>Here I'm using Euclidean distance to find the five animals closest to a shark:</p>\n\n<pre><code>=&gt; SELECT name FROM animals WHERE name != 'shark' ORDER BY embedding &lt;-&gt; (SELECT embedding FROM animals WHERE name = 'shark') LIMIT 5;\n name \n-----------\n crocodile\n dolphin\n whale\n turtle\n alligator\n(5 rows)\n</code></pre>\n\n<p>It works! It's worth noting that the model that we used is based on words appearing together in Wikipedia articles, and different models or source corpuses likely yield different results. The results here are also limited to the hundred or so animals that I added to my database.</p>\n<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#pgvector-optimization-and-performance-considerations\" name=\"pgvector-optimization-and-performance-considerations\">pgvector Optimization and Performance Considerations</a>\n</h2>\n\n<p>As you add more vector data to your database, you may notice performance issues or slowness in performing queries. You can index vector data like other columns in Postgres, and pgvector provides a few ways to do so, but there are some important considerations to keep in mind:</p>\n\n<ul>\n<li>Adding an index causes pgvector to switch to using approximate nearest neighbor search instead of exact nearest neighbor search, possibly causing a difference in query results.</li>\n<li>Indexing functions are based on distance calculations, so create one based on the calculation you plan to rely on the most in your application.</li>\n<li>There are two index types supported, IVFFlat and HNSW. Before you add an IVFFlat index, make sure you have some data in your table for better recall.</li>\n</ul>\n\n<p>Check out the <a href=\"https://github.com/pgvector/pgvector#indexing\">pgvector documentation</a> for more information on indexing and other performance considerations.</p>\n<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#collaborate-and-share-your-pgvector-projects\" name=\"collaborate-and-share-your-pgvector-projects\">Collaborate and Share Your pgvector Projects</a>\n</h2>\n\n<p>Now that pgvector for Heroku Postgres is out in the world, we're really excited to hear what you do with it! One of pgvector's great advantages is that it lets vector data live alongside all the other data you might already have in Postgres. You can add an embedding column to your existing tables and start experimenting. Our <a href=\"https://blog.heroku.com/pgvector-launch\">launch blog</a> for this feature includes a lot of ideas and possible use cases for how to use this new tool, and I'm sure you can come up with many more. If you have questions, our <a href=\"https://help.heroku.com/\">Support team</a> is available to assist. Don't forget you can share your solutions using the <a href=\"https://devcenter.heroku.com/articles/heroku-button\">Heroku Button</a> on your repo. If you feel like blogging on your success, tag us on social media and we would love to read about it!</p>",
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://blog.heroku.com/engineering/feed",
      "value": "<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#introducing-pgvector-for-heroku-postgres\" name=\"introducing-pgvector-for-heroku-postgres\">Introducing pgvector for Heroku Postgres</a>\n</h2>\n\n<p>Over the past few weeks, we worked on adding <a href=\"https://github.com/pgvector/pgvector\">pgvector</a> as an extension on Heroku Postgres. We're excited to release this feature, and based on the feedback on <a href=\"https://github.com/heroku/roadmap/issues/156\">our public roadmap</a>, many of you are too. We want to share a bit more about how you can use it and how it may be helpful to you. </p>\n\n<p>All <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-plans#plan-tiers\">Standard-tier or higher</a> databases running Postgres 15 now support the <a href=\"https://devcenter.heroku.com/articles/heroku-postgres-extensions-postgis-full-text-search#pgvector\"><code>pgvector</code> extension</a>. You can get started by running <code>CREATE EXTENSION vector;</code> in a client session. Postgres 15 has been the default version on Heroku Postgres since March 2023.  If you're on an older version and want to use pgvector, <a href=\"https://devcenter.heroku.com/articles/upgrading-heroku-postgres-databases\">upgrade</a> to Postgres 15.</p>\n\n<p>The extension adds the vector data type to Heroku Postgres along with additional functions to work with it. Vectors are important for working with large language models and other machine learning applications, as the <a href=\"https://huggingface.co/blog/getting-started-with-embeddings#understanding-embeddings\">embeddings</a> generated by these models are often output in vector format. Working with vectors lets you implement things like similarity search across these embeddings. See our <a href=\"https://blog.heroku.com/pgvector-launch#understanding-pgvector-and-its-significance\">launch blog</a> for more background into what pgvector is, its significance, and ideas for how to use this new data type.</p>\n<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#an-example-word-vector-similarity-search\" name=\"an-example-word-vector-similarity-search\">An Example: Word Vector Similarity Search</a>\n</h2>\n\n<p>To show a simple example of how to generate and save vector data to your Heroku database, I'm using the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/\">Wikipedia2Vec</a> pretrained embeddings. However, you can train your own embeddings or use other models providing embeddings via API, like <a href=\"https://huggingface.co/blog/getting-started-with-embeddings\">HuggingFace</a> or <a href=\"https://openai.com/\">OpenAI</a>. The model you want to use depends on the type of data you're working with. There are models for tasks like computing sentence similarities, searching large texts, or performing image classification. Wikipedia2Vec uses a <a href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2vec</a> algorithm to generate vectors for individual words, which maps similar words close to each other in a continuous vector space. </p>\n\n<p>I like animals, so I want to use Wikipedia2Vec to group similar animals. I\u2019m using the vector embeddings of each animal and the distance between them to find animals that are alike.</p>\n\n<p>If I want to get the embedding for a word from Wikipedia2Vec, I need to use a model. I downloaded one from the <a href=\"https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\">pretrained embeddings</a> on their website. Then I can use their Python module and the function <code>get_word_vector</code> as follows:</p>\n\n<pre><code>from wikipedia2vec import Wikipedia2Vec\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nwiki2vec.get_word_vector('llama')\n</code></pre>\n\n<p>The output of the vector looks like this:</p>\n\n<pre><code>memmap([-0.15647224,  0.04055957,  0.48439676, -0.22689971, -0.04544162,\n        -0.06538601,  0.22609918, -0.26075622, -0.7195759 , -0.24022003,\n         0.1050799 , -0.5550985 ,  0.4054564 ,  0.14180332,  0.19856507,\n         0.09962048,  0.38372937, -1.1912689 , -0.93939453, -0.28067762,\n         0.04410955,  0.43394643, -0.3429818 ,  0.22209083, -0.46317756,\n        -0.18109794,  0.2775289 , -0.21939017, -0.27015808,  0.72002393,\n        -0.01586861, -0.23480305,  0.365697  ,  0.61743397, -0.07460125,\n        -0.10441436, -0.6537417 ,  0.01339269,  0.06189647, -0.17747395,\n         0.2669941 , -0.03428648, -0.8533792 , -0.09588563, -0.7616592 ,\n        -0.11528812, -0.07127796,  0.28456485, -0.12986512, -0.8063386 ,\n        -0.04875885, -0.27353695, -0.32921   , -0.03807172,  0.10544889,\n         0.49989182, -0.03783042, -0.37752548, -0.19257008,  0.06255971,\n         0.25994852, -0.81092316, -0.15077794,  0.00658835,  0.02033841,\n        -0.32411653, -0.03033727, -0.64633304, -0.43443972, -0.30764043,\n        -0.11036412,  0.04134453, -0.26934972, -0.0289086 , -0.50319433,\n        -0.0204528 , -0.00278326,  0.36589545,  0.5446438 , -0.10852882,\n         0.09699931, -0.01168614,  0.08618425, -0.28925297, -0.25445923,\n         0.63120073,  0.52186656,  0.3439454 ,  0.6686451 ,  0.1076297 ,\n        -0.34688494,  0.05976971, -0.3720558 ,  0.20328045, -0.485623  ,\n        -0.2222396 , -0.22480975,  0.4386788 , -0.7506131 ,  0.14270408],\n       dtype=float32)\n</code></pre>\n\n<p>To get your vector data into your database:</p>\n\n<ol>\n<li>Generate the embeddings.</li>\n<li>Add a column to your database to store your embeddings.</li>\n<li>Save the embeddings to the database.</li>\n</ol>\n\n<p>I already have the embeddings from Wikipedia2Vec, so let\u2019s walk through preparing my database and saving them. When creating a vector column, it's necessary to declare a length for it, so check and see the length of the embedding the model outputs. In my case, the embeddings are 100 numbers long, so I add that column to my table.</p>\n\n<pre><code>CREATE TABLE animals(id serial PRIMARY KEY, name VARCHAR(100), embedding VECTOR(100));\n</code></pre>\n\n<p>From there, save the items you're interested in to your database. You can do it directly in SQL:</p>\n\n<pre><code>INSERT INTO animals(name, embedding) VALUES ('llama', '[-0.15647223591804504, \n\u2026\n-0.7506130933761597, 0.1427040845155716]');\n</code></pre>\n\n<p>But you can also use your <a href=\"https://devcenter.heroku.com/articles/connecting-heroku-postgres\">favorite programming language</a> along with a Postgres client and a <a href=\"https://github.com/pgvector/pgvector#languages\">pgvector library</a>. For this example, I used Python, <a href=\"https://github.com/psycopg/psycopg\">psycopg</a>, and <a href=\"https://github.com/pgvector/pgvector-python\">pgvector-python</a>. Here I'm using the pretrained embedding file to generate embeddings for a list of animals I made, <code>valeries-animals.txt</code>,  and save them to my database.</p>\n\n<pre><code>import psycopg\nfrom pathlib import Path\nfrom pgvector.psycopg import register_vector\nfrom wikipedia2vec import Wikipedia2Vec\n\nwiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\nanimals = Path('valeries-animals.txt').read_text().split('\\n')\n\nwith psycopg.connect(DATABASE_URL, sslmode='require', autocommit=True) as conn:\n    register_vector(conn)\n    cur = conn.cursor()\n    for animal in animals:\n        cur.execute(\"INSERT INTO animals(name, embedding) VALUES (%s, %s)\", (animal, wiki2vec.get_word_vector(animal)))\n</code></pre>\n\n<p>Now that I have the embeddings in my database, I can use pgvector's functions to query them. The extension includes functions to calculate Euclidean distance (<code>&lt;-&gt;</code>), cosine distance (<code>&lt;=&gt;</code>), and inner product (<code>&lt;#&gt;</code>). You can use all three for <a href=\"https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity\">calculating similarity</a> between vectors. Which one you use depends on <a href=\"https://cmry.github.io/notes/euclidean-v-cosine\">your data as well as your use case</a>.</p>\n\n<p>Here I'm using Euclidean distance to find the five animals closest to a shark:</p>\n\n<pre><code>=&gt; SELECT name FROM animals WHERE name != 'shark' ORDER BY embedding &lt;-&gt; (SELECT embedding FROM animals WHERE name = 'shark') LIMIT 5;\n name \n-----------\n crocodile\n dolphin\n whale\n turtle\n alligator\n(5 rows)\n</code></pre>\n\n<p>It works! It's worth noting that the model that we used is based on words appearing together in Wikipedia articles, and different models or source corpuses likely yield different results. The results here are also limited to the hundred or so animals that I added to my database.</p>\n<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#pgvector-optimization-and-performance-considerations\" name=\"pgvector-optimization-and-performance-considerations\">pgvector Optimization and Performance Considerations</a>\n</h2>\n\n<p>As you add more vector data to your database, you may notice performance issues or slowness in performing queries. You can index vector data like other columns in Postgres, and pgvector provides a few ways to do so, but there are some important considerations to keep in mind:</p>\n\n<ul>\n<li>Adding an index causes pgvector to switch to using approximate nearest neighbor search instead of exact nearest neighbor search, possibly causing a difference in query results.</li>\n<li>Indexing functions are based on distance calculations, so create one based on the calculation you plan to rely on the most in your application.</li>\n<li>There are two index types supported, IVFFlat and HNSW. Before you add an IVFFlat index, make sure you have some data in your table for better recall.</li>\n</ul>\n\n<p>Check out the <a href=\"https://github.com/pgvector/pgvector#indexing\">pgvector documentation</a> for more information on indexing and other performance considerations.</p>\n<h2 class=\"anchored\">\n  <a href=\"https://blog.heroku.com/engineering/feed#collaborate-and-share-your-pgvector-projects\" name=\"collaborate-and-share-your-pgvector-projects\">Collaborate and Share Your pgvector Projects</a>\n</h2>\n\n<p>Now that pgvector for Heroku Postgres is out in the world, we're really excited to hear what you do with it! One of pgvector's great advantages is that it lets vector data live alongside all the other data you might already have in Postgres. You can add an embedding column to your existing tables and start experimenting. Our <a href=\"https://blog.heroku.com/pgvector-launch\">launch blog</a> for this feature includes a lot of ideas and possible use cases for how to use this new tool, and I'm sure you can come up with many more. If you have questions, our <a href=\"https://help.heroku.com/\">Support team</a> is available to assist. Don't forget you can share your solutions using the <a href=\"https://devcenter.heroku.com/articles/heroku-button\">Heroku Button</a> on your repo. If you feel like blogging on your success, tag us on social media and we would love to read about it!</p>"
    },
    "authors": [
      {
        "name": "Valerie Woolard"
      }
    ],
    "author": "Valerie Woolard",
    "author_detail": {
      "name": "Valerie Woolard"
    }
  }
}