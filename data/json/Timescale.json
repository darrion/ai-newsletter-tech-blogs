{
  "company": "Timescale",
  "title": "Timescale",
  "xmlUrl": "https://blog.timescale.com/feed",
  "htmlUrl": "https://blog.timescale.com/",
  "content": "\n\n\n\n\nGetting Sensor Data Into TimescaleDB via Django\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nProducts \n\n\n\n\n\n\n\n\n\r\n          Timescale is a mature PostgreSQL cloud, specialized for your demanding workloads.\r\n        \n\n\n\n\nProduct Link\n\n\n\n\n\nTime series and analytics\n\n\r\n                  PostgreSQL, but faster. Built for lightning-fast ingest and querying of time-based\r\n                  data.\r\n                \n\n\n\n\n\n\nProduct Link\n\nVector (AI/ML) Early access\n\n\r\n                 PostgreSQL engineered for fast search with high recall on millions of vector embeddings.\r\n                \n\n\n\n\nProduct Link\n\nDynamic Postgres Early access\n\n\r\n                  PostgreSQL managed services with the benefits of serverless, but none of the problems.\r\n                \n\n\n\n\n\r\n              Industries that rely on us\r\n            \n\r\n              Crypto and finance\r\n            \n\r\n              Energy and environment\r\n            \n\r\n              Transportation and logistics\r\n            \n\n\n\r\n              Timescale benchmarks\r\n            \n\r\n              vs RDS\r\n            \n\r\n              vs Amazon Timestream\r\n            \n\r\n              vs Influx\r\n            \n\r\n             vs Cassandra\r\n            \n\r\n              vs MongoDB\r\n            \n\r\n              vs ClickHouse\r\n            \n\n\n\n\n\n\n\r\n          We\u2019re in your corner even during the trial phase. Contact us to discuss your use case with\r\n          a Timescale technical expert.\r\n        \n\n\r\n            Schedule a technical review\r\n          \n\r\n            Explore our Enterprise Tier\r\n          \n\n\n\n\n \n\n          Customer Stories\n        \n\n\n\nDevelopers \n\n\n\n\n\n\n\n\n\n\r\n            Docs\nTutorials\nGuides\nBlog\nSupport\nDevelopers' newsletter\nPostgreSQL Cheat Sheet\n\nSupport\nCommunity\nGitHub\nSlack\nForum\nContribute\nContribute to code or Docs\n\n\n\nLearn\n\n\nTime-Series Database Basics\n5 InfluxDB Alternatives for Your Time-Series Data\nInfluxQL, Flux, and SQL: Which Query Language Is Best? (With Cheatsheet)\nAn Intro to Database Indexes in Postgres\nTimescale & PostgreSQL\nPostgreSQL Performance Tuning: How to Size Your Database\nWhen to Consider Postgres Partitioning\nBeyond the Basics: Exploring PostgreSQL Extensions\n\n\nBuilding Blocks\nReal-time Analytics in Postgres: Why It's Hard (and How to Solve It)\nMore Time-Series Data Analysis, Less Lines of Code: Meet Hyperfunctions\nTimescale Benchmarks\nvs RDS\nvs Amazon Timestream\nvs Influx\nvs Cassandra\nvs MongoDB\nvs ClickHouse\n\n\n\n\n\n \nPricing\n\n\n\n\n\n      Contact us\n    \n\nLogin\n\n\n\n        Try for free\n      \n\n\n \n\n\n\n\n\n\n\n\n\n\n\nProducts\n\n\n\n\n\r\n      Timescale is a mature PostgreSQL cloud, specialized for your demanding workloads.\r\n    \n\n\n\n\nProduct Link\n\n\n\n\n\nTime series and analytics\n\n\r\n              PostgreSQL, but faster. Built for lightning-fast ingest and querying of time-based\r\n              data.\r\n            \n\n\n\n\nProduct Link\n\nVector (AI/ML) Early access\n\n\r\n              PostgreSQL engineered for fast search with high recall on millions of vector\r\n              embeddings.\r\n            \n\n\n\n\nProduct Link\n\nDynamic Postgres Early access\n\n\r\n              PostgreSQL managed services with the benefits of serverless, but none of the problems.\r\n            \n\n\n\n\n\r\n          Industries that rely on us\r\n        \n\r\n          Crypto and finance\r\n        \n\r\n          Energy and environment\r\n        \n\r\n          Transportation and logistics\r\n        \n\r\n          Crypto and finance\r\n        \n\n\n\r\n          Timescale benchmarks\r\n        \n\r\n          vs RDS\r\n        \n\r\n          vs Amazon Timestream\r\n        \n\r\n          vs Influx\r\n        \n\r\n          vs Cassandra\r\n        \n\r\n          vs MongoDB\r\n        \n\r\n          vs ClickHouse\r\n        \n\n\n\n\n\r\n      We\u2019re in your corner even during the trial phase. Contact us to discuss your use case with a\r\n      Timescale technical expert.\r\n    \n\n\r\n        Schedule a technical review\r\n      \n\r\n        Explore our Enterprise Tier\r\n      \n\n\n \n\nCustomer Stories\n\n\n\nDevelopers\n\n\n\n\n\r\n      Docs\nTutorials\nGuides\nBlog\nSupport\nDevelopers' newsletter\nPostgreSQL Cheat Sheet\n\nSupport\nCommunity\nGithub\nSlack\nForum\nContribute\nContribute to code or Docs\n\n\nLearn\n\nTime-Series Database Basics\n5 InfluxDB Alternatives for Your Time-Series Data\nInfluxQL, Flux, and SQL: Which Query Language Is Best? (With Cheatsheet)\nAn Intro to Database Indexes in Postgres\nTimescale & PostgreSQL\nPostgreSQL Performance Tuning: How to Size Your Database\nWhen to Consider Postgres Partitioning\nBeyond the Basics: Exploring PostgreSQL Extensions\n\n\nBuilding Blocks\nReal-time Analytics in Postgres: Why It's Hard (and How to Solve It)\nMore Time-Series Data Analysis, Less Lines of Code: Meet Hyperfunctions\nTimescale Benchmarks\nvs RDS\nvs Amazon Timestream\nvs Influx\nvs Cassandra\nvs MongoDB\nvs ClickHouse\n\n\n \n\nPricing\n\n\nContact us\n\n\nLogin\n\n\n\n\n          Try for free\n        \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Categories\n\nAll posts\nAI\nAnnouncements\nCloud\nDeveloper Q&A\nEngineering\nGeneral\nGrafana\nObservability\nPostgreSQL\nProduct Updates\n\n\n\n\n\n\n\n\n\n\n\n                All posts\n              \n\n                AI\n              \n\n                Announcements\n              \n\n                Cloud\n              \n\n                Developer Q&A\n              \n\n                Engineering\n              \n\n                General\n              \n\n                Grafana\n              \n\n                Observability\n              \n\n                PostgreSQL\n              \n\n                Product Updates\n              \n\n\n\n\n\n\n\nSubscribe to the Timescale Newsletter\n\n\n\n\n\n\nSubscribe\n\n\nBy submitting you acknowledge Timescale's\u00a0\n    Privacy Policy.\n  \n \n\n\n\n\n\n\n\n        Getting Sensor Data Into TimescaleDB via Django\n      \n\n\n\n\n\n          Table of contents\n        \n\n\n\nOver 2022-23, while working at Mainstream Renewable Power on an internal web application, I maintained a \"data pipeline\" that fetches files of sensor data readings from the world's most remote places and transforms them into useful datasets. These datasets form the basis upon which the construction of renewables (wind turbines or solar panels) on site hinges. I rebuilt the pipeline on top of TimescaleDB, which enabled me to massively reduce the complexity of the system involved.\ud83d\udca5Don\u2019t know what TimescaleDB is? Read this article.I reflect on this experience in detail in Struggling to Sync Sensors & Databases. I do not, however, discuss how I adapted Django, a Python web framework, to play nicely with this database. In my case, Django served as the \u201cglue\u201d between web browsers and the database. Specifically, to display a web page, it asks a database for the data it needs to render files that the browser interprets (HTML, CSS, and JavaScript) so it can display a user interface.Let\u2019s walk through an example project to make these adaptations a bit more concrete.This tutorial assumes some familiarity with Django or a similar web framework. If you have never used Django, I highly recommend the official tutorial.If you want to follow along locally, you can set up a developer environment via django-timescaledb-example.If you have any trouble getting set up, feel free to ask a question at django-timescaledb-example/discussions.Create a Sensor Data App Using DjangoLet\u2019s first runpython manage.py startapp sensor\n\nto create filessensor\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 admin.py\n\u251c\u2500\u2500 apps.py\n\u251c\u2500\u2500 migrations\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 models.py\n\u251c\u2500\u2500 tests.py\n\u2514\u2500\u2500 views.py\n\nand register the app in core/settings.py.INSTALLED_APPS = [\n\n    # Builtin\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n\n    # Custom\n    'sensor'\n]\n\nCreate a HomepageLet\u2019s quickly create a homepage that will be displayed on first opening this web application in a browser.# sensor/views.py\n\nfrom django.shortcuts import render\n\n\ndef index(request):\n    return render(request, \"index.html\")\n\n<!--- sensor/templates/index.html -->\n\n<div style=\"text-align: center\">\n  <h1>django-timescaledb-example</h1>\n</div>\n\n# sensor/urls.py\n\nfrom django.urls import path\n\nfrom . import views\n\n\napp_name = \"sensor\"\n\n\nurlpatterns = [\n    path('', views.index, name=\"root\"),\n]\n\n# core/urls.py\n\nfrom django.contrib import admin\nfrom django.shortcuts import redirect\nfrom django.urls import include\nfrom django.urls import path\n\n\nurlpatterns = [\n    path('', lambda request: redirect('sensor:root')),\n\n    path('admin/', admin.site.urls),\n    path('sensor/', include('sensor.urls')),\n]\n\nSo now http://localhost:8000 should display index.html. We can build on this index.html to link to other pages.Create a Data Model for FilesNow, I can adapt sensor/models.py to add a File model to track uploaded files,# sensor/models.py\n\nclass File(models.Model):\n    file = models.FileField(upload_to=\"readings/\", blank=False, null=False)\n    uploaded_at = models.DateTimeField(auto_now_add=True)\n    parsed = models.DateTimeField(blank=False, null=False)\n    parse_error = models.TextField(blank=True, null=True)\n\ncreate its database migration,python manage.py makemigrations sensor\n\nand roll it out:python manage.py migrate\n\nAny change to sensor/models.py requires a corresponding database migration!Handle File Uploads via BrowserNow that we have somewhere to store files of readings, we need to handle file uploads.Let\u2019s consider only the case where time-series data originates only from text files. How do I copy data from files into TimescaleDB via Django?The Django documentation covers File Uploads. However, it doesn\u2019t advise on importing file contents to a database. One normally uses Django to add and save new entries to PostgreSQL using input from a browser:Django sends a web page to a browser containing one or more <form> elements.Once filled in, these <form> elements are sent back to Django.Django processes these entries and saves them to the database using the Django ORM.The key enabler here is the ORM (or \u201cObject Relational Mapper\u201d). It maps a Python class to a database table so that this table\u2019s data is easily accessible from within Python. Without an ORM, one would have to use the SQL language to communicate with the database.We need to do a bit of work to adapt this workflow to handle file contents.In a similar manner, I can create a \u201cview\u201d to render HTML to accept browser file uploads:# sensor/views.py\n\nfrom django.shortcuts import render\nfrom django.shortcuts import redirect\nfrom django.http import HttpResponse\n\nfrom .forms import FileForm\n\n\n# ...\n\n\ndef upload_file(request):\n    if request.method == \"POST\":\n        form = FileForm(request.POST, request.FILES)\n        if form.is_valid():\n            form.save()\n            return HttpResponse(\"File upload was successful\")\n        else:\n            return HttpResponse(\"File upload failed\")\n    else:\n        form = FileForm()\n    return render(request, \"upload_file.html\", {\"form\": form})\n\n\n# ...\n\n# sensor/forms.py\n\nfrom django.forms import ModelForm\n\nfrom .models import File\n\n\nclass FileForm(ModelForm):\n    class Meta:\n        model = File\n        fields = \"__all__\"\n\n<!--- sensor/templates/upload_file.html -->\n\n<div style=\"text-align: center\">\n  <h1>Upload File</h1>\n  <form enctype=\"multipart/form-data\" method=\"post\">\n    {% csrf_token %}\n    {% for field in form %}\n      <div style=\"margin-bottom: 10px\">\n        {{ field.label_tag }}\n        {{ field }}\n        {% if field.help_text %}\n          <span class=\"question-mark\" title=\"{{ field.help_text }}\">&#63;</span>\n        {% endif %}\n      </div>\n    {% endfor %}\n    <input type=\"submit\" value=\"Save\"></input>\n  </form>\n</div>\n\n# sensor/urls.py\n\nfrom django.urls import path\n\nfrom . import views\n\n\napp_name = \"sensor\"\n\n\nurlpatterns = [\n    path('', views.index, name=\"root\"),\n    path('upload-file/', views.upload_file, name=\"upload-file\"),\n]\n\nThis requires someone to click through this web application every time they want to add new data. If data is synced automatically from remote sensors to a file system somewhere, then why not set up automatic file uploads? For this, we need an API.An API (or Application Programming Interface) lets our web application accept file uploads from another program.The django-rest-framework library does a lot of heavy lifting here, so let\u2019s use it.If you are not interested in APIs, feel free to skip the API sections.If you have never used django-rest-framework, consider first completing the official tutorial before continuing.Create an API HomepageAs suggested by Two Scoops of Django 3.x, let\u2019s create core/api_urls.py to wire up our API.# core/urls.py\n\nfrom django.contrib import admin\nfrom django.shortcuts import redirect\nfrom django.urls import include\nfrom django.urls import path\n\n\nurlpatterns = [\n    path('', lambda request: redirect('sensor:root')),\n\n    path('admin/', admin.site.urls),\n    path('api/', include('core.api_urls')),\n    path('sensor/', include('sensor.urls')),\n]\n\n# core/api_urls.py\n\nfrom django.urls import include\nfrom django.urls import path\nfrom rest_framework.decorators import api_view\nfrom rest_framework.response import Response\nfrom rest_framework.reverse import reverse\n\n\napp_name = \"api\"\n\n\n@api_view(['GET'])\ndef api_root(request, format=None):\n    return Response({\n        'sensors': reverse('api:sensor:api-root', request=request, format=format),\n    })\n\n\nurlpatterns = [\n    path('', api_root, name=\"api-root\"),\n    path('sensor/', include('sensor.api_urls'), name='sensor'),\n]\n\n# core/api_urls.py\n\nfrom django.urls import include\nfrom django.urls import path\nfrom rest_framework.decorators import api_view\nfrom rest_framework.response import Response\nfrom rest_framework.reverse import reverse\n\nfrom .api import viewsets\n\n\napp_name = \"sensor\"\n\n\n@api_view(['GET'])\ndef api_root(request, format=None):\n    return Response({})\n\n\nurlpatterns = [\n    path('', api_root, name=\"api-root\"),\n]\n\nSo now we can add new views and/or view sets to sensor/api_urls.py. Plus, they will be \u201cconnectable\u201d via /api/sensor/.Handle File Uploads via APIWe can use a viewset to create an endpoint like /api/sensor/file/ to which another program can upload files:# sensor/api/viewsets.py\n\nfrom rest_framework import viewsets\n\nfrom ..models import File\nfrom .serializers import FileSerializer\n\n\nclass FileViewSet(viewsets.ReadOnlyModelViewSet):\n    queryset = File.objects.all()\n    serializer_class = FileSerializer\n\n# sensor/api/serializers.py\n\nfrom rest_framework import serializers\n\nfrom ..models import File\n\n\nclass FileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = File\n        fields = ['__all__']\n\nCreate a Data Model for Sensor Data ReadingsLet\u2019s add a reading model to store readings:# sensor/models.py\n\nfrom django.db import models\n\n\n# ....\n\n\nclass Reading(models.Model):\n\n    class Meta:\n        managed = False\n\n    file = models.ForeignKey(File, on_delete=models.RESTRICT)\n    timestamp = models.DateTimeField(blank=False, null=False, primary_key=True)\n    sensor_name = models.TextField(blank=False, null=False)\n    reading = models.TextField(blank=False, null=False)\n\nThis time, we\u2019re using timestamp instead of the default id field as a primary key since row uniqueness can be defined by a composite of file, timestamp, and sensor_name if required.Don\u2019t we want to store readings in a TimescaleDB hypertable to make them easier to work with? Django won\u2019t automatically create a hypertable (it wasn\u2019t designed to), so we need to do so ourselves. Since we need to customize table creation ourselves rather than let Django do it, we have to set managed to False. Let\u2019s create a \u201cbase\u201d migration,python manage.py makemigrations sensor --name \"sensor_reading\"\n\nand manually edit the migration:# sensor/migrations/0002_sensor_reading.py\n\nimport django.db.models.deletion\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('sensor', '0001_initial')\n    ]\n    operations = [\n        migrations.CreateModel(\n            name='Reading',\n            fields=[\n                ('file', models.ForeignKey(default=None, on_delete=django.db.models.deletion.RESTRICT, to='sensor.file')),\n                ('timestamp', models.DateTimeField(primary_key=True)),\n                ('sensor_name', models.TextField()),\n                ('reading', models.FloatField()),\n            ],\n            options={\n                'managed': False,\n            },\n        ),\n        migrations.RunSQL(\n            \"\"\"\n            CREATE TABLE sensor_reading (\n                file_id INTEGER NOT NULL REFERENCES sensor_file (id),\n                timestamp TIMESTAMP NOT NULL,\n                sensor_name TEXT NOT NULL,\n                reading FLOAT\n            );\n            SELECT create_hypertable('sensor_reading', 'timestamp');\n            \"\"\",\n            reverse_sql=\"\"\"\n                DROP TABLE sensor_reading;\n            \"\"\"\n        ),\n    ]\n\nNow we can roll out migrations,python manage.py migrate\n\nand connect to the database to inspect the newly created hypertable.Import FilesLet\u2019s imagine that all of our sensor data readings are stored nicely formatted in JSON files, like: [\n    {\n      \"reading\": 20.54,\n      \"sensor_name\": \"M(m/s)\",\n      \"timestamp\": \"2015-12-22 00:00:00\",\n    },\n    {\n      \"reading\": 211.0,\n      \"sensor_name\": \"D(deg)\",\n      \"timestamp\": \"2015-12-22 00:00:00\",\n    },\n]\n\nHow do we import these readings?We can add a method to File to bring the JSON file into Python and create a new Reading entry for each reading in the file:# sensor/models.py\n\nfrom datetime import datetime\nfrom datetime import timezone\nfrom itertools import islice\n\nfrom django.contrib.postgres.fields import ArrayField\nfrom django.db import models\nfrom django.db import transaction\n\nfrom .io import validate_datetime_fieldnames_in_lines\nfrom .io import yield_readings_in_narrow_format\n\n\n# ...\n\n\nclass File(models.Model):\n\n    # ...\n\n    def import_to_db(self):\n\n        # NOTE: assume uploaded file is JSON\n        with self.file.open(mode=\"rb\") as f:\n\n            reading_objs = (\n                Reading(\n                    file=self,\n                    timestamp=r[\"timestamp\"],\n                    sensor_name=r[\"sensor_name\"],\n                    reading=r[\"reading\"]\n                )\n                for r in json.load(f)\n            )\n\n            batch_size = 1_000\n\n            with transaction.atomic():\n                while True:\n                    batch = list(islice(reading_objs, batch_size))\n                    if not batch:\n                        break\n                    Reading.objects.bulk_create(batch, batch_size)\n\nHow do we call the import_to_db method?We can go about this a few different ways, but perhaps the simplest is just to implement it directly in the views and viewsets so that it will be triggered on browser and API file uploads.For Django, we can call it directly in our upload-file view like:if form.is_valid():\n    form.save()\n    form.instance.import_to_db()\n\nAnd for django-rest-framework we can override the perform_create method.class FileViewSet(viewsets.ModelViewSet):\n\n    # ...\n\n    def perform_create(self, serializer):\n        instance = serializer.save()\n        instance.import_to_db()\n\nImport Files via CeleryWhat if each file contains a few gigabytes of readings? Won\u2019t this take an age to process?If you can\u2019t guarantee that the sensor files are small enough to be processed quickly, you might need to offload file importing to a task queue.A task queue works like a restaurant. The waiters add an order to the queue, and the chefs pull orders from the queue when they have time to process them.Celery is a mature Python task queue library and works well with Django, so let\u2019s use it. It coordinates \u201cwaiters\u201d and \u201cchefs\u201d using the above analogy by leveraging a database (or message broker), typically Redis or RabbitMQ.A task queue can significantly improve performance here. It makes file uploads instant from the user\u2019s perspective since now file upload tasks are added to a queue rather than running immediately. It also enables parallel processing of files since task queue workers run in parallel to one another.Celery may not work well on Windows, so consider trying dramatiq instead if this is a hard requirement.Why not use Postgres as a message broker? The django-q implementation enables this via the Django ORM message broker. This might be a better choice for small-scale applications since it reduces system complexity.To set up Celery, we can follow their official tutorial:# core/celery.py\n\nimport os\n\nfrom celery import Celery\n\n# Set the default Django settings module for the 'celery' program.\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'core.settings')\n\napp = Celery('core')\n\n# Using a string here means the worker doesn't have to serialize\n# the configuration object to child processes.\n# - namespace='CELERY' means all celery-related configuration keys\n#   should have a `CELERY_` prefix.\napp.config_from_object('django.conf:settings', namespace='CELERY')\n\n# Load task modules from all registered Django apps.\napp.autodiscover_tasks()\n\n\n@app.task(bind=True, ignore_result=True)\ndef debug_task(self):\n    print(f'Request: {self.request!r}')\n\n\n# core/settings.py\n\n# ...\n\nCELERY_BROKER_URL = \"redis://localhost:6379/0\"\n\n\nSo now we can add tasks to sensor/tasks.py like:# sensor/tasks.py\n\nfrom celery import shared_task\n\nfrom .models import File\n\n\n@shared_task\ndef import_to_db(file_id):\n    file_obj = File.objects.get(id=file_id)\n    file_obj.import_to_db()\n\n\nAnd replace all calls to <file_obj>.import_to_db() with tasks.import_to_db(file_obj), and this task won\u2019t be run immediately but rather will be run by Celery when it has the availability to do so!Next Steps?If you really want to further eke out import performance, you\u2019ll have to go deeper and experiment with the following:Batch sizes: how many readings do you want to save at once? Compression: TimescaleDB really shines once hypertables are compressed since it reduces storage costs and delivers faster analytics queries.We want to thank Rowan Molony for this awesome guest blog post (scroll down for some bonus advice) and for gracefully implementing our team's technical revisions in collaboration with the Timescale developer advocate J\u00f4natas. Join Rowan and thousands of other developers building with TimescaleDB in our Slack community. Once you're there, reach out to Ana if you, too, would like to write a guest blog post.To try TimescaleDB, you can self-host or create a fully managed Timescale account (it's free for 30 days, no credit card required).BonusImport messy filesHow do we convert files likeLat=0  Lon=0  Hub-Height=160  Timezone=00.0  Terrain-Height=0.0\nComputed at 100 m resolution\n \nYYYYMMDD HHMM   M(m/s) D(deg) SD(m/s)  DSD(deg)  Gust3s(m/s)    T(C)    PRE(hPa)       RiNumber  VertM(m/s)\n20151222 0000  20.54   211.0    1.22       0.3        21.00     11.9      992.8            0.15    0.18\n20151222 0010  21.02   212.2    2.55       0.6        21.35     11.8      992.7            0.29   -0.09\n\n\ninto[\n    {\n      'reading': '20.54',\n      'sensor_name': 'M(m/s)',\n      'timestamp': datetime.datetime(2015, 12, 22, 0, 0),\n    },\n    {\n      'reading': '211.0',\n      'sensor_name': 'D(deg)',\n      'timestamp': datetime.datetime(2015, 12, 22, 0, 0),\n    },\n    # ...\n]\n\n\n\u00a0so we can store them in the Reading data model?In this file, YYYYMMDD and HHMM clearly represent the timestamp, so 20151222 0000 corresponds to datetime.datetime(2015, 12, 22, 0, 0). However, this may differ between sources.One way to generalize the importer is to upload a FileType specification alongside each file so we know how to standardize it.We can create a new model FileType and link it to File like:# sensor/models.py\n\nfrom django.contrib.postgres.fields import ArrayField\nfrom django.db import models\n\n\nclass FileType(models.Model):\n\n    name = models.TextField()\n    na_values = ArrayField(\n        base_field=models.CharField(max_length=10),\n        default=[\"NaN\"],\n        help_text=textwrap.dedent(\n            r\"\"\"A list of strings to recognise as empty values.\n            \n            Default: [\"NaN\"]\n\n            Note: \"\" is also included by default\n\n            Example: [\"NAN\", \"-9999\", \"-9999.0\"]\n            \"\"\"\n        ),\n    )\n    delimiter = models.CharField(\n        max_length=5,\n        help_text=textwrap.dedent(\n            r\"\"\"The character used to separate fields in the file.\n            \n            Default: \",\"\n            \n            Examples: \",\" or \";\" or \"\\s+\" for whitespace or \"\\t\" for tabs\n            \"\"\"\n        ),\n        default=\",\",\n    )\n    datetime_fieldnames = ArrayField(\n        base_field=models.CharField(max_length=50),\n        default=[\"Time\"],\n        help_text=textwrap.dedent(\n            r\"\"\"A list of datetime field names.\n            \n            Examples:\n            \n            1) Data has a single datetime field named \"Time\" which has values like\n            '2021-06-29 00:00:00.000':  [\"Time\"]\n\n            2) Data has two datetime fields named \"Date\" and \"Time\" which have values\n            like '01.01.1999' and '00:00' respectively: [\"Date\",\"Time\"]\n            \"\"\"\n        ),\n    )\n    encoding = models.CharField(\n        max_length=25,\n        help_text=textwrap.dedent(\n            r\"\"\"The encoding of the file.\n\n            Default: \"utf-8\"\n\n            Examples: utf-8 or latin-1 or cp1252\n            \"\"\"\n        ),\n        default=\"utf-8\",\n    )\n    datetime_formats = ArrayField(\n        base_field=models.CharField(max_length=25),\n        help_text=textwrap.dedent(\n            r\"\"\"The datetime format of `datetime_columns`.\n\n            See https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\n            for format codes\n\n            Default: \"%Y-%m-%d %H:%M:%S\"\n\n            Examples: \"%Y-%m-%d %H:%M:%S\" for \"2021-03-01 00:00:00\"\n            \"\"\"\n        ),\n        default=[r\"%Y-%m-%d %H:%M:%S\"],\n    )\n\n\nclass File(models.Model):\n\n    # ...\n    type = models.ForeignKey(FileType, on_delete=models.RESTRICT)\n\nDjango forms are smart enough to automatically render the upload-file view with a type field since we specified fields = \"__all__\" in sensor/forms.py.django-rest-framework viewsets will also include it, thanks to fields = \"__all__\" in sensor/api/serializers.py. However, its default behavior for foreign keys is not ideal. It expects to receive a numeric ID for field type, whereas it\u2019s more intuitive to specify the name field instead.\u00a0We can easily override this default behavior by specifying SlugRelatedField in our serializer:# sensor/api/serializers.py\n\nfrom rest_framework import serializers\n\nfrom ..models import File\nfrom ..models import FileType\n\n\nclass FileSerializer(serializers.ModelSerializer):\n\n    type = serializers.SlugRelatedField(\n        slug_field=\"name\", queryset=FileType.objects.all()\n    )\n\n    class Meta:\n        model = File\n        fields = '__all__'\n\n\nso we can create files by passing the endpoint a payload like:{\n\u00a0\u00a0\u00a0\u00a0\"file\": \"file\",\n\u00a0\u00a0\u00a0\u00a0\"type\": \"name-of-file-type\",\n}\n\nNow we have all of the information we need to extract time series from files into our data model.Validate messy filesWhat if a File is created with an inappropriate FileType? How do we catch this before it causes importing to fail?We can implement a clean method on File! Django will automatically call this method on running form.is_valid() in our view; however, we\u2019ll have to connect django-rest-framework ourselves. We can just add a validate method to our serializer to achieve the same behavior.# sensor/api/serializers.py\n\nfrom rest_framework import serializers\n\nfrom ..models import File\nfrom ..models import FileType\n\n\nclass FileSerializer(serializers.ModelSerializer):\n\n    # ...\n\n    def validate(self, attrs):\n        instance = File(**attrs)\n        instance.clean()\n        return attrs\n\n\nNow, we can implement the clean method to check file contents prior to saving a file:# sensor/models.py\n\nfrom django.db import models\nfrom django.core.exceptions import ValidationError\n\nfrom .io import validate_datetime_fieldnames_in_lines\n\n# ...\n\nclass File(models.Model):\n\n    # ...\n\n    def clean(self):\n\n        if self.type is None:\n            raise ValidationError(\"File type must be specified!\")\n\n        # NOTE: This file is automatically closed upon saving a model instance\n        # ... each time a file is read the file pointer must be reset to enable rereads\n        f = self.file.open(mode=\"rb\")\n\n        # NOTE: automatically called by Django Forms & DRF Serializer Validate Method\n        validate_datetime_fieldnames_in_lines(\n            lines=f,\n            encoding=self.type.encoding,\n            delimiter=self.type.delimiter,\n            datetime_fieldnames=self.type.datetime_fieldnames,\n        )\n        self.file.seek(0)\n\n\n# ...\n\n# sensor/io.py\n\nimport re\nimport typing\n\nfrom django.core.exceptions import ValidationError\n\n\ndef yield_split_lines(\n    lines: typing.Iterable[bytes],\n    encoding: str,\n    delimiter: str,\n) -> typing.Iterator[typing.Tuple[typing.Any]]:\n\n    # Unescape strings `\\\\t` to `\\t` for use in a regular expression\n    # https://stackoverflow.com/questions/1885181/how-to-un-escape-a-backslash-escaped-string\n    unescape_backslash = lambda s: (\n        s.encode('raw_unicode_escape').decode('unicode_escape')\n    )\n    split = lambda s: re.split(unescape_backslash(delimiter), s)\n    return (split(line.decode(encoding)) for line in iter(lines))\n\n\ndef validate_datetime_fieldnames_in_lines(\n    lines: typing.Iterable[bytes],\n    encoding: str,\n    delimiter: str,\n    datetime_fieldnames: typing.Iterable[str],\n) -> None:\n\n    split_lines = yield_split_lines(lines=lines, encoding=encoding, delimiter=delimiter)\n    fieldnames = None\n\n    for line in split_lines:\n        if set(datetime_fieldnames).issubset(set(line)):\n            fieldnames = line\n            break\n    \n    if fieldnames == None:\n        raise ValidationError(f\"No `datetime_fieldnames` {datetime_fieldnames} found!\")\n\n\nImport and validate messy filesNow we have everything we need to import files:# sensor/models.py\n\nfrom itertools import islice\n\nfrom django.db import models\nfrom django.db import transaction\n\nfrom .io import validate_datetime_fieldnames_in_lines\nfrom .io import yield_readings_in_narrow_format\n\n\n# ...\n\n\nclass File(models.Model):\n\n    # ...\n\n    def import_to_db(self):\n\n        with self.file.open(mode=\"rb\") as f:\n    \n            reading_objs = (\n                Reading(\n                    file=self,\n                    timestamp=r[\"timestamp\"],\n                    sensor_name=r[\"sensor_name\"],\n                    reading=r[\"reading\"]\n                )\n                for r in yield_readings_in_narrow_format(\n                    lines=f,\n                    encoding=self.type.encoding,\n                    delimiter=self.type.delimiter,\n                    datetime_fieldnames=self.type.datetime_fieldnames,\n                    datetime_formats=self.type.datetime_formats,\n                )\n            )\n\n            batch_size = 1_000\n        try:\n                with transaction.atomic():\n                    while True:\n                        batch = list(islice(reading_objs, batch_size))\n                        if not batch:\n                            break\n                        Reading.objects.bulk_create(batch, batch_size)\n\n            except Exception as e:\n                self.parsed_at = None\n                self.parse_error = str(e)\n                self.save()\n                raise e\n\n            else:\n                self.parsed_at = datetime.now(timezone.utc)\n                self.parse_error = None\n                self.save()\n\n\n# sensor/io.py\n\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport typing\n\nfrom django.core.exceptions import ValidationError\n\n\n# ...\n\n\ndef yield_readings_in_narrow_format(\n    lines: typing.Iterable[bytes],\n    encoding: str,\n    delimiter: str,\n    datetime_fieldnames: typing.Iterable[str],\n    datetime_formats: typing.Iterable[str],\n) -> typing.Iterator[typing.Tuple[typing.Any]]:\n    \"\"\"\n    https://en.wikipedia.org/wiki/Wide_and_narrow_data\n    \"\"\"\n\n    split_lines = yield_split_lines(lines=lines, encoding=encoding, delimiter=delimiter)\n    fieldnames = None\n\n    for line in split_lines:\n        if set(datetime_fieldnames).issubset(set(line)):\n            fieldnames = line\n            break\n    \n    if fieldnames == None:\n        raise ValidationError(f\"No `datetime_fieldnames` {datetime_fieldnames} found!\")\n\n    # NOTE: `split_lines` is an iterator so prior loop exhausts the header lines\n    for line in split_lines:\n        fields = OrderedDict([(f, v) for f, v in zip(fieldnames, line)])\n        readings = OrderedDict(\n            [(f, v) for f, v in fields.items() if f not in datetime_fieldnames]\n        )\n\n        timestamp_strs = [fields[k] for k in datetime_fieldnames]\n        timestamp_str = \" \".join(\n            str(item) for item in timestamp_strs if item is not None\n        )\n\n        for datetime_format in datetime_formats:\n            try:\n                timestamp = datetime.strptime(timestamp_str, datetime_format)\n            except ValueError:\n                pass\n            else:\n                for sensor, reading in readings.items():\n                    yield {\n                        \"timestamp\": timestamp,\n                        \"sensor_name\": sensor,\n                        \"reading\": reading,\n                    }\n\n\nOnce again, let\u2019s adapt the views and viewsets  to call the import_to_db method.For Django, we can call it directly in our upload-file view likeif form.is_valid():\n    form.save()\n    form.instance.import_to_db()\n\nand for django-rest-framework we can override the perform_create method:class FileViewSet(viewsets.ModelViewSet):\n\n    # ...\n\n    def perform_create(self, serializer):\n        instance = serializer.save()\n        instance.import_to_db()\n\n\n\nIngest and query in milliseconds, even at terabyte scale.\n\nTry Timescale for free\n\n\n\n\n\n            This post was written by\n        \n\n\nRowan Molony\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShare this Post\n\n\n\n\n\n\n19 Jan 2024\n14 min read\n\nPython\n\n\n            Contributors\n          \n\n\nRowan Molony\n\n\n\n\nShare\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      Related posts\n    \n\n\n\n\n\n\n\n\n\nPostgreSQL\n\n\n\n\n\n\n\n\nHow to Build LLM Applications With pgvector Vector Store in LangChain\n\n\n\n\n\n12 Jul 2023\n\n\n            11 min read\n          \n\n\n\n\n\u201cHello World\u201d for pgvector and LangChain! Learn how to build LLM applications with LangChain framework, using PostgreSQL and pgvector as a vector database for embeddings data.\n\n\n\n\n\n\n\n\n\n\n\n\nDev Q&A\n\n\n\n\n\n\n\n\nSaving Devs\u2019 Time and Compute Power With Retention Policies: The Story of Crypto Trading Platform Pintu\n\n\n\n\n\n27 Jun 2023\n\n\n            6 min read\n          \n\n\n\n\nRead how crypto trading platform Pintu accelerated their go-to-market motion and saved developers' time and compute power using retention policies and incrementally-updated materialized views.\n\n\n\n\n\n\n\n\n\n\n\n\nCloud\n\n\n\n\n\n\n\n\nDo More on AWS With Timescale: Build an Application Using Lambda Functions in Python\n\n\n\n\n\n24 Feb 2023\n\n\n            2 min read\n          \n\n\n\n\nWatch this step-by-step guide on creating a serverless time-series application consisting of two Lambda functions written in Python, using Timescale and AWS Lambda.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            Share this post\n          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProducts\n\n          Launch 2023\n        \n\n          Time Series and Analytics\n        \n\n          Timescale Vector\n        \n\n          Enterprise Tier\n        \n\n          Dynamic PostgreSQL\n        \n\n          Cloud Status\n        \n\n          Support\n        \n\n          Security\n        \n\n          Cloud Terms of Service\n        \n\n\nLearn\n\n          Documentation\n        \n\n          Blog\n        \n\n          Forum\n        \n\n          Tutorials\n        \n\n          Release notes\n        \n\n          Case studies\n        \n\n          Time series database\n        \n\n\nCompany\n\n          Contact us\n        \n\n          Careers\n        \n\n          About\n        \nNewsroom\nBrandCommunity\nTimescale shop\nCode of conduct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubscribe to the Timescale Newsletter\n\n\n\n\n\n\n\n\n\n\n\nSubscribe\n\n\n\n\nBy submitting you acknowledge Timescale's\u00a0\n          Privacy Policy.\n        \n\n\n\n\n\n        2024\n        \u00a9 Timescale, Inc. All Rights Reserved.\n      \n\nPrivacy preferences\nLegal\nPrivacy\nSitemap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
  "latestPost": {
    "title": "Getting Sensor Data Into TimescaleDB via Django",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://www.timescale.com/blog/rss/",
      "value": "Getting Sensor Data Into TimescaleDB via Django"
    },
    "summary": "Learn how to build an app to import data into TimescaleDB via Django, courtesy of our community member, Rowan Molony.",
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://www.timescale.com/blog/rss/",
      "value": "Learn how to build an app to import data into TimescaleDB via Django, courtesy of our community member, Rowan Molony."
    },
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://www.timescale.com/blog/getting-sensor-data-into-timescaledb-via-django/"
      }
    ],
    "link": "https://www.timescale.com/blog/getting-sensor-data-into-timescaledb-via-django/",
    "id": "65a950ae0d3dd40001dbf965",
    "guidislink": false,
    "tags": [
      {
        "term": "Python",
        "scheme": null,
        "label": null
      },
      {
        "term": "Guest Blog",
        "scheme": null,
        "label": null
      }
    ],
    "authors": [
      {
        "name": "Rowan Molony"
      }
    ],
    "author": "Rowan Molony",
    "author_detail": {
      "name": "Rowan Molony"
    },
    "published": "Fri, 19 Jan 2024 12:37:22 GMT",
    "published_parsed": [
      2024,
      1,
      19,
      12,
      37,
      22,
      4,
      19,
      0
    ],
    "media_content": [
      {
        "url": "https://www.timescale.com/blog/content/images/2024/01/Getting-sensor-data-into-timescaledb-via-django--1-.png",
        "medium": "image"
      }
    ],
    "content": [
      {
        "type": "text/html",
        "language": null,
        "base": "https://www.timescale.com/blog/rss/",
        "value": "<img alt=\"Getting Sensor Data Into TimescaleDB via Django\" src=\"https://www.timescale.com/blog/content/images/2024/01/Getting-sensor-data-into-timescaledb-via-django--1-.png\" /><p>Over 2022-23, while working at <a href=\"https://www.mainstreamrp.com/?ref=timescale.com\"><u>Mainstream Renewable Power</u></a> on an internal web application, I maintained a &quot;data pipeline&quot; that fetches files of sensor data readings from the world&apos;s most remote places and transforms them into useful datasets. </p><p>These datasets form the basis upon which the construction of renewables (wind turbines or solar panels) on site hinges. I rebuilt the pipeline on top of <code>TimescaleDB</code>, which enabled me to massively reduce the complexity of the system involved.</p><div class=\"kg-card kg-callout-card kg-callout-card-purple\"><div class=\"kg-callout-emoji\">&#x1f4a5;</div><div class=\"kg-callout-text\">Don&#x2019;t know what TimescaleDB is? <a href=\"https://www.timescale.com/learn/is-postgres-partitioning-really-that-hard-introducing-hypertables?ref=timescale.com\">Read this article</a>.</div></div><p><br /></p><p><br /><br />I reflect on this experience in detail in <a href=\"https://www.rowanmolony.com/blog/2023-12-04-struggling-to-sync-sensors-and-databases.html?ref=timescale.com\"><em><u>Struggling to Sync Sensors &amp; Databases</u></em></a>. I do not, however, discuss how I adapted <code>Django</code>, a Python web framework, to play nicely with this database. In my case, Django served as the &#x201c;glue&#x201d; between web browsers and the database. Specifically, to display a web page, it asks a database for the data it needs to render files that the browser interprets (<code>HTML</code>, <code>CSS</code>, and <code>JavaScript</code>) so it can display a user interface.</p><p>Let&#x2019;s walk through an example project to make these adaptations a bit more concrete.</p><p>This tutorial assumes some familiarity with Django or a similar web framework. If you have never used Django, I highly recommend <a href=\"https://docs.djangoproject.com/en/5.0/intro/tutorial01/?ref=timescale.com\"><u>the official tutorial</u></a>.</p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">If you want to follow along locally, you can set up a developer environment via <a href=\"https://github.com/rdmolony/django-timescaledb-example?ref=timescale.com\">django-timescaledb-example</a>.</div></div><p><br /></p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">If you have any trouble getting set up, feel free to ask a question at <a href=\"https://github.com/rdmolony/django-timescaledb-example/discussions?ref=timescale.com\">django-timescaledb-example/discussions</a>.</div></div><h2 id=\"create-a-sensor-data-app-using-django\"><br />Create a Sensor Data App Using Django</h2><p>Let&#x2019;s first run</p><pre><code class=\"language-Python\">python manage.py startapp sensor\n</code></pre>\n<p>to create files</p><pre><code class=\"language-Python\">sensor\n&#x251c;&#x2500;&#x2500; __init__.py\n&#x251c;&#x2500;&#x2500; admin.py\n&#x251c;&#x2500;&#x2500; apps.py\n&#x251c;&#x2500;&#x2500; migrations\n&#x2502;   &#x2514;&#x2500;&#x2500; __init__.py\n&#x251c;&#x2500;&#x2500; models.py\n&#x251c;&#x2500;&#x2500; tests.py\n&#x2514;&#x2500;&#x2500; views.py\n</code></pre>\n<p>and register the app in <code>core/settings.py</code>.</p><pre><code class=\"language-Python\">INSTALLED_APPS = [\n\n    # Builtin\n    &apos;django.contrib.admin&apos;,\n    &apos;django.contrib.auth&apos;,\n    &apos;django.contrib.contenttypes&apos;,\n    &apos;django.contrib.sessions&apos;,\n    &apos;django.contrib.messages&apos;,\n    &apos;django.contrib.staticfiles&apos;,\n\n    # Custom\n    &apos;sensor&apos;\n]\n</code></pre>\n<h2 id=\"create-a-homepage\">Create a Homepage</h2><p>Let&#x2019;s quickly create a homepage that will be displayed on first opening this web application in a browser.</p><pre><code class=\"language-Python\"># sensor/views.py\n\nfrom django.shortcuts import render\n\n\ndef index(request):\n    return render(request, &quot;index.html&quot;)\n</code></pre>\n<pre><code class=\"language-Python\">&lt;!--- sensor/templates/index.html --&gt;\n\n&lt;div style=&quot;text-align: center&quot;&gt;\n  &lt;h1&gt;django-timescaledb-example&lt;/h1&gt;\n&lt;/div&gt;\n</code></pre>\n<pre><code class=\"language-Python\"># sensor/urls.py\n\nfrom django.urls import path\n\nfrom . import views\n\n\napp_name = &quot;sensor&quot;\n\n\nurlpatterns = [\n    path(&apos;&apos;, views.index, name=&quot;root&quot;),\n]\n</code></pre>\n<pre><code class=\"language-Python\"># core/urls.py\n\nfrom django.contrib import admin\nfrom django.shortcuts import redirect\nfrom django.urls import include\nfrom django.urls import path\n\n\nurlpatterns = [\n    path(&apos;&apos;, lambda request: redirect(&apos;sensor:root&apos;)),\n\n    path(&apos;admin/&apos;, admin.site.urls),\n    path(&apos;sensor/&apos;, include(&apos;sensor.urls&apos;)),\n]\n</code></pre>\n<p>So now <code>http://localhost:8000</code> should display <code>index.html</code>. We can build on this <code>index.html</code> to link to other pages.</p><h2 id=\"create-a-data-model-for-files\">Create a Data Model for Files</h2><p>Now, I can adapt <code>sensor/models.py</code> to add a File model to track uploaded files,</p><pre><code class=\"language-Python\"># sensor/models.py\n\nclass File(models.Model):\n    file = models.FileField(upload_to=&quot;readings/&quot;, blank=False, null=False)\n    uploaded_at = models.DateTimeField(auto_now_add=True)\n    parsed = models.DateTimeField(blank=False, null=False)\n    parse_error = models.TextField(blank=True, null=True)\n</code></pre>\n<p>create its database migration,</p><pre><code class=\"language-Python\">python manage.py makemigrations sensor\n</code></pre>\n<p>and roll it out:</p><pre><code class=\"language-Python\">python manage.py migrate\n</code></pre>\n<div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">Any change to sensor/models.py requires a corresponding database migration!</div></div><h2 id=\"handle-file-uploads-via-browser\"><br />Handle File Uploads via Browser</h2><p>Now that we have somewhere to store files of readings, we need to handle file uploads.</p><p>Let&#x2019;s consider only the case where time-series data originates only from text files. How do I copy data from files into TimescaleDB via Django?</p><p>The Django documentation covers <a href=\"https://docs.djangoproject.com/en/5.0/topics/http/file-uploads/?ref=timescale.com\"><u>File Uploads</u></a>. However, it doesn&#x2019;t advise on importing file contents to a database. One normally uses Django to add and save new entries to PostgreSQL using input from a browser:</p><ul><li>Django sends a web page to a browser containing one or more <code>&lt;form&gt;</code> elements.</li><li>Once filled in, these <code>&lt;form&gt;</code> elements are sent back to Django.</li><li>Django processes these entries and saves them to the database using the Django ORM.</li></ul><p>The key enabler here is the <a href=\"https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping?ref=timescale.com\"><u>ORM (or &#x201c;Object Relational Mapper&#x201d;)</u></a>. It maps a Python class to a database table so that this table&#x2019;s data is easily accessible from within Python. Without an ORM, one would have to use the SQL language to communicate with the database.</p><p>We need to do a bit of work to adapt this workflow to handle file contents.</p><p>In a similar manner, I can create a &#x201c;view&#x201d; to render HTML to accept browser file uploads:</p><pre><code class=\"language-Python\"># sensor/views.py\n\nfrom django.shortcuts import render\nfrom django.shortcuts import redirect\nfrom django.http import HttpResponse\n\nfrom .forms import FileForm\n\n\n# ...\n\n\ndef upload_file(request):\n    if request.method == &quot;POST&quot;:\n        form = FileForm(request.POST, request.FILES)\n        if form.is_valid():\n            form.save()\n            return HttpResponse(&quot;File upload was successful&quot;)\n        else:\n            return HttpResponse(&quot;File upload failed&quot;)\n    else:\n        form = FileForm()\n    return render(request, &quot;upload_file.html&quot;, {&quot;form&quot;: form})\n\n\n# ...\n</code></pre>\n<h3 id=\"id\"></h3><pre><code class=\"language-Python\"># sensor/forms.py\n\nfrom django.forms import ModelForm\n\nfrom .models import File\n\n\nclass FileForm(ModelForm):\n    class Meta:\n        model = File\n        fields = &quot;__all__&quot;\n</code></pre>\n<pre><code class=\"language-Python\">&lt;!--- sensor/templates/upload_file.html --&gt;\n\n&lt;div style=&quot;text-align: center&quot;&gt;\n  &lt;h1&gt;Upload File&lt;/h1&gt;\n  &lt;form enctype=&quot;multipart/form-data&quot; method=&quot;post&quot;&gt;\n    {% csrf_token %}\n    {% for field in form %}\n      &lt;div style=&quot;margin-bottom: 10px&quot;&gt;\n        {{ field.label_tag }}\n        {{ field }}\n        {% if field.help_text %}\n          &lt;span class=&quot;question-mark&quot; title=&quot;{{ field.help_text }}&quot;&gt;&amp;#63;&lt;/span&gt;\n        {% endif %}\n      &lt;/div&gt;\n    {% endfor %}\n    &lt;input type=&quot;submit&quot; value=&quot;Save&quot;&gt;&lt;/input&gt;\n  &lt;/form&gt;\n&lt;/div&gt;\n</code></pre>\n<pre><code class=\"language-Python\"># sensor/urls.py\n\nfrom django.urls import path\n\nfrom . import views\n\n\napp_name = &quot;sensor&quot;\n\n\nurlpatterns = [\n    path(&apos;&apos;, views.index, name=&quot;root&quot;),\n    path(&apos;upload-file/&apos;, views.upload_file, name=&quot;upload-file&quot;),\n]\n</code></pre>\n<p>This requires someone to click through this web application every time they want to add new data. If data is synced automatically from remote sensors to a file system somewhere, then why not set up automatic file uploads? For this, we need an API.</p><p>An API (or Application Programming Interface) lets our web application accept file uploads from another program.</p><p>The <code>django-rest-framework</code> library does a lot of heavy lifting here, so let&#x2019;s use it.</p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">If you are not interested in APIs, feel free to skip the API sections.</div></div><p><br /></p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">If you have never used django-rest-framework, consider first completing the <a href=\"https://www.django-rest-framework.org/tutorial/quickstart/?ref=timescale.com\">official tutorial</a> before continuing.</div></div><h2 id=\"create-an-api-homepage\"><br />Create an API Homepage</h2><p>As suggested by <a href=\"https://www.feldroy.com/books/two-scoops-of-django-3-x?ref=timescale.com\"><u>Two Scoops of Django 3.x</u></a>, let&#x2019;s create <code>core/api_urls.py</code> to wire up our API.</p><pre><code class=\"language-Python\"># core/urls.py\n\nfrom django.contrib import admin\nfrom django.shortcuts import redirect\nfrom django.urls import include\nfrom django.urls import path\n\n\nurlpatterns = [\n    path(&apos;&apos;, lambda request: redirect(&apos;sensor:root&apos;)),\n\n    path(&apos;admin/&apos;, admin.site.urls),\n    path(&apos;api/&apos;, include(&apos;core.api_urls&apos;)),\n    path(&apos;sensor/&apos;, include(&apos;sensor.urls&apos;)),\n]\n</code></pre>\n<pre><code class=\"language-Python\"># core/api_urls.py\n\nfrom django.urls import include\nfrom django.urls import path\nfrom rest_framework.decorators import api_view\nfrom rest_framework.response import Response\nfrom rest_framework.reverse import reverse\n\n\napp_name = &quot;api&quot;\n\n\n@api_view([&apos;GET&apos;])\ndef api_root(request, format=None):\n    return Response({\n        &apos;sensors&apos;: reverse(&apos;api:sensor:api-root&apos;, request=request, format=format),\n    })\n\n\nurlpatterns = [\n    path(&apos;&apos;, api_root, name=&quot;api-root&quot;),\n    path(&apos;sensor/&apos;, include(&apos;sensor.api_urls&apos;), name=&apos;sensor&apos;),\n]\n</code></pre>\n<pre><code class=\"language-Python\"># core/api_urls.py\n\nfrom django.urls import include\nfrom django.urls import path\nfrom rest_framework.decorators import api_view\nfrom rest_framework.response import Response\nfrom rest_framework.reverse import reverse\n\nfrom .api import viewsets\n\n\napp_name = &quot;sensor&quot;\n\n\n@api_view([&apos;GET&apos;])\ndef api_root(request, format=None):\n    return Response({})\n\n\nurlpatterns = [\n    path(&apos;&apos;, api_root, name=&quot;api-root&quot;),\n]\n</code></pre>\n<p><br />So now we can add new views and/or view sets to <code>sensor/api_urls.py</code>. Plus, they will be &#x201c;connectable&#x201d; via <code>/api/sensor/</code>.<br /></p><h2 id=\"handle-file-uploads-via-api\">Handle File Uploads via API</h2><p>We can use a <code>viewset</code> to create an endpoint like <code>/api/sensor/file/</code> to which another program can upload files:</p><pre><code class=\"language-Python\"># sensor/api/viewsets.py\n\nfrom rest_framework import viewsets\n\nfrom ..models import File\nfrom .serializers import FileSerializer\n\n\nclass FileViewSet(viewsets.ReadOnlyModelViewSet):\n    queryset = File.objects.all()\n    serializer_class = FileSerializer\n</code></pre>\n<p></p><pre><code class=\"language-Python\"># sensor/api/serializers.py\n\nfrom rest_framework import serializers\n\nfrom ..models import File\n\n\nclass FileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = File\n        fields = [&apos;__all__&apos;]\n</code></pre>\n<h2 id=\"create-a-data-model-for-sensor-data-readings\">Create a Data Model for Sensor Data Readings</h2><p>Let&#x2019;s add a reading model to store readings:</p><pre><code class=\"language-Python\"># sensor/models.py\n\nfrom django.db import models\n\n\n# ....\n\n\nclass Reading(models.Model):\n\n    class Meta:\n        managed = False\n\n    file = models.ForeignKey(File, on_delete=models.RESTRICT)\n    timestamp = models.DateTimeField(blank=False, null=False, primary_key=True)\n    sensor_name = models.TextField(blank=False, null=False)\n    reading = models.TextField(blank=False, null=False)\n</code></pre>\n<p>This time, we&#x2019;re using timestamp instead of the default <code>id</code> field as a primary key since row uniqueness can be defined by a composite of <code>file</code>, <code>timestamp</code>, and <code>sensor_name</code> if required.</p><p>Don&#x2019;t we want to store readings in a TimescaleDB hypertable to make them easier to work with? Django won&#x2019;t automatically create a hypertable (it wasn&#x2019;t designed to), so we need to do so ourselves. Since we need to customize table creation ourselves rather than let Django do it, we have to set managed to False. </p><p>Let&#x2019;s create a &#x201c;base&#x201d; migration,</p><pre><code class=\"language-Python\">python manage.py makemigrations sensor --name &quot;sensor_reading&quot;\n</code></pre>\n<p>and manually edit the migration:</p><pre><code class=\"language-Python\"># sensor/migrations/0002_sensor_reading.py\n\nimport django.db.models.deletion\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (&apos;sensor&apos;, &apos;0001_initial&apos;)\n    ]\n    operations = [\n        migrations.CreateModel(\n            name=&apos;Reading&apos;,\n            fields=[\n                (&apos;file&apos;, models.ForeignKey(default=None, on_delete=django.db.models.deletion.RESTRICT, to=&apos;sensor.file&apos;)),\n                (&apos;timestamp&apos;, models.DateTimeField(primary_key=True)),\n                (&apos;sensor_name&apos;, models.TextField()),\n                (&apos;reading&apos;, models.FloatField()),\n            ],\n            options={\n                &apos;managed&apos;: False,\n            },\n        ),\n        migrations.RunSQL(\n            &quot;&quot;&quot;\n            CREATE TABLE sensor_reading (\n                file_id INTEGER NOT NULL REFERENCES sensor_file (id),\n                timestamp TIMESTAMP NOT NULL,\n                sensor_name TEXT NOT NULL,\n                reading FLOAT\n            );\n            SELECT create_hypertable(&apos;sensor_reading&apos;, &apos;timestamp&apos;);\n            &quot;&quot;&quot;,\n            reverse_sql=&quot;&quot;&quot;\n                DROP TABLE sensor_reading;\n            &quot;&quot;&quot;\n        ),\n    ]\n</code></pre>\n<p>Now we can roll out migrations,</p><pre><code class=\"language-Python\">python manage.py migrate\n</code></pre>\n<p>and connect to the database to inspect the newly created hypertable.</p><h2 id=\"import-files\">Import Files</h2><p>Let&#x2019;s imagine that all of our sensor data readings are stored nicely formatted in JSON files, like: </p><pre><code class=\"language-Python\">[\n    {\n      &quot;reading&quot;: 20.54,\n      &quot;sensor_name&quot;: &quot;M(m/s)&quot;,\n      &quot;timestamp&quot;: &quot;2015-12-22 00:00:00&quot;,\n    },\n    {\n      &quot;reading&quot;: 211.0,\n      &quot;sensor_name&quot;: &quot;D(deg)&quot;,\n      &quot;timestamp&quot;: &quot;2015-12-22 00:00:00&quot;,\n    },\n]\n</code></pre>\n<p>How do we import these readings?</p><p>We can add a method to File to bring the JSON file into Python and create a new Reading entry for each reading in the file:</p><pre><code class=\"language-Python\"># sensor/models.py\n\nfrom datetime import datetime\nfrom datetime import timezone\nfrom itertools import islice\n\nfrom django.contrib.postgres.fields import ArrayField\nfrom django.db import models\nfrom django.db import transaction\n\nfrom .io import validate_datetime_fieldnames_in_lines\nfrom .io import yield_readings_in_narrow_format\n\n\n# ...\n\n\nclass File(models.Model):\n\n    # ...\n\n    def import_to_db(self):\n\n        # NOTE: assume uploaded file is JSON\n        with self.file.open(mode=&quot;rb&quot;) as f:\n\n            reading_objs = (\n                Reading(\n                    file=self,\n                    timestamp=r[&quot;timestamp&quot;],\n                    sensor_name=r[&quot;sensor_name&quot;],\n                    reading=r[&quot;reading&quot;]\n                )\n                for r in json.load(f)\n            )\n\n            batch_size = 1_000\n\n            with transaction.atomic():\n                while True:\n                    batch = list(islice(reading_objs, batch_size))\n                    if not batch:\n                        break\n                    Reading.objects.bulk_create(batch, batch_size)\n</code></pre>\n<p>How do we call the <code>import_to_db</code> method?</p><p>We can go about this a few different ways, but perhaps the simplest is just to implement it directly in the <code>views</code> and <code>viewsets</code> so that it will be triggered on browser and API file uploads.</p><p>For Django, we can call it directly in our <code>upload-file</code> view like:</p><pre><code class=\"language-Python\">if form.is_valid():\n    form.save()\n    form.instance.import_to_db()\n</code></pre>\n<p>And for <code>django-rest-framework</code> we can override the <code>perform_create method</code>.</p><pre><code class=\"language-Python\">class FileViewSet(viewsets.ModelViewSet):\n\n    # ...\n\n    def perform_create(self, serializer):\n        instance = serializer.save()\n        instance.import_to_db()\n</code></pre>\n<h2 id=\"import-files-via-celery\">Import Files via Celery</h2><p>What if each file contains a few gigabytes of readings? Won&#x2019;t this take an age to process?</p><p>If you can&#x2019;t guarantee that the sensor files are small enough to be processed quickly, you might need to offload file importing to a task queue.</p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">A task queue works like a restaurant. The waiters add an order to the queue, and the chefs pull orders from the queue when they have time to process them.</div></div><p><br /></p><p></p><p><br />Celery is a mature Python task queue library and works well with Django, so let&#x2019;s use it. It coordinates &#x201c;waiters&#x201d; and &#x201c;chefs&#x201d; using the above analogy by leveraging a database (or message broker), typically Redis or RabbitMQ.</p><p>A task queue can significantly improve performance here. It makes file uploads instant from the user&#x2019;s perspective since now file upload tasks are added to a queue rather than running immediately. It also enables parallel processing of files since task queue workers run in parallel to one another.</p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">Celery may not work well on Windows, so consider trying <a href=\"https://pypi.org/project/dramatiq/?ref=timescale.com\">dramatiq</a> instead if this is a hard requirement.</div></div><p><br /></p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-text\">Why not use Postgres as a message broker? The <a href=\"https://github.com/Koed00/django-q?ref=timescale.com\">django-q</a> implementation enables this via the Django ORM message broker. This might be a better choice for small-scale applications since it reduces system complexity.</div></div><p><br /><br /></p><p></p><p>To set up Celery, we can follow their <a href=\"https://docs.celeryq.dev/en/main/django/first-steps-with-django.html?ref=timescale.com\"><u>official tutorial</u></a>:</p><pre><code class=\"language-Python\"># core/celery.py\n\nimport os\n\nfrom celery import Celery\n\n# Set the default Django settings module for the &apos;celery&apos; program.\nos.environ.setdefault(&apos;DJANGO_SETTINGS_MODULE&apos;, &apos;core.settings&apos;)\n\napp = Celery(&apos;core&apos;)\n\n# Using a string here means the worker doesn&apos;t have to serialize\n# the configuration object to child processes.\n# - namespace=&apos;CELERY&apos; means all celery-related configuration keys\n#   should have a `CELERY_` prefix.\napp.config_from_object(&apos;django.conf:settings&apos;, namespace=&apos;CELERY&apos;)\n\n# Load task modules from all registered Django apps.\napp.autodiscover_tasks()\n\n\n@app.task(bind=True, ignore_result=True)\ndef debug_task(self):\n    print(f&apos;Request: {self.request!r}&apos;)\n\n</code></pre>\n<pre><code class=\"language-Python\"># core/settings.py\n\n# ...\n\nCELERY_BROKER_URL = &quot;redis://localhost:6379/0&quot;\n\n</code></pre>\n<p>So now we can add tasks to <code>sensor/tasks.py</code> like:</p><pre><code class=\"language-Python\"># sensor/tasks.py\n\nfrom celery import shared_task\n\nfrom .models import File\n\n\n@shared_task\ndef import_to_db(file_id):\n    file_obj = File.objects.get(id=file_id)\n    file_obj.import_to_db()\n\n</code></pre>\n<p>And replace all calls to <code>&lt;file_obj&gt;.import_to_db()</code> with <code>tasks.import_to_db(file_obj)</code>, and this task won&#x2019;t be run immediately but rather will be run by Celery when it has the availability to do so!</p><h2 id=\"next-steps\">Next Steps?</h2><p>If you really want to further eke out import performance, you&#x2019;ll have to go deeper and experiment with the following:</p><ul><li>Batch sizes: how many readings do you want to save at once? </li><li>Compression: TimescaleDB really shines once hypertables are compressed since it reduces storage costs and delivers faster analytics queries.</li></ul><p><em>We want to thank Rowan Molony for this awesome guest blog post (scroll down for some bonus advice) and for gracefully implementing our team&apos;s technical revisions in collaboration with the Timescale developer advocate </em><a href=\"https://timescaledb.slack.com/team/U01U05H6HLK?ref=timescale.com\" rel=\"noreferrer\"><em>J&#xf4;natas</em></a><em>. Join Rowan and thousands of other developers building with TimescaleDB in our </em><a href=\"https://slack.timescale.com/?ref=timescale.com\" rel=\"noreferrer\"><em>Slack community</em></a><em>. Once you&apos;re there, reach out to </em><a href=\"https://timescaledb.slack.com/team/U03797BSQKT?ref=timescale.com\" rel=\"noreferrer\"><em>Ana</em></a><em> if you, too, would like to write a guest blog post.</em></p><p><em>To try TimescaleDB, you can </em><a href=\"https://docs.timescale.com/self-hosted/latest/install/?ref=timescale.com\" rel=\"noreferrer\"><em>self-host</em></a><em> or create a </em><a href=\"https://console.cloud.timescale.com/signup?ref=timescale.com\" rel=\"noreferrer\"><em>fully managed Timescale account</em></a><em> (it&apos;s free for 30 days, no credit card required).</em></p><h2 id=\"bonus\">Bonus</h2><h3 id=\"import-messy-files\">Import messy files<br /></h3><p>How do we convert files like</p><pre><code>Lat=0  Lon=0  Hub-Height=160  Timezone=00.0  Terrain-Height=0.0\nComputed at 100 m resolution\n \nYYYYMMDD HHMM   M(m/s) D(deg) SD(m/s)  DSD(deg)  Gust3s(m/s)    T(C)    PRE(hPa)       RiNumber  VertM(m/s)\n20151222 0000  20.54   211.0    1.22       0.3        21.00     11.9      992.8            0.15    0.18\n20151222 0010  21.02   212.2    2.55       0.6        21.35     11.8      992.7            0.29   -0.09\n\n</code></pre>\n<p>into</p><pre><code class=\"language-Python\">[\n    {\n      &apos;reading&apos;: &apos;20.54&apos;,\n      &apos;sensor_name&apos;: &apos;M(m/s)&apos;,\n      &apos;timestamp&apos;: datetime.datetime(2015, 12, 22, 0, 0),\n    },\n    {\n      &apos;reading&apos;: &apos;211.0&apos;,\n      &apos;sensor_name&apos;: &apos;D(deg)&apos;,\n      &apos;timestamp&apos;: datetime.datetime(2015, 12, 22, 0, 0),\n    },\n    # ...\n]\n\n</code></pre>\n<p>&#xa0;so we can store them in the Reading data model?</p><p>In this file, YYYYMMDD and HHMM clearly represent the timestamp, so 20151222 0000 corresponds to <code>datetime.datetime(2015, 12, 22, 0, 0)</code>. However, this may differ between sources.</p><p>One way to generalize the importer is to upload a FileType specification alongside each file so we know how to standardize it.</p><p>We can create a new model FileType and link it to File like:</p><pre><code class=\"language-Python\"># sensor/models.py\n\nfrom django.contrib.postgres.fields import ArrayField\nfrom django.db import models\n\n\nclass FileType(models.Model):\n\n    name = models.TextField()\n    na_values = ArrayField(\n        base_field=models.CharField(max_length=10),\n        default=[&quot;NaN&quot;],\n        help_text=textwrap.dedent(\n            r&quot;&quot;&quot;A list of strings to recognise as empty values.\n            \n            Default: [&quot;NaN&quot;]\n\n            Note: &quot;&quot; is also included by default\n\n            Example: [&quot;NAN&quot;, &quot;-9999&quot;, &quot;-9999.0&quot;]\n            &quot;&quot;&quot;\n        ),\n    )\n    delimiter = models.CharField(\n        max_length=5,\n        help_text=textwrap.dedent(\n            r&quot;&quot;&quot;The character used to separate fields in the file.\n            \n            Default: &quot;,&quot;\n            \n            Examples: &quot;,&quot; or &quot;;&quot; or &quot;\\s+&quot; for whitespace or &quot;\\t&quot; for tabs\n            &quot;&quot;&quot;\n        ),\n        default=&quot;,&quot;,\n    )\n    datetime_fieldnames = ArrayField(\n        base_field=models.CharField(max_length=50),\n        default=[&quot;Time&quot;],\n        help_text=textwrap.dedent(\n            r&quot;&quot;&quot;A list of datetime field names.\n            \n            Examples:\n            \n            1) Data has a single datetime field named &quot;Time&quot; which has values like\n            &apos;2021-06-29 00:00:00.000&apos;:  [&quot;Time&quot;]\n\n            2) Data has two datetime fields named &quot;Date&quot; and &quot;Time&quot; which have values\n            like &apos;01.01.1999&apos; and &apos;00:00&apos; respectively: [&quot;Date&quot;,&quot;Time&quot;]\n            &quot;&quot;&quot;\n        ),\n    )\n    encoding = models.CharField(\n        max_length=25,\n        help_text=textwrap.dedent(\n            r&quot;&quot;&quot;The encoding of the file.\n\n            Default: &quot;utf-8&quot;\n\n            Examples: utf-8 or latin-1 or cp1252\n            &quot;&quot;&quot;\n        ),\n        default=&quot;utf-8&quot;,\n    )\n    datetime_formats = ArrayField(\n        base_field=models.CharField(max_length=25),\n        help_text=textwrap.dedent(\n            r&quot;&quot;&quot;The datetime format of `datetime_columns`.\n\n            See https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\n            for format codes\n\n            Default: &quot;%Y-%m-%d %H:%M:%S&quot;\n\n            Examples: &quot;%Y-%m-%d %H:%M:%S&quot; for &quot;2021-03-01 00:00:00&quot;\n            &quot;&quot;&quot;\n        ),\n        default=[r&quot;%Y-%m-%d %H:%M:%S&quot;],\n    )\n\n\nclass File(models.Model):\n\n    # ...\n    type = models.ForeignKey(FileType, on_delete=models.RESTRICT)\n</code></pre>\n<p>Django forms are smart enough to automatically render the <code>upload-file</code> view with a type field since we specified <code>fields = &quot;__all__&quot; in sensor/forms.py</code>.</p><p><code>django-rest-framework</code> viewsets will also include it, thanks to <code>fields = &quot;__all__&quot;</code> in <code>sensor/api/serializers.py</code>. However, its default behavior for foreign keys is not ideal. It expects to receive a numeric ID for field type, whereas it&#x2019;s more intuitive to specify the name field instead.&#xa0;</p><p>We can easily override this default behavior by specifying SlugRelatedField in our serializer:</p><pre><code class=\"language-Python\"># sensor/api/serializers.py\n\nfrom rest_framework import serializers\n\nfrom ..models import File\nfrom ..models import FileType\n\n\nclass FileSerializer(serializers.ModelSerializer):\n\n    type = serializers.SlugRelatedField(\n        slug_field=&quot;name&quot;, queryset=FileType.objects.all()\n    )\n\n    class Meta:\n        model = File\n        fields = &apos;__all__&apos;\n\n</code></pre>\n<p>so we can create files by passing the endpoint a payload like:</p><pre><code>{\n&#xa0;&#xa0;&#xa0;&#xa0;&quot;file&quot;: &quot;file&quot;,\n&#xa0;&#xa0;&#xa0;&#xa0;&quot;type&quot;: &quot;name-of-file-type&quot;,\n}\n</code></pre>\n<p>Now we have all of the information we need to extract time series from files into our data model.</p><h3 id=\"validate-messy-files\">Validate messy files</h3><p>What if a File is created with an inappropriate FileType? How do we catch this before it causes importing to fail?</p><p>We can implement a clean method on File! Django will automatically call this method on running <code>form.is_valid()</code> in our view; however, we&#x2019;ll have to connect <code>django-rest-framework</code> ourselves. We can just add a validate method to our serializer to achieve the same behavior.</p><pre><code class=\"language-Python\"># sensor/api/serializers.py\n\nfrom rest_framework import serializers\n\nfrom ..models import File\nfrom ..models import FileType\n\n\nclass FileSerializer(serializers.ModelSerializer):\n\n    # ...\n\n    def validate(self, attrs):\n        instance = File(**attrs)\n        instance.clean()\n        return attrs\n\n</code></pre>\n<p>Now, we can implement the clean method to check file contents prior to saving a file:</p><pre><code class=\"language-Python\"># sensor/models.py\n\nfrom django.db import models\nfrom django.core.exceptions import ValidationError\n\nfrom .io import validate_datetime_fieldnames_in_lines\n\n# ...\n\nclass File(models.Model):\n\n    # ...\n\n    def clean(self):\n\n        if self.type is None:\n            raise ValidationError(&quot;File type must be specified!&quot;)\n\n        # NOTE: This file is automatically closed upon saving a model instance\n        # ... each time a file is read the file pointer must be reset to enable rereads\n        f = self.file.open(mode=&quot;rb&quot;)\n\n        # NOTE: automatically called by Django Forms &amp; DRF Serializer Validate Method\n        validate_datetime_fieldnames_in_lines(\n            lines=f,\n            encoding=self.type.encoding,\n            delimiter=self.type.delimiter,\n            datetime_fieldnames=self.type.datetime_fieldnames,\n        )\n        self.file.seek(0)\n\n\n# ...\n</code></pre>\n<pre><code class=\"language-Python\"># sensor/io.py\n\nimport re\nimport typing\n\nfrom django.core.exceptions import ValidationError\n\n\ndef yield_split_lines(\n    lines: typing.Iterable[bytes],\n    encoding: str,\n    delimiter: str,\n) -&gt; typing.Iterator[typing.Tuple[typing.Any]]:\n\n    # Unescape strings `\\\\t` to `\\t` for use in a regular expression\n    # https://stackoverflow.com/questions/1885181/how-to-un-escape-a-backslash-escaped-string\n    unescape_backslash = lambda s: (\n        s.encode(&apos;raw_unicode_escape&apos;).decode(&apos;unicode_escape&apos;)\n    )\n    split = lambda s: re.split(unescape_backslash(delimiter), s)\n    return (split(line.decode(encoding)) for line in iter(lines))\n\n\ndef validate_datetime_fieldnames_in_lines(\n    lines: typing.Iterable[bytes],\n    encoding: str,\n    delimiter: str,\n    datetime_fieldnames: typing.Iterable[str],\n) -&gt; None:\n\n    split_lines = yield_split_lines(lines=lines, encoding=encoding, delimiter=delimiter)\n    fieldnames = None\n\n    for line in split_lines:\n        if set(datetime_fieldnames).issubset(set(line)):\n            fieldnames = line\n            break\n    \n    if fieldnames == None:\n        raise ValidationError(f&quot;No `datetime_fieldnames` {datetime_fieldnames} found!&quot;)\n\n</code></pre>\n<h3 id=\"import-and-validate-messy-files\">Import and validate messy files</h3><p>Now we have everything we need to import files:</p><pre><code class=\"language-Python\"># sensor/models.py\n\nfrom itertools import islice\n\nfrom django.db import models\nfrom django.db import transaction\n\nfrom .io import validate_datetime_fieldnames_in_lines\nfrom .io import yield_readings_in_narrow_format\n\n\n# ...\n\n\nclass File(models.Model):\n\n    # ...\n\n    def import_to_db(self):\n\n        with self.file.open(mode=&quot;rb&quot;) as f:\n    \n            reading_objs = (\n                Reading(\n                    file=self,\n                    timestamp=r[&quot;timestamp&quot;],\n                    sensor_name=r[&quot;sensor_name&quot;],\n                    reading=r[&quot;reading&quot;]\n                )\n                for r in yield_readings_in_narrow_format(\n                    lines=f,\n                    encoding=self.type.encoding,\n                    delimiter=self.type.delimiter,\n                    datetime_fieldnames=self.type.datetime_fieldnames,\n                    datetime_formats=self.type.datetime_formats,\n                )\n            )\n\n            batch_size = 1_000\n        try:\n                with transaction.atomic():\n                    while True:\n                        batch = list(islice(reading_objs, batch_size))\n                        if not batch:\n                            break\n                        Reading.objects.bulk_create(batch, batch_size)\n\n            except Exception as e:\n                self.parsed_at = None\n                self.parse_error = str(e)\n                self.save()\n                raise e\n\n            else:\n                self.parsed_at = datetime.now(timezone.utc)\n                self.parse_error = None\n                self.save()\n\n</code></pre>\n<pre><code class=\"language-Python\"># sensor/io.py\n\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport re\nimport typing\n\nfrom django.core.exceptions import ValidationError\n\n\n# ...\n\n\ndef yield_readings_in_narrow_format(\n    lines: typing.Iterable[bytes],\n    encoding: str,\n    delimiter: str,\n    datetime_fieldnames: typing.Iterable[str],\n    datetime_formats: typing.Iterable[str],\n) -&gt; typing.Iterator[typing.Tuple[typing.Any]]:\n    &quot;&quot;&quot;\n    https://en.wikipedia.org/wiki/Wide_and_narrow_data\n    &quot;&quot;&quot;\n\n    split_lines = yield_split_lines(lines=lines, encoding=encoding, delimiter=delimiter)\n    fieldnames = None\n\n    for line in split_lines:\n        if set(datetime_fieldnames).issubset(set(line)):\n            fieldnames = line\n            break\n    \n    if fieldnames == None:\n        raise ValidationError(f&quot;No `datetime_fieldnames` {datetime_fieldnames} found!&quot;)\n\n    # NOTE: `split_lines` is an iterator so prior loop exhausts the header lines\n    for line in split_lines:\n        fields = OrderedDict([(f, v) for f, v in zip(fieldnames, line)])\n        readings = OrderedDict(\n            [(f, v) for f, v in fields.items() if f not in datetime_fieldnames]\n        )\n\n        timestamp_strs = [fields[k] for k in datetime_fieldnames]\n        timestamp_str = &quot; &quot;.join(\n            str(item) for item in timestamp_strs if item is not None\n        )\n\n        for datetime_format in datetime_formats:\n            try:\n                timestamp = datetime.strptime(timestamp_str, datetime_format)\n            except ValueError:\n                pass\n            else:\n                for sensor, reading in readings.items():\n                    yield {\n                        &quot;timestamp&quot;: timestamp,\n                        &quot;sensor_name&quot;: sensor,\n                        &quot;reading&quot;: reading,\n                    }\n\n</code></pre>\n<p>Once again, let&#x2019;s adapt the views and viewsets  to call the <code>import_to_db</code> method.</p><p>For Django, we can call it directly in our <code>upload-file</code> view like</p><pre><code class=\"language-Python\">if form.is_valid():\n    form.save()\n    form.instance.import_to_db()\n</code></pre>\n<p>and for <code>django-rest-framework</code> we can override the <code>perform_create</code> method:</p><pre><code class=\"language-Python\">class FileViewSet(viewsets.ModelViewSet):\n\n    # ...\n\n    def perform_create(self, serializer):\n        instance = serializer.save()\n        instance.import_to_db()\n\n</code></pre>"
      }
    ]
  }
}